[
  {
    "objectID": "index.html#section-1",
    "href": "index.html#section-1",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "",
    "text": "Sobre mim\n \n\nMe chamo Prof.¬†Dr.¬†Pedro Rafael D. Marinho. Meu curr√≠culo Lattes poder√° ser acessado clicando aqui.\nSou docente do Departamento de Estat√≠stica da UFPB. üë®‚Äçüè´\nToda minha forma√ß√£o acad√™mica √© na √°rea de estat√≠stica (bacharelado ao doutorado).\nTenho entusiasmo por programa√ß√£o, ci√™ncia de dados e aprendizagem de m√°quina üíªüìà.\n Me acompanhe no GitHub: https://github.com/prdm0.\n Me acompanhe no Linkedin: https://www.linkedin.com/in/prdm0/."
  },
  {
    "objectID": "index.html#sobre-mim",
    "href": "index.html#sobre-mim",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Sobre mim",
    "text": "Sobre mim\n \n\nMe chamo Prof.¬†Dr.¬†Pedro Rafael D. Marinho. Meu curr√≠culo Lattes poder√° ser acessado clicando aqui.\nSou docente do Departamento de Estat√≠stica da UFPB. üë®‚Äçüè´\nToda minha forma√ß√£o acad√™mica √© na √°rea de estat√≠stica (bacharelado ao doutorado).\nTenho entusiasmo por programa√ß√£o, ci√™ncia de dados e aprendizagem de m√°quina üíªüìà.\n Me acompanhe no GitHub: https://github.com/prdm0.\n Me acompanhe no Linkedin: https://www.linkedin.com/in/prdm0/."
  },
  {
    "objectID": "index.html#meu-segundo-lar",
    "href": "index.html#meu-segundo-lar",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Meu segundo lar",
    "text": "Meu segundo lar\n\n\n\n\nDepartamento de Estat√≠stica da UFPB."
  },
  {
    "objectID": "index.html#que-linguagem-de-programa√ß√£o-utilizar",
    "href": "index.html#que-linguagem-de-programa√ß√£o-utilizar",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Que linguagem de programa√ß√£o utilizar?",
    "text": "Que linguagem de programa√ß√£o utilizar?\n\nNesse curso, ser√° abordado a linguagem de programa√ß√£o R, mas lembre-se que voc√™ poder√° utilizar qualquer linguagem de programa√ß√£o para fazer ci√™ncia de dados. Por√©m, R e Python s√£o as minhas sugest√µes, haja vista que, atualmente, elas s√£o as linguagens com maior quantidade de ferramentas e usu√°rios trabalhando na √°rea de ci√™ncia de dados.\n\nOutros motivos que me leva a lecionar a disciplina utilizando a linguagem R s√£o:\n\nPossui ferramentas muito bem pensadas para manipula√ß√£o e tratamento de dados;\nNormalmente, os frameworks de machine learning de R s√£o menos verbosos que os de Python;\nMatrizes e data frames s√£o estruturas de dados que j√° encontra-se definidas dentro da linguagem, n√£o precisando assim de importar bibliotecas.\n\nIsso √© meu gosto pessoal. √â um gosto que, talvez, faz mais sentido, em se tratando de algu√©m que vem da estat√≠stica. No mercado de trabalho e em seus estudos, ap√≥s cursar as disciplinas de R e Python, fornecidas pelo Bacharelado em Estat√≠stica da UFPB, voc√™ ter√° a capacidade de estudar os frameworks de machine learning, aos seus pr√≥prios passos e escolher o que melhor te agrada. A linguagem Julia tamb√©m poder√° ser uma √≥tima op√ß√£o."
  },
  {
    "objectID": "index.html#aprendizagem-de-m√°quina",
    "href": "index.html#aprendizagem-de-m√°quina",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Aprendizagem de m√°quina",
    "text": "Aprendizagem de m√°quina"
  },
  {
    "objectID": "index.html#aprendizagem-de-m√°quina-1",
    "href": "index.html#aprendizagem-de-m√°quina-1",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Aprendizagem de m√°quina",
    "text": "Aprendizagem de m√°quina\n \nAlguns pontos:\n\n\nA Aprendizagem de M√°quina (AM), tamb√©m chamada de Machine Learning (ML), no ingl√™s, nasceu na d√©cada de 60 como um campo da intelig√™nica artificial.\nEm sua origem, as aplica√ß√µes de AM tinha como objetivo aprender padr√µes com base nos dados.\nOriginalmente, as aplica√ß√µes de AM eram de cunho estritamente computacional. Todavia, desde o in√≠cio dos anos 90, a √°rea de aprendizagem de m√°quina expandiu seus horizontes e come√ßou a se estabelecer como um campo por sim mesma.\nEm particular, a √°rea de aprendizagem de m√°quina come√ßou a estabelecer muitas intersec√ß√µes com a estat√≠stica. Muitos de seus algoritmos s√£o constru√≠dos com base em metodologias que surgiram na estat√≠stica.\nAtualmente, a comunidade de AM √© bastante interdisciplinar e utiliza-se de ideias desenvolvidas em diversas √°reas, sendo a estat√≠stica uma delas."
  },
  {
    "objectID": "index.html#tipos-de-aprendizado",
    "href": "index.html#tipos-de-aprendizado",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Tipos de Aprendizado",
    "text": "Tipos de Aprendizado\n\nAprendizado supervisionado\n\nNesse curso, inicialmente estudaremos problemas de aprendizado supervisionado, que consiste em aprender a fazer predi√ß√µes a partir de conjunto de dados em que r√≥tulos (valores da vari√°vel resposta Y) s√£o observados. Trataremos tanto de problemas de regress√£o (estimar um valor n√∫m√©rico) quanto problemas de classifica√ß√£o (classificar um cliente como aprovado ou reprovado, em um problema de concess√£o de cr√©dito). Por exemplo, os modelos de regress√£o s√£o exemplos de aprendizado supervisionado.\n\nAprendizado n√£o supervisionado\n\nNa segunda parte do curso, aprenderemos alguns m√©todos de aprendizado n√£o supervisionado, ou seja, algoritmos que n√£o utilizam-se de r√≥tulos, em que busca-se aprender mais sobre a estrutura dos dados. Por exemplo, os m√©todos de agrupamento (cluster), s√£o exempƒ∫os de m√©todos de aprendizado n√£o supervisionado."
  },
  {
    "objectID": "index.html#tipos-de-aprendizado-1",
    "href": "index.html#tipos-de-aprendizado-1",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Tipos de Aprendizado",
    "text": "Tipos de Aprendizado\n\nMuito embora no nosso curso focaremos nas abordagens de aprendizagem supervisionada e n√£o-supervisionada, os tipos de aprendizagem, em geral, podem ser mais amplos, em que temos:\n\n\nAprendizagem supervisionada;\nAprendizagem n√£o-supervisionada;\nAprendizagem semi-supervisionada;\nAprendizagem por refor√ßo."
  },
  {
    "objectID": "index.html#o-que-√©-aprender",
    "href": "index.html#o-que-√©-aprender",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "O que √© aprender?",
    "text": "O que √© aprender?\n\nAntes de detalharmos os tipos de aprendizagem de m√°quina, uma d√∫vida que poder√° surgir √©: ‚ÄúO que √© aprender?‚Äù. ‚ÄúComo a m√°quina aprende?‚Äù."
  },
  {
    "objectID": "index.html#o-que-√©-aprender-1",
    "href": "index.html#o-que-√©-aprender-1",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "O que √© aprender?",
    "text": "O que √© aprender?\n\nDe forma simples, aprender √© ganhar conhecimento atrav√©s de estudo, experi√™ncias, por meio de ensinamentos.\n\nT√°, mais como √© que a m√°quina aprende?\n\n\nAprendizagem √© o processo em que se adquire conhecimento, isto √©, √© o processo em que utilizamos de algoritmos e fornecemos dados a esses algoritmos para que possamos extrair conhecimento. Nesse processo de aprendisagem, os algoritmos fazem uso de dados para a extress√£o de conhecimento, atrav√©s de procedimentos supervisionado, n√£o-supervisionado, semi-supervisionado ou por refor√ßo, a depender do algoritmo que voc√™ deseja utilizar.\n\n\n\nAprendizado √© o modelo ajustado, isto √©, √© o conhecimento adquirido ap√≥s o treinamamento obtido no processo de aprendizagem. Voc√™ poder√° entender como sendo o modelo ajustado e que utilizamores para a tomada de decis√µes."
  },
  {
    "objectID": "index.html#o-que-√©-aprender-2",
    "href": "index.html#o-que-√©-aprender-2",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "O que √© aprender?",
    "text": "O que √© aprender?\n\nPortanto, voc√™ poder√° entender, basiciamente, existe quatro tipos de aprendizagem, sendo os dois primeiros o que mais focaremos nesse curso e que de loge s√£o os mais utilizados:\n\n\nAprendizagem supervisionada;\nAprendizagem n√£o-supervisionada;\nAprendizagem semi-supervisionada;\nAprendizagem por refor√ßo."
  },
  {
    "objectID": "index.html#aprendizagem-supervisionada",
    "href": "index.html#aprendizagem-supervisionada",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Aprendizagem supervisionada",
    "text": "Aprendizagem supervisionada\n\nNesse tipo de aprendizagem, o algoritmo ir√° receber um conjunto de dados em que conhecemos r√≥tulos para a vari√°vel de interesse. √â como se voc√™ soubesse onde um bom modelo deve chegar, para assim ser reconhecido como um bom modelo. Por exemplo,\n\n\nClassifica√ß√£o: precisamos determinar a classe de uma inst√¢ncia de dados, o seu atributo, i.e., \\widehat{y} = \\mathrm{argmax}_y\\,P(Y = y\\,|\\, X = \\bf{x}), em que y √© um atributo que desejamos prever (cahorro, gato, sapo), e \\bf{x} √© um vetor de caracter√≠sticas (peso, altura, comprimento, se tem rabo, etc).\n\n\n\nRegress√£o: precisamos estimar uma quantidade num√©rica, i.e., o valor da vari√°vel alvo por meio de uma inst√¢ncia de dados, ou seja, precisamos estimar Y = \\mathbb{E}(Y\\,|\\,X = \\bf{x}), i.e., devemos encontrar meios de obter \\widehat{Y}.\n\n\n\nAlgumas observa√ß√µes de nomenclaturas:\n\n√â comum chamar cada exemplo de dados, i.e., o vetor \\bf{x} que ser√° passado ao modelo de atributos ou features;\nTamb√©m √© comum chamarmos de r√≥tulo ou label a classe ou valor alvo, ou seja, estas s√£o as formas de nomearmos Y, sendo Y uma quantidade num√©rica (modelos de regress√£o) ou n√£o (modelos de classifica√ß√£o)."
  },
  {
    "objectID": "index.html#aprendizagem-supervisionada-1",
    "href": "index.html#aprendizagem-supervisionada-1",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Aprendizagem supervisionada",
    "text": "Aprendizagem supervisionada\n\nEm se tratando de m√©todos de classifica√ß√£o, podemos ter os m√©todos:\n\n\nGenerativos: s√£o os m√©todos que dada as vari√°veis X e Y, o objetivo √© encontrar a distribui√ß√£o de probabilidade conjunta P(X, Y), para ent√£o poder determinar P(Y\\, | \\, X = \\bf{x}). Alguns m√©todos s√£o:\n\nNaive Bayes;\nDescriminante linear.\n\n\n\n\nDescriminativos: s√£o os m√©todos que estimam diretamente a probabilidade condicional P(Y \\, | \\, X = \\bf{x}) ou que mesmo nem assumem modelos probabil√≠sticos. Podemos citar:\n\nRegress√£o logistica;\nPerceptron;\nSupport Vector Machine - SVM."
  },
  {
    "objectID": "index.html#aprendizagem-supervisionada-2",
    "href": "index.html#aprendizagem-supervisionada-2",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Aprendizagem supervisionada",
    "text": "Aprendizagem supervisionada\n\n\n\n\n\n Poder√≠amos estar interessados em classificar o tamanho de morangos:\n\n\nS (Slow): pequeno;\nM (Medium): m√©dio;\nL (Large): grande."
  },
  {
    "objectID": "index.html#aprendizagem-supervisionada-3",
    "href": "index.html#aprendizagem-supervisionada-3",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Aprendizagem supervisionada",
    "text": "Aprendizagem supervisionada\n \n\nMais dois problemas de classifica√ß√£o (linear x n√£o-linear)."
  },
  {
    "objectID": "index.html#aprendizagem-supervisionada-4",
    "href": "index.html#aprendizagem-supervisionada-4",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Aprendizagem supervisionada",
    "text": "Aprendizagem supervisionada\n \n\nUm exemplo de de um problema de regress√£o. Aqui, a ideia √© utilizar a equa√ß√£o da reta estimada, a reta que minimiza a soma dos quadrados entre a reta e os ponto seria a melhor, de modo a ter uma estimativa num√©rica atrav√©s de novos atributos passado ao modelo, i.e., por meio da equa√ß√£o da reta e de um novo valor de x."
  },
  {
    "objectID": "index.html#aprendizagem-supervisionada-5",
    "href": "index.html#aprendizagem-supervisionada-5",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Aprendizagem supervisionada",
    "text": "Aprendizagem supervisionada\n\nUm outro exemplo seria a classifica√ß√£o de imagem/v√≠deo, utilizando um algoritmo de rede neural, por exemplo, usando uma Convolutional Neural Network - CNN. Foram utilizados diversas imagens de pessoas ‚Äúcom‚Äù e ‚Äúsem‚Äù m√°scara. Em que ‚Äúcom‚Äù representa detec√ß√£o da m√°scara na face da pessoa e ‚Äúsem‚Äù a n√£o detec√ß√£o."
  },
  {
    "objectID": "index.html#aprendizagem-n√£o-supervisionada",
    "href": "index.html#aprendizagem-n√£o-supervisionada",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Aprendizagem n√£o-supervisionada",
    "text": "Aprendizagem n√£o-supervisionada\n\nNesse tipo de aprendizagem, os algoritmos trabalham sobre dados n√£o rotulados, por exemplo, em uma trarefa de agrupamento.\n\nOs algoritmos verificam se as inst√¢ncias observadas poder√£o ser arranjadas de alguma maneira, por exemplo, usando alguma m√©trica de dist√¢ncia, formando grupos (clusters).\n\nA ideia √© maximizar a dist√¢ncia entre os clusters e minimizar a dist√¢ncia entre os elementos no interrior do grupo. Em outras palavras, o que se quer √© tornar os grupos mais diferentes poss√≠veis e tornar os elementos dos grupos o mais parecido poss√≠vel.\n\nAqui, por n√£o haver r√≥tulos, um problema comum √© determinar a quantidade de grupos ideal que muitas vezes s√£o obtidos de forma subjetiva ou por heur√≠sticas. A quantidade de grupos √© um dilema!"
  },
  {
    "objectID": "index.html#aprendizagem-n√£o-supervisionada-1",
    "href": "index.html#aprendizagem-n√£o-supervisionada-1",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Aprendizagem n√£o-supervisionada",
    "text": "Aprendizagem n√£o-supervisionada\n\n\nAp√≥s a detec√ß√£o dos grupos, √© preciso analisar o resultado de modo a tentar extrair informa√ß√µes coerentes de modo a saber o que cada grupo representa no problema em quest√£o."
  },
  {
    "objectID": "index.html#aprendizagem-semi-supervisionada",
    "href": "index.html#aprendizagem-semi-supervisionada",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Aprendizagem semi-supervisionada",
    "text": "Aprendizagem semi-supervisionada\n\nA aprendizagem semi-supervisionada √© uma abordagem na √°rea de aprendizagem de m√°quina onde um algoritmo utiliza tanto dados rotulados quanto n√£o rotulados para treinamento. Por exemplo, algoritmos que propagam r√≥tulos, como o Label Propagation, em que r√≥tulos conhecidos s√£o propagados para dados n√£o rotulados com base em sua sua proximidade no espa√ßo de caracter√≠sticas.\n\nUma outra abordagem seria misturar modelos (Model Blending), em que diferentes modelos s√£o treinados em diferentes partes do conjunto de dados, por exemplo, um modelo para a parte roturada e um para a parte n√£o rotulada."
  },
  {
    "objectID": "index.html#aprendizagem-por-refor√ßo",
    "href": "index.html#aprendizagem-por-refor√ßo",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Aprendizagem por refor√ßo",
    "text": "Aprendizagem por refor√ßo\n\nNesse tipo de aprendizagem, n√£o h√° uma fonte externa de exemplos. O agente (modelo) aprende aprende com sua pr√≥pria experi√™ncia, por tentativas e erros, em que voc√™ dever√° definir uma medida de sucesso, e eventualmente recompensar os acertos. No v√≠deo abaixo, veja um joguinho que criei em R, onde o carrinho aprendeu a desviar de obst√°culos aleat√≥rios que aparecem em sua frente. Utilizou-se uma rede neural cuja a sa√≠da poderia ser (‚Äúparado‚Äù, ‚Äúpara cima‚Äù ou ‚Äúpara baixo‚Äù). Veja o c√≥digo clicando aqui."
  },
  {
    "objectID": "index.html#dados-explora√ß√£o-e-tratamentom",
    "href": "index.html#dados-explora√ß√£o-e-tratamentom",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Dados: explora√ß√£o e tratamentom",
    "text": "Dados: explora√ß√£o e tratamentom\n\nUm dos passos mais importante no fluxo de trabalho (workflow) de um modelo de aprendizagem de m√°quina, consiste na prepara√ß√£o dos dados, onde realizamos transforma√ß√µes, inputa√ß√µes de valores ausentes, identifica√ß√£o de outliers, remo√ß√£o de vari√°veis altamente correlacionadas, entre outros.\n\nFazer uma an√°lise explorat√≥ria dos dados √© um passo importante para que se possa entender e detecatar poss√≠veis inconsist√™ncias na base de dados. N√£o adianta fazer uso de modelos muito sofisticados quando se tem uma base de dados cheia de problemas.\n\nNormalmente trabalhamos com juntos de dados (tabelas) relacionais, em que cada linha √© uma observa√ß√£o e cada coluna representa um atributo do objeto/observa√ß√£o. A linha de uma base de dados relacional, sem sua a vari√°vel de interesse, lembre-se que denominamos Y de r√≥tulo ou label, fornece o vetor de caracter√≠sticas \\bf{x} que descreve uma dada observa√ß√£o."
  },
  {
    "objectID": "index.html#dados-explora√ß√£o-e-tratamento",
    "href": "index.html#dados-explora√ß√£o-e-tratamento",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Dados: explora√ß√£o e tratamento",
    "text": "Dados: explora√ß√£o e tratamento\nNo artigo Tidy Data, 2014, publicado no Journal of Statistical Sofware, o Hadley Wickham discute que o princ√≠pio de dados organizados est√£o intimamente relacionados com banco de dados relacional e mais pr√≥ximo do recioc√≠nio que empregamos na √°lgebra. Nesse artigo, ele define o que √© Tidy Dados, sendo essa uma maneira de mapear um conjunto de dados.\n\nSegundo o artigo, um conjunto de dados √© bagun√ßado ou arrumado/tidy, dependendo de como as linhas, colunas e tabelas s√£o combinadas com as observa√ß√µes, vari√°veis e tipos. Em dados arrumados (dados tidy), temos que:\n\n\nCada vari√°vel forma uma coluna;\nCada observa√ß√£o forma uma linha;\nCada valor deve ter sua pr√≥pria c√©lula.\n\n\nEmbora existam situa√ß√µes em que j√° podemos come√ßar a analisar uma base de dados real, essa √© a exce√ß√£o e n√£o a regra. Normalmente, nos deparamos com bases de dados que violam uma ou mais dessas regras. Sempre, que poss√≠vel, procure utilizar dados no formato Tidy."
  },
  {
    "objectID": "index.html#dados-explora√ß√£o-e-tratamento-1",
    "href": "index.html#dados-explora√ß√£o-e-tratamento-1",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Dados: explora√ß√£o e tratamento",
    "text": "Dados: explora√ß√£o e tratamento\n\n\nRepresenta√ß√£o de uma base de dados no formato tidy."
  },
  {
    "objectID": "index.html#dados-explora√ß√£o-e-tratamento-2",
    "href": "index.html#dados-explora√ß√£o-e-tratamento-2",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Dados: explora√ß√£o e tratamento",
    "text": "Dados: explora√ß√£o e tratamento\n\n\n‚ÄúAs fam√≠lias felizes s√£o todas iguais; toda fam√≠lia infeliz √© infeliz √† sua maneira.‚Äù ‚Äì Leo Tolstoy\n\n\n‚ÄúConjuntos de dados organizados s√£o todos iguais, mas todo conjunto de dados confuso √© confuso √† sua maneira.‚Äù ‚Äì Hadley Wickham\n\n\n\nTrabalhar com a Tabela do lado esquerdo √© melhor que a Tabela do lado direito. Prefira, sempre que poss√≠vel, o formato tidy. N√£o permita-se ficar estressado t√£o facilmente."
  },
  {
    "objectID": "index.html#dados-explora√ß√£o-e-tratamento-3",
    "href": "index.html#dados-explora√ß√£o-e-tratamento-3",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Dados: explora√ß√£o e tratamento",
    "text": "Dados: explora√ß√£o e tratamento\nA linguagem de programa√ß√£o R possue diversas ferramentas que permite manipular e explorar bases de dados. Enumero algumas:\n\ndplyr: biblioteca que implementa √© uma gram√°tica de manipula√ß√£o de dados, fornecendo um conjunto consistente de verbos que ajudam a resolver os desafios mais comuns de manipula√ß√£o de dados;\ntidyr: ferramentas para ajudar a criar dados organizados, onde cada coluna √© uma vari√°vel, cada linha √© uma observa√ß√£o e cada c√©lula cont√©m um √∫nico valor;\nggplot2: um sistema para criar gr√°ficos ‚Äòdeclarativamente‚Äô, baseado no livro The Grammar of Graphics, de Leland Wilkinson;\nvisdat: uma biblioteca √∫til para um visualiza√ß√£o explorat√≥ria preliminar de dados;\nexplore: biblioteca que apresenta algumas rotinas de an√°lise para realizar uma an√°lise explorat√≥ria nos dados.\n\nTodas essas bibliotecas est√£o muito bem documentadas. √â importante que voc√™s explorem as documentas dessas bibliotecas, pois eventualmente irei utizar alguma delas."
  },
  {
    "objectID": "index.html#dados-explora√ß√£o-e-tratamento-4",
    "href": "index.html#dados-explora√ß√£o-e-tratamento-4",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Dados: explora√ß√£o e tratamento",
    "text": "Dados: explora√ß√£o e tratamento\n\nNo Cap√≠tulo 12, do livro R for Data Science, o autor aborda mais sobre o formato Tidy e como trabalhar com a biblioteca tidyr. Aqui o autor aborda de forma b√°sica o pacote dplyr.\n\nDurante o curso, na medida da necessidade de utiliza√ß√£o dessas ferramentas, durante a exposi√ß√£o de exemplos, abordaremos alguns conceitos. Ok?"
  },
  {
    "objectID": "index.html#dados-explora√ß√£o-e-tratamento-5",
    "href": "index.html#dados-explora√ß√£o-e-tratamento-5",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Dados: explora√ß√£o e tratamento",
    "text": "Dados: explora√ß√£o e tratamento\n\nMuitas vezes, no processo de tratamento dos dados, tamb√©m estamos preocupados em remover atributos que n√£o s√£o significativo para a modelagem, em que nesse momento a experi√™ncia dos especialistas s√£o fundamentais.\n\n√â comum enriquercermos a base de dados com informa√ß√µes de outras bases de dados, em um sistema de gerenciamento de banco de dados relacional, em que as bases de dados est√£o relacionadas por uma chave. Nesse caso, buscamos por novos atributos para um mesmo objeto (para uma mesma linha da base), em que atributos cruzados devem ter um √∫nico valor, para cada objeto, respeitando a regra tr√™s de conjuntos de dados tidy.\n\nAs vezes transformamos vari√°veis. Por exemplo, √© comum tomar o logaritmo de uma vari√°vel num√©rica que √© assim√©trica, se x &gt;= 1, em que x √© um atributo num√©rico qualquer.\n\nEm diveras situa√ß√µes, tamb√©m √© comum a base de dados apresentar informa√ß√µes faltantes. Nos data frames de R, a falta de informa√ß√£o na base, normlamente ser√£o representadas por NA."
  },
  {
    "objectID": "index.html#dados-explora√ß√£o-e-tratamento-6",
    "href": "index.html#dados-explora√ß√£o-e-tratamento-6",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Dados: explora√ß√£o e tratamento",
    "text": "Dados: explora√ß√£o e tratamento\n\nPoder√° ser que um dado atributo apresente informa√ß√£o faltante, e normalmente n√£o optaremos em remover a observa√ß√£o e precisaremos imputar a informa√ß√£o, por exemplo:\n\n\nTomando alguma medida de tend√™ncia central como m√©dia/moda/mediana dos valores que s√£o conhecidos para aquele atributo;\nCriar um novo valor que √© indica√ß√£o de valor faltante;\nUsar algoritmos como k-nearest neighbors - KNN (k vizinhos mais pr√≥ximos) para imputar valores coerentes;\nInterpolar os dados.\n\n\nEsses s√£o alguns exemplos de como podemos imputar observa√ß√µes faltantes. Muitas vezes n√£o podemos nos dar o luxo de percer observa√ß√µes de nossa base de dados."
  },
  {
    "objectID": "index.html#dados-explora√ß√£o-e-tratamento-7",
    "href": "index.html#dados-explora√ß√£o-e-tratamento-7",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Dados: explora√ß√£o e tratamento",
    "text": "Dados: explora√ß√£o e tratamento\n\n√â comum ser necess√°rio transformar os dados:\n\nPode ser necess√°rio transformar os tipos ou os valores dos atributos para tentar obter um melhor ajuste do modelo;\nPode-se discretizar valores cont√≠nuos ou transform√°-los em intervalos;\n√â comum transformar atributos categ√≥ricos com p categorias, em p novos atributos bin√°rios.\n\nOne-hot encoding\nVari√°veis dummy\n\nOutra transforma√ß√£o muito comum √© a normaliza√ß√£o dos dados. Normalizar os dados √© muito √∫til quando os atributos num√©ricos possuem escalas muito diferentes.\n\n\n\nX_{novo} = \\frac{X - X_{min}}{X_{max} - X_{min}}, em que X_{novo} \\in [0, 1].\n\nX_{novo} = Z = \\frac{X - \\mu}{\\sigma^2}, em que \\mathbb{E}(X) = \\mu √© a m√©dia dos dados e \\mathrm{Var}(X) = \\sigma^2. Na pr√°tica, em um contexto de v.a., iids, usamos \\overline{x} como estimador de \\mu e S^2 (vari√¢ncia amostral) como estimador de \\sigma^2."
  },
  {
    "objectID": "index.html#dados-explora√ß√£o-e-tratamento-8",
    "href": "index.html#dados-explora√ß√£o-e-tratamento-8",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Dados: explora√ß√£o e tratamento",
    "text": "Dados: explora√ß√£o e tratamento\n\nLembre-se, como citado anteriormente, tomar o logaritmo natural, ou mesmo na base 10 de vari√°veis num√©ricas muito assim√©tricas, poder√° ajudar um pouco, desde que seja possivel tomar o \\log(\\cdot).\n\n\n\n\nset.seed(0)\nrgamma(1000, 2, 2) |&gt; \n  hist()\n\n\n\n\n\n\nset.seed(0)\nrgamma(1000, 2, 2) |&gt; \n  log() |&gt; hist()"
  },
  {
    "objectID": "index.html#dados-explora√ß√£o-e-tratamento-9",
    "href": "index.html#dados-explora√ß√£o-e-tratamento-9",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": " Dados: explora√ß√£o e tratamento",
    "text": "Dados: explora√ß√£o e tratamento\n\nAnteriormente eu citei algumas bibliotecas √∫teis de R para explorar os dados, na fase de tratamento das observa√ß√µes. Por√©m, n√£o estranhe n√£o ter cidado bibliotecas do framework tidymodels, em especial o recipes que √© muito utilizado no workflow de aprendizagem de m√°quina na fase de pr√©-processamento dos dados. Muitas dessas transforma√ß√µes s√£o aplicadas como receitas de pr√©-processamento com o pacote recipes.\n\nO tidymodels ser√° muito √∫til para n√≥s, mas, aos poucos, seu uso e explica√ß√µes mais detalhadas ser√£o apresentadas, apesar que em algumas situa√ß√µes mais simples, poderei n√£o utiliz√°-lo, expor detalhes que eventualmente n√£o ser√° poss√≠vel ou estariam camuflados na utiliza√ß√£o do tidymodels."
  },
  {
    "objectID": "index.html#as-duas-culturas",
    "href": "index.html#as-duas-culturas",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "As duas culturas",
    "text": "As duas culturas\n\nEm Breiman, L. (2001a). Statistical modeling: The two cultures. Statistical Science, 16(3), 199‚Äì231, o Leo Breiman argumenta que existe duas culturas no uso de modelos estat√≠sticos, em especialmente na √°rea de modelos de regress√£o. Segundo eles, as culturas s√£o:\n\n\nData modeling culture: nela, em geral, se assume que o modelo de regress√£o utilizado r(x), por exemplo, r(x) = \\beta_0 + \\sum_{i = 1}^d \\beta_ix_i √© correto. O principal objetivo dessa abordagem √© a interpreta√ß√£o dos par√¢metros que indexam o modelo r(x). Nesse tipo de cultura, a ideia tamb√©m √© construir intervalos aleat√≥rios e testar hip√≥teses para os \\beta_i's. Sob essa √≥tica, muitas suposi√ß√µes sob o modelo s√£o realizadas, em que formas para checar essas suposi√ß√µes s√£o desenvolvidas, uma vez que elas s√£o fundamentais para esse tipo de modelagem.\n\n\n\nAlgorithmic modeling culture: essa √© a cultura que domina a comunidade de aprendizagem de m√°quina. Nessa abordagem, o principal objetivo s√£o as predi√ß√µes por meio de novas observa√ß√µes. N√£o se assume que o modelo utilizado √© o modelo correto. Nesse tipo de modelagem, muitas vezes os algoritmos n√£o envolve nenhuma estrutura probabil√≠stica. Muitas vezes, modelos n√£o bem especificado conduzem a boas predi√ß√µes."
  },
  {
    "objectID": "index.html#as-duas-culturas-1",
    "href": "index.html#as-duas-culturas-1",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "As duas culturas",
    "text": "As duas culturas\n\n\n\n\n\n\nBreiman, L. (2001a). Statistical modeling: The two cultures. Statistical Science, 16(3), 199‚Äì231.\n\n\n\n\n\n\nLeo como um probabilista jovem na Universidade da Calif√≥rina."
  },
  {
    "objectID": "index.html#as-duas-culturas-2",
    "href": "index.html#as-duas-culturas-2",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "As duas culturas",
    "text": "As duas culturas\n\nH√° diversos artigos interessantes que s√£o respostas ao artigo do Leo Breiman, como por exemplo, o artigo Statistical Modeling: The Two Cultures: Comment do David Cox e com coment√°rios do Brad Efron.\n\nSir David Cox."
  },
  {
    "objectID": "index.html#as-duas-culturas-3",
    "href": "index.html#as-duas-culturas-3",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "As duas culturas",
    "text": "As duas culturas\n\nMuito embora exista essa divis√£o entre as culturas, Breiman foi um estat√≠stico que desempenhou um grande trabalho para unir a √°rea de estat√≠stica com aprendizado de m√°quina. Por conta dessa grande import√¢ncia, um pr√™mio concedido em sua homenagem foi criado pela American Statistical Association.\n\nLeo Breiman trabalhando em sua resid√™ncia, em 1985."
  },
  {
    "objectID": "index.html#regress√£o",
    "href": "index.html#regress√£o",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Regress√£o",
    "text": "Regress√£o\n\nM√©todos de regress√£o surgiram h√° mais de dois s√©culos com Legendre (1805) e Gauss (1809), que exploraram o m√©todo dos m√≠nimos quadrados com o objetivo de prever √≥rbitas ao redor do Sol. Hoje em dia, o problema de estima√ß√£o de uma fun√ß√£o de regress√£o possui papel central em estat√≠stica.\n\n\nApesar de as primeiras t√©cnicas para solucionar esse problema datarem de ao menos 200 anos, os avan√ßos computacionais recentes permitiram que novas metodologias fossem exploradas. Em particular, com a capacidade cada vez maior de armazenamento de dados, m√©todos com menos suposi√ß√µes sobre o verdadeiro estado da natureza ganham cada vez mais espa√ßo. Com isso, v√°rios desafios surgiram: por exemplo, m√©todos tradicionais n√£o s√£o capazes de lidar de forma satisfat√≥ria com bancos de dados em que h√° mais covari√°veis que observa√ß√µes, uma situa√ß√£o muito comum nos dias de hoje. Similarmente, s√£o frequentes as aplica√ß√µes em que cada observa√ß√£o consiste em uma imagem ou um documento de texto, objetos complexos que levam a an√°lises que requerem metodologias mais elaboradas. ‚Äì Izbick et al."
  },
  {
    "objectID": "index.html#regress√£o-1",
    "href": "index.html#regress√£o-1",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Regress√£o",
    "text": "Regress√£o\n\nDe forma geral, temos que o objetivo de um modelo de regress√£o √© determinar a rela√ß√£o entre uma vari√°vel aleat√≥ria (label) Y \\in \\mathbb{R} e um vetor de covari√°veis (features) \\mathbf{x} = (x_1, \\cdots, x_d) \\in \\mathbb{R}^d. Mais especificamente, busaca-se estimar\nr(\\bf{x}) := \\mathbb{E}(Y\\,|\\,\\bf{X} = \\bf{x}),\nsendo esta chamada de fun√ß√£o de regress√£o. Temos que:\n\n\nSe Y √© uma vari√°vel quantitativa, ent√£o estamos sob um problema de regress√£o;\nSe Y √© uma vari√°vel qualitativa, ent√£o teremos um problema de classifica√ß√£o.\n\nEm aprendizagem de m√°quina, assumimos que n√£o temos meios de calcular r({\\bf{x}}), i.e., n√£o conhecemos a distribui√ß√£o condicional de {\\bf{Y}\\,|\\,X}. Portanto, n√£o temos meios de calcular\n\\mathbb{E}({\\bf X}|Y = y) = \\int x\\,\\mathrm{d}F_{\\bf X}({\\bf x} | Y = y)."
  },
  {
    "objectID": "index.html#nota√ß√µes",
    "href": "index.html#nota√ß√µes",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Nota√ß√µes",
    "text": "Nota√ß√µes\n\nA vari√°vel Y recebe frequentemente o nome de vari√°vel resposta, vari√°vel dependente, r√≥tulo ou label. J√° as observa√ß√µes contidas no vetor \\bf{x} = (x_1, \\cdots, x_d), s√£o, em geral, denominadas de vari√°veis explicativas, vari√°veis independentes, caracter√≠sticas, atributos, preditores, covari√°veis ou features.\n\nA ideia, nessa primeira parte do curso, √© descrever algumas t√©cnicas para estimar (treinar, como √© dito em aprendizagem de m√°quina) r(\\bf{x}).\n\nA menos quando dito o contr√°rio, assumiremos que nossa amostra s√£o i.i.d. (independentes e identicamente distribu√≠das), ou seja, (\\bf{X}_1, Y_1), \\cdots, (\\bf{X}_n, Y_n) s√£o i.i.d.\n\nDenota-se por x_{i,j} o valor da j-√©sima covari√°vel na i-√©sima amostra, com j = 1, \\cdots, d e i = 1, \\cdots, n."
  },
  {
    "objectID": "index.html#nota√ß√µes-1",
    "href": "index.html#nota√ß√µes-1",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Nota√ß√µes",
    "text": "Nota√ß√µes\n\n\nNota√ß√£o utilizada nesse material para as vari√°veis envolvidas em um problema de regress√£o.\n\n\n\n\n\n\nLabel\nFeatures\n\n\n\n\nY_1\nX_{1,1},\\cdots, X_{1,d}\\,\\,\\, (= \\bf{X}_1)\n\n\n\\vdots\n\\,\\,\\,\\vdots\\,\\,\\,\\,\\, \\ddots\\,\\,\\ \\vdots\n\n\nY_n\nX_{n,1},\\cdots, X_{n,d}\\,\\,\\, (= \\bf{X}_n)"
  },
  {
    "objectID": "index.html#regress√£o-2",
    "href": "index.html#regress√£o-2",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Regress√£o",
    "text": "Regress√£o\nNossa ideia √© construir uma boa estimativa g da fun√ß√£o de regress√£o r(\\bf{x}) := \\mathbb{E}(Y\\,|\\,\\bf{X} = \\bf{x}), para novas observa√ß√µes, i.e., queremos obter uma fun√ß√£o g, tal que:\ng: \\mathbb{R}^d \\rightarrow \\mathbb{R},\nde tal forma que g possua um bom poder preditivo. Em aprendizagem de m√°quina, s√≥ estaremos interessados em obter uma fun√ß√£o g que estime bem um n√∫mero real (em problemas de regress√£o), ou que classifique bem (em um problema de classifica√ß√£o), utilizando as d covari√°veis. Ou seja, para m novas observa√ß√µes, desejamos obter g, que\ng({\\bf{x}}_{n + 1}) \\approx y_{n + 1}, \\cdots, g({\\bf{x}}_{n + m}) \\approx y_{n + m}."
  },
  {
    "objectID": "index.html#fun√ß√£o-de-risco",
    "href": "index.html#fun√ß√£o-de-risco",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Fun√ß√£o de risco",
    "text": "Fun√ß√£o de risco\n\nPara que possamos construir boas fun√ß√µes de predi√ß√£o, √© preciso que tenhamos um crit√©rio para medir o desempenho de uma dada fun√ß√£o g:\\mathbb{R}^d \\rightarrow \\mathbb{R}. Em contexto de regress√£o, usaremos o risco quadr√°tico, muito embora esta n√£o √© a √∫nica op√ß√£o. Denotaremos a fun√ß√£o de risco quadr√°tico por:\nR_{pred}(g) = \\mathbb{E}\\left[({\\bf Y} - g({\\bf X}))^2\\right], em que (\\bf X, Y) s√£o observa√ß√µes novas que n√£o foram utilizadas para treinar/estimar g. L√™-se R_{pred}(g) como ‚Äúrisco preditivo de g‚Äù. Note que, como \\bf X s√£o observa√ß√µes conhecidas e g(\\cdot) √© um modelo preditivo, portanto, g √© conhecido, ent√£o, \\widehat{\\bf Y} = g(\\bf X) √© um estimador dos labels, i.e., de \\bf Y.\n\nDiremos que L(g({\\bf X}); {\\bf Y}) = ({\\bf Y} - g({\\bf X}))^2 √© a fun√ß√£o de perda quadr√°tica, as vezes chamado de perda L_2. Outra fun√ß√µes como a fun√ß√£o de perda absoluta denotada por L(g({\\bf X}); {\\bf Y}) = |{\\bf Y} - g({\\bf X})|, as vezes chamada de perda L_1 poderiam ser consideradas."
  },
  {
    "objectID": "index.html#fun√ß√£o-de-risco-1",
    "href": "index.html#fun√ß√£o-de-risco-1",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Fun√ß√£o de risco",
    "text": "Fun√ß√£o de risco\n\nEm linhas gerais, seja L(\\cdot) uma fun√ß√£o qualquer, tal que \\forall \\, 0 &lt; u &lt; v, de modo que:\n\n\n0 = L(0) \\leq L(u) \\leq L(v);\n0 = L(0) \\leq L(-u) \\leq L(-v).\n\n\nQualquer fun√ß√£o L(\\cdot) que satisfaz as propriedades acima √© chamada de fun√ß√£o de perda. Em especial, temos que:\n\n\nFun√ß√£o de perda quadr√°tica: L(u) = u^2;\nFun√ß√£o de perda absoluta: L(u) = |u|;\nFun√ß√£o de perda degradu: L(0) = 0, se |u| &lt; \\delta e 1 caso contr√°rio, para algum \\delta &gt; 0;"
  },
  {
    "objectID": "index.html#fun√ß√£o-de-risco-2",
    "href": "index.html#fun√ß√£o-de-risco-2",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Fun√ß√£o de risco",
    "text": "Fun√ß√£o de risco\n\nNormalmente considera-se a perda L_2, uma vez que em modelos de regress√£o, minimizar R_{pred}(g), em g, equivale a encontrar r({\\bf x}) = \\mathbb{E}({\\bf X}|{\\bf Y}), i.e., equivale a estimar a fun√ß√£o de regress√£o.\n\nTeorema: Suponha que definimos o risco de uma fun√ß√£o de predi√ß√£o g: \\mathbb{R}^d \\rightarrow \\mathbb{R} via fun√ß√£o perda quadr√°tica, i.e, R_{pred}(g) = \\mathbb{E}\\left[({\\bf Y} - g({\\bf X}))^2\\right], em que \\bf (X, Y) s√£o novas observa√ß√µes que n√£o foram utilizadas para estimar g. Suponha tamb√©m que estimaos o risco de um estimador de regress√£o r({\\bf X}) via fun√ß√£o perda quadr√°tica R_{pred}(g) = \\mathbb{E}\\left[r({\\bf X} - g({\\bf X}))^2\\right]. Ent√£o,\nR_{pred}(g) = R_{reg}(g) + \\mathbb{E}\\left[\\mathbb{V}[{\\bf Y} | {\\bf X}]\\right],\nem que \\mathbb{E}\\left[\\mathbb{V}[{\\bf Y} | {\\bf X}]\\right] √© a vari√¢ncia m√©dia do modelo que n√£o depende de g. Portanto, estimar bem r({\\bf x}) √© de fundamental import√¢ncia para criar uma boa fun√ß√£o de predi√ß√£o. Em especial, sob a √≥tica do risco quadr√°tico, a melhor fun√ß√£o de predi√ß√£o para \\bf Y √© a fun√ß√£o de regress√£o r({\\bf x}), de tal modo que:\n\\argmin_g R_{pred}(g) = \\argmin_g R_{reg}(g) = r({\\bf x})."
  },
  {
    "objectID": "index.html#fun√ß√£o-de-risco-3",
    "href": "index.html#fun√ß√£o-de-risco-3",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Fun√ß√£o de risco",
    "text": "Fun√ß√£o de risco\n\nLembre-se: r({\\bf x}) = \\mathbb{E}(Y | \\bf{X} = \\bf{x}) √© a nossa fun√ß√£o de regress√£o.\n\nA defini√ß√£o de risco preditivo R_{pred}, que tamb√©m denotaremos simplesmente por R, tem um apelo frequentista. Dessa forma, para um novo conjunto com m novas observa√ß√µs, ({\\bf X}_{n+1}, Y_{n+1}), \\cdots, ({\\bf X}_{n+m}, Y_{n+m}), temos que que essa nova amostra √© i.i.d √† amostra observada (utilizada no treinamento do modelo/na estima√ß√£o). Ent√£o, pela Lei dos Grandes N√∫meros, temos que um bom estimador para a fun√ß√£o para o risco preditivo √© dado por:\n\\frac{1}{m}\\sum_{i = 1}^m (Y_{n + i} - g(X_{n + i}))^2 \\approx R(g) := \\mathbb{E}\\left[(Y - g({\\bf X}))^2\\right]. Chamaremos a quantidade acima de Erro Quadr√°tico M√©dio - EQM. Em aprendizagem de m√°quina, normalmente estaremos no contexto em que temos muitas observa√ß√µes, e que portanto, poderemos fazer esse apelo frequentista.\n\n‚Äô‚ÄôDesejamos encontrar g (encontrar m√©todos) que minimize de forma satisfat√≥ria R, i.e., m√©todos que nos conduzam √† um risco baixo."
  },
  {
    "objectID": "index.html#fun√ß√£o-de-risco-4",
    "href": "index.html#fun√ß√£o-de-risco-4",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Fun√ß√£o de risco",
    "text": "Fun√ß√£o de risco\n\nSendo assim, se R(g) possui um valor baixo, ent√£o, temos que\ng({\\bf x}_{n+1}) \\approx y_{n+1}, \\cdots, g({\\bf x}_{n+m}) \\approx y_{n+m}."
  },
  {
    "objectID": "index.html#regress√£o-linear",
    "href": "index.html#regress√£o-linear",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Regress√£o linear",
    "text": "Regress√£o linear\n\nNesse momento, vamos pensar um pouco em regress√£o linear. No caso mais simples, queremos prever o comportamento de uma vari√°vel de interesse Y condicional a uma vari√°vel explicativa X (regress√£o linear simples, i.e., d = 1). O melhor preditor de Y condicional em X √© aquele que minimiza a fun√ß√£o de perda esperada, ou seja, √© aquele que resolve:\n\\argmin_g \\mathbb{E}(L(Y - g)\\,|\\,X).\nPara o caso da fun√ß√£o perda quadr√°tica (fun√ß√£o L_2), o melhor preditor de Y condicional √† X √© a m√©dia condicional de Y dado X, i.e., r(X) = \\mathbb{E}(Y\\,|\\,X). J√°, na situa√ß√£o em que considera-se a perda absoluta (fun√ß√£o L_1), o melhor estimador √© a mediana condicional.\n\nOs modelos de regress√£o, em geral, fazem uso da fun√ß√£o de perda quadr√°tica."
  },
  {
    "objectID": "index.html#regress√£o-linear-simples",
    "href": "index.html#regress√£o-linear-simples",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Regress√£o linear simples",
    "text": "Regress√£o linear simples\n\nNo caso da regress√£o linear simples (d = 1), temos que o modelo √© dado por:\ng(x) = \\beta_0 + \\beta_1 x_{i,1} + \\varepsilon_i, \\,\\, i = 1, \\cdots, n.\nAssumindo que a regress√£o linear simples √© o modelo g que iremos utilizar, ent√£o, desejamos minimizar:\n\\argmin_{\\beta} R(g_\\beta) = \\argmin_{\\beta} \\sum_{i = 1}^n(y_i - \\beta_0 - \\beta_1x_{i,1})^2. Derivando em rela√ß√£o √† \\beta e igualando a zero, ap√≥s algumas manipula√ß√µes alg√©bricas, temos que:\n\\widehat{\\beta} = \\frac{\\sum_{i = 1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^n(x_i - \\overline{x})^2} = r_{xy}\\frac{s_y}{s_x}, em que s_x e s_y s√£o os desvio-padr√£o de x e y, respectivamente, e r_{xy} √© o coeficiente de correla√ß√£o da amostra."
  },
  {
    "objectID": "index.html#regress√£o-linear-simples-1",
    "href": "index.html#regress√£o-linear-simples-1",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Regress√£o linear simples",
    "text": "Regress√£o linear simples\n\nr_{xy} = \\frac{\\overline{xy} - \\overline{x}\\,\\overline{y}}{\\sqrt{(\\overline{x^2} - \\overline{x}^2)(\\overline{y^2} - \\overline{y}^2)}}. O coeficiente de determina√ß√£o R^2 do modelo √© dado por r_{xy}^2, quando o modelo √© linear e possue uma √∫nica vari√°vel independente (feature).\n\nPortanto, temos que:\n\\widehat{\\beta_0} = \\overline{y} - \\widehat{\\beta}\\overline{x},\nNa data modeling culture (na estat√≠stica), normalmente assumimos que o \\varepsilon_i tem distribui√ß√£o normal e vari√¢ncia constante, \\forall\\, i = 1, \\cdots, n. Assume-se tamb√©m que \\mathbb{E}(\\varepsilon_i) = 0, \\, \\forall i."
  },
  {
    "objectID": "index.html#regress√£o-linear-simples-2",
    "href": "index.html#regress√£o-linear-simples-2",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Regress√£o linear simples",
    "text": "Regress√£o linear simples\n\nAqui n√£o iremos nos preocupar com essas suposi√ß√µes, uma vez que em algorithmic modeling culture, n√£o estamos preocupados com suposi√ß√µes nem interpreta√ß√µes, ok!?"
  },
  {
    "objectID": "index.html#regress√£o-linear-multipla",
    "href": "index.html#regress√£o-linear-multipla",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Regress√£o linear multipla",
    "text": "Regress√£o linear multipla\n\nA fun√ß√£o de perda quadr√°tica (fun√ß√£o L_2) tem algumas vantagens em rela√ß√£o a fun√ß√£o de perda absoluta. Listo algumas:\n\nA fun√ß√£o de perda quadr√°tica penaliza mais os erros maiores, devido ao vato dos erros serem levado ao quadrado;\nA fun√ß√£o de perda quadr√°tica √© mais sens√≠vel a presen√ßa de outlier, que em compensa√ß√£o s√£o menos penalizados ao se considerar a fun√ß√£o de perda absoluta (fun√ß√£o L_1);\nEm situa√ß√µes em que o erro tem distribui√ß√£o normal, a estimativa de m√≠nimos quadrados √© a solu√ß√£o de m√°xima verossimilhan√ßa e √© a estimativa linear n√£o viesada e com menor vari√¢ncia. Portanto, gozamos de um estimador com √≥timas propriedades, muito embora ele tamb√©m √© um bom estimador mesmo quando a suposi√ß√£o de normalidade n√£o √© verificada;\nA fun√ß√£o de perda quadr√°tica √© deferenci√°vel, j√° a fun√ß√£o de perda absoluta n√£o √©.\n\nPara o caso de regerss√£o linear m√∫ltipla, i.e., quando d &gt; 1, poderemos utilizar uma nota√ß√£o matricial para representar o modelo linear m√∫ltiplo de regress√£o."
  },
  {
    "objectID": "index.html#regress√£o-linear-multipla-1",
    "href": "index.html#regress√£o-linear-multipla-1",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Regress√£o linear multipla",
    "text": "Regress√£o linear multipla\n\nConsiderando o modelo de regress√£o linear m√∫ltiplo, temos que:\nY = g({\\bf X}) = \\beta^{T}{\\bf X} + \\varepsilon,\nem que Y √© um vetor n \\times 1, {\\bf X} √© uma matriz fixa e conhecida com os atributos de dimens√£o n \\times d, em que a primeira coluna √© preenchida de 1, \\beta = (\\beta_0, \\cdots, \\beta_d). Na cultura de machine learning, iremos desconsiderar \\varepsilon, n√£o feremos suposi√ß√µes sobre \\varepsilon. Portanto, considere\ng({\\bf x}) = \\beta^{T}{\\bf X} = \\beta_{0}x_0 + \\beta_1x_{i,1} + \\cdots + \\beta_dx_{i,d}, em que x_0 \\equiv 1."
  },
  {
    "objectID": "index.html#regress√£o-linear-multipla-2",
    "href": "index.html#regress√£o-linear-multipla-2",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Regress√£o linear multipla",
    "text": "Regress√£o linear multipla\n\nO m√©todo dos m√≠nimos quadrados, para o caso de regress√£o linear m√∫ltipla (d &gt; 1) √© dado por aquele que minimiza R(\\beta^{T}{\\bf X}), i.e., minimiza:\n\\argmin_\\beta \\sum_{i = 1}^n (Y_i - \\beta_0 - \\beta_1x_{i,1} - \\cdots - \\beta_dx_{i,d})^2. Temos que\n\\widehat{\\beta} = ({\\bf X}^{T}{\\bf X})^{-1}{\\bf X}^{T}Y.\nPortanto, a fun√ß√£o de regress√£o estimada √© dada por:\ng({\\bf x}) = \\widehat{\\beta}^{T}{\\bf x}."
  },
  {
    "objectID": "index.html#regress√£o-linear-multipla-3",
    "href": "index.html#regress√£o-linear-multipla-3",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Regress√£o linear multipla",
    "text": "Regress√£o linear multipla\n\nGrande parte da literatura estat√≠stica √© voltada para justificar que o m√©todo de m√≠nimos quadrados sob um ponto de vista de um estimador de m√°xima verossimilhan√ßa, assim como tamb√©m para constru√ß√£o de testes de ader√™ncia, m√©todos para constru√ß√£o de intervalos de confian√ßa e teste de hip√≥tese para \\beta_i (par√¢metros que indexam o modelo), an√°lise de res√≠duos, entre outros.\n\nAssumir que a verdadeira regress√£o r({\\bf x}) = \\mathbb{E}({\\bf X}\\,|\\,Y) √© uma suposi√ß√£o muito forte. Contudo, existe, na literatura, justificativas para o uso de m√©todos de m√≠nimos quadrados para estimar os coeficientes, mesmo quando a regress√£o real r({\\bf x}) n√£o satisfaz a suposi√ß√£o de linearidade."
  },
  {
    "objectID": "index.html#regress√£o-linear-multipla-4",
    "href": "index.html#regress√£o-linear-multipla-4",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Regress√£o linear multipla",
    "text": "Regress√£o linear multipla\n\nO estimador de m√≠nimos quadrados \\widehat{\\beta} = ({\\bf X}^{T}{\\bf X})^{-1}{\\bf X}^{T}Y √© bom, por alguns motivos:\n\n\n√â igual ao estimador de m√°xima verossimilhan√ßa sob normalidade, linearidade e homoscedasticidade, portanto, consistente sob essas condi√ß√µes\n√â best linear unbiased prediction - BLUE sob linearidade e homoscedasticidade;\nO m√©todo de m√≠nimos quadrados tem alguma garantia, mesmo sem assumir muitas suposi√ß√µes."
  },
  {
    "objectID": "index.html#m√≠nimos-quadrados-sem-suposi√ß√£o-de-linearidade",
    "href": "index.html#m√≠nimos-quadrados-sem-suposi√ß√£o-de-linearidade",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "M√≠nimos quadrados sem suposi√ß√£o de linearidade",
    "text": "M√≠nimos quadrados sem suposi√ß√£o de linearidade\n\nQuando a suposi√ß√£o de linearidade falha, ou seja, quando a regress√£o verdadeira que desconhecemos r({\\bf x}) n√£o √© linear, frequentemente existe um vetor \\beta_{*}, tal que g_{\\beta_{*}}({\\bf x}) = \\beta_{*}^{T}{\\bf x} tem um bom poder preditivo. Nesses casos, o m√©trodo dos m√≠nimos quadrados \\widehat{\\beta} tende a produzir estimadores com baixo risco. Isso se deve ao fato que \\widehat{\\beta} converge para o melhor preditor linear (para o or√°culo \\beta_{*}) que √© dado por:\n\\beta_{*} = \\argmin_\\beta R(g_\\beta) =  \\argmin_\\beta \\mathbb{E}\\left[(Y - \\beta^{T}X)^2\\right], mesmo que a verdadeira regress√£o r({\\bf x}) n√£o seja linear, em que ({\\bf X}, Y) √© uma nova observa√ß√£o.\n\nTeorema: Seja \\beta_{*} o melhor estimador linear e \\widehat{\\beta} o estimador de m√≠nimos quadrados. Ent√£o,\n\\widehat{\\beta}\\overset{p}{\\longrightarrow}  \\beta_{*}\\,\\, \\mathrm{e}\\,\\, R(g_{\\widehat{\\beta}})\\overset{p}{\\longrightarrow} R(g_{\\beta_{*}}),  quando n \\longrightarrow \\infty. Para uma demonstra√ß√£o, veja http://www.rizbicki.ufscar.br/AME.pdf, p√°gina. 29."
  },
  {
    "objectID": "index.html#m√≠nimos-quadrados-sem-suposi√ß√£o-de-linearidade-1",
    "href": "index.html#m√≠nimos-quadrados-sem-suposi√ß√£o-de-linearidade-1",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "M√≠nimos quadrados sem suposi√ß√£o de linearidade",
    "text": "M√≠nimos quadrados sem suposi√ß√£o de linearidade\n\nEm palavras, o que o Teorema anterior diz √© que mesmo quando a regress√£o verdadeira n√£o √© linear, o estimador de m√≠nimos quadrados √© consistente para nos conduzir a um bom estimador linear, ou seja, ao menos conseguiremos o melhor estimador linear como uma aproxima√ß√£o √† r({\\bf x}) que n√£o √© linear.\n\nIsso n√£o quer dizer que voc√™ ter√° boas estimativas em todas as situa√ß√µes, muito embora o or√°culo \\beta_{*}, em muitas situa√ß√µes, ter√° bom poder preditivo. Em outras palavras, em situa√ß√µes que um problema, em sua natureza, n√£o linear, poderemos alcan√ßar boas estimativas por uma aproxima√ß√£o linear pelo m√©todo dos m√≠nimos quadrados."
  },
  {
    "objectID": "index.html#predi√ß√£o-versus-infer√™ncia",
    "href": "index.html#predi√ß√£o-versus-infer√™ncia",
    "title": "Machine Learning / Aprendizagem de M√°quina",
    "section": "Predi√ß√£o versus Infer√™ncia",
    "text": "Predi√ß√£o versus Infer√™ncia\n\nInfer√™ncia: assume que o modelo linear √© correto. O principal objetivo consiste em interpretar os par√¢metros:\n\n\nQuais s√£o os par√¢metros significantes?\nQual o efeito do aumento da dose de um rem√©dio no paciente?\n\n\nPredi√ß√£o: queremos criar g({\\bf x}) com bom poder preditivo, mesmo que a especifica√ß√£o do modelo n√£o esteja correta. N√£o assume que a verdadeira regress√£o √© de fato linear! A interpreta√ß√£o aqui n√£o √© o foco. Tudo bem?\n\n\n\n\n\nDepartamento de Estat√≠stica da UFPB"
  }
]