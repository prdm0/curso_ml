<!DOCTYPE html>
<html lang="pt"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/quarto-contrib/roughnotation-0.5.1/rough-notation.iife.js"></script>
<script src="site_libs/quarto-contrib/roughnotation-init-1.0.0/rough.js"></script><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.176">

  <meta name="author" content="Prof.&nbsp;Dr.&nbsp;Pedro Rafael D. Marinho Departamento de Estat√≠stica - UFPB ">
  <meta name="dcterms.date" content="2023-08-09">
  <title>Machine Learning / Aprendizagem de M√°quina</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-pointer/pointer.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-attribution/attribution.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="site_libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
  <script src="site_libs/jquery-1.12.4/jquery.min.js"></script>
  <link href="site_libs/leaflet-1.3.1/leaflet.css" rel="stylesheet">
  <script src="site_libs/leaflet-1.3.1/leaflet.js"></script>
  <link href="site_libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet">
  <script src="site_libs/proj4-2.6.2/proj4.min.js"></script>
  <script src="site_libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
  <link href="site_libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet">
  <script src="site_libs/leaflet-binding-2.1.2/leaflet.js"></script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning / Aprendizagem de M√°quina</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Prof.&nbsp;Dr.&nbsp;Pedro Rafael D. Marinho<br>Departamento de Estat√≠stica - UFPB<br> 
</div>
</div>
</div>

  <p class="date">2023-08-09</p>
</section>
<section class="slide level2">

<!-- ```{r} -->
<!-- #| echo: false -->
<!-- #| warning: false -->
<!-- #| eval: true -->
<!-- if(fs::dir_exists("index_files/")) -->
<!--   fs::dir_delete("index_files/") -->
<!-- ``` -->
<div class="r-fit-text">
<p>Aprendizagem de M√°quina</p>
<p><span class="flow">Bacharelado em Estat√≠stica</span></p>
<p>UFPB</p>
</div>
</section>
<section>
<section id="section" class="title-slide slide level1 title center">
<h1></h1>
<div class="r-fit-text">
<p><span class="flow">Apresenta√ß√£o</span></p>
</div>
</section>
<section id="section-1" class="slide level2" data-background-image="https://raw.githubusercontent.com/prdm0/imagens/main/eu.jpg" data-background-size="contain" data-background-position="left">
<h2></h2>
<div class="columns">
<div class="column" style="width:40%;">

</div><div class="column" style="width:60%;">
<section id="sobre-mim" class="slide level2">
<h2>Sobre mim</h2>
<p><br> <br></p>
<ul>
<li class="fragment"><p>Me chamo <a href="https://prdm.netlify.app/about_pt_br.html" data-preview-link="true">Prof.&nbsp;Dr.&nbsp;Pedro Rafael D. Marinho</a>. Meu curr√≠culo Lattes poder√° ser acessado clicando <a href="http://lattes.cnpq.br/7185368598935272" data-preview-link="true">aqui</a>.</p></li>
<li class="fragment"><p>Sou docente do Departamento de Estat√≠stica da UFPB. üë®‚Äçüè´</p></li>
<li class="fragment"><p>Toda minha forma√ß√£o acad√™mica √© na √°rea de estat√≠stica (bacharelado ao doutorado).</p></li>
<li class="fragment"><p>Tenho entusiasmo por programa√ß√£o, ci√™ncia de dados e aprendizagem de m√°quina üíªüìà.</p></li>
<li class="fragment"><p><svg aria-hidden="true" role="img" viewbox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg> Me acompanhe no GitHub: <a href="https://github.com/prdm0" class="uri">https://github.com/prdm0</a>.</p></li>
<li class="fragment"><p><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg> Me acompanhe no Linkedin: <a href="https://www.linkedin.com/in/prdm0/" class="uri">https://www.linkedin.com/in/prdm0/</a>.</p></li>
</ul>
</section>
</div>
</div>
</section></section>
<section>
<section id="o-departamento" class="title-slide slide level1 title center">
<h1>O Departamento</h1>

</section>
<section id="meu-segundo-lar" class="slide level2" data-background-color="black" data-background-image="https://raw.githubusercontent.com/prdm0/imagens/main/foto_aerea_ufpb.jpeg" data-background-size="1600px" data-background-repeat="repeat" data-background-opacity="0.35">
<h2>Meu segundo lar</h2>
<div class="cell" width="100" height="100">
<div class="cell-output-display">
<div class="leaflet html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-742f20a0c9e6c1d425a6" style="width:960px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-742f20a0c9e6c1d425a6">{"x":{"options":{"crs":{"crsClass":"L.CRS.EPSG3857","code":null,"proj4def":null,"projectedBounds":null,"options":{}}},"calls":[{"method":"addMarkers","args":[-7.1404,-34.846199,null,null,null,{"interactive":true,"draggable":false,"keyboard":true,"title":"","alt":"","zIndexOffset":0,"opacity":1,"riseOnHover":false,"riseOffset":250},null,null,null,null,null,{"interactive":false,"permanent":false,"direction":"auto","opacity":1,"offset":[0,0],"textsize":"10px","textOnly":false,"className":"","sticky":true},null]},{"method":"addTiles","args":["https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png",null,null,{"minZoom":0,"maxZoom":18,"tileSize":256,"subdomains":"abc","errorTileUrl":"","tms":false,"noWrap":false,"zoomOffset":0,"zoomReverse":false,"opacity":1,"zIndex":1,"detectRetina":false,"attribution":"&copy; <a href=\"https://openstreetmap.org\">OpenStreetMap<\/a> contributors, <a href=\"https://creativecommons.org/licenses/by-sa/2.0/\">CC-BY-SA<\/a>"}]}],"limits":{"lat":[-7.1404,-7.1404],"lng":[-34.846199,-34.846199]},"setView":[[-7.1404,-34.846199],37,{"maxWidth":1500,"minWidth":1600,"autoPan":true,"keepInView":false,"closeButton":true,"className":""}]},"evals":[],"jsHooks":[]}</script>
<p>Departamento de Estat√≠stica da UFPB.</p>
</div>
</div>
</section>
<section id="que-linguagem-de-programa√ß√£o-utilizar" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M64 96c0-35.3 28.7-64 64-64H512c35.3 0 64 28.7 64 64V352H512V96H128V352H64V96zM0 403.2C0 392.6 8.6 384 19.2 384H620.8c10.6 0 19.2 8.6 19.2 19.2c0 42.4-34.4 76.8-76.8 76.8H76.8C34.4 480 0 445.6 0 403.2zM281 209l-31 31 31 31c9.4 9.4 9.4 24.6 0 33.9s-24.6 9.4-33.9 0l-48-48c-9.4-9.4-9.4-24.6 0-33.9l48-48c9.4-9.4 24.6-9.4 33.9 0s9.4 24.6 0 33.9zM393 175l48 48c9.4 9.4 9.4 24.6 0 33.9l-48 48c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l31-31-31-31c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0z"></path></svg> Que linguagem de programa√ß√£o utilizar?</h2>
<p><br></p>
<p>Nesse curso, ser√° abordado a linguagem de programa√ß√£o <a href="https://www.r-project.org/">R</a>, mas lembre-se que voc√™ poder√° utilizar qualquer linguagem de programa√ß√£o para fazer ci√™ncia de dados. Por√©m, R e Python s√£o as minhas sugest√µes, haja vista que, atualmente, elas s√£o as linguagens com maior quantidade de ferramentas e usu√°rios trabalhando na √°rea de <a href="https://en.wikipedia.org/wiki/Data_science">ci√™ncia de dados</a>.</p>
<p><br></p>
<p><span class="black">Outros motivos que me leva a lecionar a disciplina utilizando a linguagem R s√£o:</span></p>
<ol type="1">
<li class="fragment">Possui ferramentas muito bem pensadas para manipula√ß√£o e tratamento de dados;</li>
<li class="fragment">Normalmente, os <em>frameworks</em> de <em>machine learning</em> de R s√£o menos verbosos que os de Python;</li>
<li class="fragment">Matrizes e data frames s√£o estruturas de dados que j√° encontra-se definidas dentro da linguagem, n√£o precisando assim de importar bibliotecas.</li>
</ol>
<p><span class="red">Isso √© meu gosto pessoal</span>! √â um gosto que, talvez, faz mais sentido, em se tratando de algu√©m que vem da estat√≠stica. No mercado de trabalho e em seus estudos, ap√≥s cursar as disciplinas de R e Python, fornecidas pelo <a href="https://www.ufpb.br/de">Bacharelado em Estat√≠stica da UFPB</a>, voc√™ ter√° a capacidade de estudar os <em>frameworks</em> de <em>machine learning</em>, aos seus pr√≥prios passos e escolher o que melhor te agrada. A linguagem <a href="https://julialang.org/">Julia</a> tamb√©m poder√° ser uma √≥tima op√ß√£o.</p>
</section></section>
<section>
<section id="section-2" class="title-slide slide level1 title center">
<h1></h1>
<div class="r-fit-text">
<p><span class="flow">Aprendizagem de M√°quina: O que √©?</span></p>
</div>
</section>
<section id="aprendizagem-de-m√°quina" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M320 0c17.7 0 32 14.3 32 32V96H472c39.8 0 72 32.2 72 72V440c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72H288V32c0-17.7 14.3-32 32-32zM208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H208zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H304zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H400zM264 256a40 40 0 1 0 -80 0 40 40 0 1 0 80 0zm152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80zM48 224H64V416H48c-26.5 0-48-21.5-48-48V272c0-26.5 21.5-48 48-48zm544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48H576V224h16z"></path></svg> Aprendizagem de m√°quina</h2>
<p><br> <br></p>
<p><img data-src="gifs/am.gif" class="fragment" width="930" height="600"></p>
</section>
<section id="aprendizagem-de-m√°quina-1" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M320 0c17.7 0 32 14.3 32 32V96H472c39.8 0 72 32.2 72 72V440c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72H288V32c0-17.7 14.3-32 32-32zM208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H208zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H304zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H400zM264 256a40 40 0 1 0 -80 0 40 40 0 1 0 80 0zm152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80zM48 224H64V416H48c-26.5 0-48-21.5-48-48V272c0-26.5 21.5-48 48-48zm544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48H576V224h16z"></path></svg> Aprendizagem de m√°quina</h2>
<p><br> <br></p>
<p><strong>Alguns pontos</strong>:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>A <strong>A</strong>prendizagem de <strong>M</strong>√°quina (AM), tamb√©m chamada de <strong>M</strong>achine <strong>L</strong>earning (ML), no ingl√™s, nasceu na d√©cada de 60 como um campo da intelig√™nica artificial;</p></li>
<li class="fragment"><p>Em sua origem, as aplica√ß√µes de AM tinha como objetivo aprender padr√µes com base nos dados;</p></li>
<li class="fragment"><p>Originalmente, as aplica√ß√µes de AM eram de cunho estritamente computacional. Todavia, desde o in√≠cio dos anos 90, a √°rea de aprendizagem de m√°quina expandiu seus horizontes e come√ßou a se estabelecer como um campo por sim mesma;</p></li>
<li class="fragment"><p>Em particular, a √°rea de aprendizagem de m√°quina come√ßou a estabelecer muitas intersec√ß√µes com a estat√≠stica. Muitos de seus algoritmos s√£o constru√≠dos com base em metodologias que surgiram na estat√≠stica;</p></li>
<li class="fragment"><p>Atualmente, a comunidade de AM √© bastante interdisciplinar e utiliza-se de ideias desenvolvidas em diversas √°reas, sendo a estat√≠stica uma delas.</p></li>
</ol>
</section>
<section id="tipos-de-aprendizado" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M320 0c17.7 0 32 14.3 32 32V96H472c39.8 0 72 32.2 72 72V440c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72H288V32c0-17.7 14.3-32 32-32zM208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H208zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H304zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H400zM264 256a40 40 0 1 0 -80 0 40 40 0 1 0 80 0zm152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80zM48 224H64V416H48c-26.5 0-48-21.5-48-48V272c0-26.5 21.5-48 48-48zm544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48H576V224h16z"></path></svg> Tipos de Aprendizado</h2>
<p><br></p>
<p><span class="black">Aprendizado supervisionado</span></p>
<p><br></p>
<p>Nesse curso, inicialmente estudaremos problemas de <span class="red">aprendizado supervisionado</span>, que consiste em aprender a fazer predi√ß√µes a partir de conjunto de dados em que r√≥tulos (valores da vari√°vel resposta <span class="math inline">Y</span>) s√£o observados. Trataremos tanto de problemas de regress√£o (estimar um valor n√∫m√©rico) quanto problemas de classifica√ß√£o (classificar um cliente como aprovado ou reprovado, em um problema de concess√£o de cr√©dito). Por exemplo, os <span class="red">modelos de regress√£o</span> s√£o exemplos de aprendizado supervisionado.</p>
<p><br></p>
<p><span class="black">Aprendizado n√£o-supervisionado</span></p>
<p><br></p>
<p>Na segunda parte do curso, aprenderemos alguns m√©todos de aprendizado <span class="red">n√£o-supervisionado</span>, ou seja, algoritmos que n√£o utilizam-se de r√≥tulos, em que busca-se aprender mais sobre a estrutura dos dados. Por exemplo, os <span class="red">m√©todos de agrupamento</span> (cluster), s√£o exempƒ∫os de m√©todos de aprendizado n√£o-supervisionado.</p>
</section>
<section id="tipos-de-aprendizado-1" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M320 0c17.7 0 32 14.3 32 32V96H472c39.8 0 72 32.2 72 72V440c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72H288V32c0-17.7 14.3-32 32-32zM208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H208zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H304zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H400zM264 256a40 40 0 1 0 -80 0 40 40 0 1 0 80 0zm152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80zM48 224H64V416H48c-26.5 0-48-21.5-48-48V272c0-26.5 21.5-48 48-48zm544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48H576V224h16z"></path></svg> Tipos de Aprendizado</h2>
<p><br></p>
<p>Muito embora no nosso curso focaremos nas abordagens de aprendizagem <strong>supervisionada</strong> e <strong>n√£o-supervisionada</strong>, os tipos de aprendizagem, em geral, podem ser mais amplos, em que temos:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Aprendizagem supervisionada</strong>;</li>
<li class="fragment"><strong>Aprendizagem n√£o-supervisionada</strong>;</li>
<li class="fragment">Aprendizagem semi-supervisionada;</li>
<li class="fragment">Aprendizagem por refor√ßo.</li>
</ol>
</section>
<section id="o-que-√©-aprender" class="slide level2">
<h2>O que √© aprender?</h2>
<p><br></p>
<p>Antes de detalharmos os tipos de aprendizagem de m√°quina, uma d√∫vida que poder√° surgir √©: <span class="red">‚ÄúO que √© aprender?‚Äù</span>. <span class="red">‚ÄúComo a m√°quina aprende?‚Äù</span>.</p>
<p><br></p>
<p><img data-src="gifs/am.gif" class="fragment" width="900" height="600"></p>
</section>
<section id="o-que-√©-aprender-1" class="slide level2">
<h2>O que √© aprender?</h2>
<p><br></p>
<p>De forma simples, aprender √© ganhar conhecimento atrav√©s de estudo, experi√™ncias, por meio de ensinamentos.</p>
<p><br></p>
<p><strong>T√°, mais como √© que a <span class="red">m√°quina</span> aprende?</strong></p>
<p><br></p>
<ol type="1">
<li class="fragment"><p><span class="red">Aprendizagem</span> √© o processo em que se adquire conhecimento, isto √©, √© o processo em que utilizamos de algoritmos e fornecemos dados a esses algoritmos para que possamos extrair conhecimento. Nesse processo de aprendizagem, os algoritmos fazem uso de dados para a extress√£o de conhecimento, atrav√©s de procedimentos <strong>supervisionado</strong>, <strong>n√£o-supervisionado</strong>, <strong>semi-supervisionado</strong> ou <strong>por refor√ßo</strong>, a depender do algoritmo que voc√™ deseja utilizar.</p></li>
<li class="fragment"><p><span class="red">Aprendizado</span> √© o modelo ajustado, isto √©, √© o conhecimento adquirido ap√≥s o treinamamento obtido no processo de aprendizagem. Voc√™ poder√° entender como sendo o modelo ajustado e que utilizamores para a tomada de decis√µes.</p></li>
</ol>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="o-que-√©-aprender-2" class="slide level2">
<h2>O que √© aprender?</h2>
<p><br></p>
<p>Portanto, voc√™ poder√° entender, basiciamente, existe quatro tipos de aprendizagem, sendo os dois primeiros o que mais focaremos nesse curso e que de longe s√£o os mais utilizados:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Aprendizagem supervisionada</strong>;</li>
<li class="fragment"><strong>Aprendizagem n√£o-supervisionada</strong>;</li>
<li class="fragment">Aprendizagem semi-supervisionada;</li>
<li class="fragment">Aprendizagem por refor√ßo.</li>
</ol>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="aprendizagem-supervisionada" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br></p>
<p>Nesse tipo de aprendizagem, o algoritmo ir√° receber um conjunto de dados em que conhecemos r√≥tulos para a vari√°vel de interesse. √â como se voc√™ soubesse onde um bom modelo deve chegar, para assim ser reconhecido como um bom modelo. Por exemplo,</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p><span class="red">Classifica√ß√£o</span>: precisamos determinar a classe de uma inst√¢ncia de dados, o seu atributo, i.e., <span class="math inline">\widehat{y} = \mathrm{argmax}_y\,P(Y = y\,|\, X = \bf{x})</span>, em que y √© um atributo que desejamos prever (cahorro, gato, sapo), e <span class="math inline">\bf{x}</span> √© um vetor de caracter√≠sticas (peso, altura, comprimento, se tem rabo, etc).</p></li>
<li class="fragment"><p><span class="red">Regress√£o</span>: precisamos estimar uma quantidade num√©rica, i.e., o valor da vari√°vel alvo por meio de uma <strong>inst√¢ncia de dados</strong>, ou seja, precisamos estimar <span class="math inline">Y = \mathbb{E}(Y|X = \bf{x})</span>, i.e., devemos encontrar meios de obter <span class="math inline">\widehat{Y}</span>.</p></li>
</ol>
<p><br></p>

<aside><div>
<p><strong>Algumas observa√ß√µes de nomenclaturas</strong>:</p>
<ol type="1">
<li class="fragment">√â comum chamar cada exemplo de dados, i.e., o vetor <span class="math inline">\bf{x}</span> que ser√° passado ao modelo de <span class="red">atributos</span> ou <span class="red"><em>features</em></span>;</li>
<li class="fragment">Tamb√©m √© comum chamarmos de <span class="red">r√≥tulo</span> ou <span class="red"><em>label</em></span> a classe ou valor alvo, ou seja, estas s√£o as formas de nomearmos <span class="math inline">Y</span>, sendo <span class="math inline">Y</span> uma quantidade num√©rica (modelos de regress√£o) ou n√£o (modelos de classifica√ß√£o).</li>
</ol>
</div></aside></section>
<section id="aprendizagem-supervisionada-1" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br></p>
<p>Em se tratando de m√©todos de classifica√ß√£o, podemos ter os m√©todos:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p><span class="red">Generativos</span>: s√£o os m√©todos que dada as vari√°veis <span class="math inline">X</span> e <span class="math inline">Y</span>, o objetivo √© encontrar a distribui√ß√£o de probabilidade conjunta <span class="math inline">P(X, Y)</span>, para ent√£o poder determinar <span class="math inline">P(Y|X = \bf{x})</span>. Alguns m√©todos s√£o:</p>
<ul>
<li class="fragment">Naive Bayes;</li>
<li class="fragment">Descriminante linear.</li>
</ul></li>
</ol>
<p><br></p>
<ol start="2" type="1">
<li class="fragment"><span class="red">Descriminativos</span>: s√£o os m√©todos que estimam diretamente a probabilidade condicional <span class="math inline">P(Y|X = \bf{x})</span> ou que mesmo nem assumem modelos probabil√≠sticos. Os modelos dessa classe s√£o projetados para aprender a fronteira de decis√£o que separa as classes diretamente com base nas caracter√≠sticas de entrada. Podemos citar:
<ul>
<li class="fragment">Regress√£o logistica;</li>
<li class="fragment">Perceptron;</li>
<li class="fragment"><strong>S</strong>upport <strong>V</strong>ector <strong>M</strong>achine - SVM.</li>
</ul></li>
</ol>
</section>
<section id="aprendizagem-supervisionada-2" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="gifs/classificacao.webp"></p>
</div><div class="column" style="width:40%;">
<p><br> Poder√≠amos estar interessados em classificar o tamanho de morangos:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>S (<strong>S</strong>low): pequeno;</p></li>
<li class="fragment"><p>M (<strong>M</strong>edium): m√©dio;</p></li>
<li class="fragment"><p>L (<strong>L</strong>arge): grande.</p></li>
</ol>
</div>
</div>
</section>
<section id="aprendizagem-supervisionada-3" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br> <br></p>

<img data-src="gifs/Classification-Examples.gif" style="width:45.0%" class="r-stretch quarto-figure-center"><p class="caption">Mais dois problemas de classifica√ß√£o (linear x n√£o-linear).</p></section>
<section id="aprendizagem-supervisionada-4" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br> <br></p>

<img data-src="gifs/regression.gif" width="1200" class="r-stretch quarto-figure-center"><p>Um exemplo de de um problema de regress√£o. Aqui, a ideia √© utilizar a equa√ß√£o da reta estimada, a reta que minimiza a soma dos quadrados entre a reta e os ponto seria a melhor, de modo a ter uma estimativa num√©rica atrav√©s de novos atributos passado ao modelo, i.e., por meio da equa√ß√£o da reta e de um novo valor de <span class="math inline">x</span>.</p>
</section>
<section id="aprendizagem-supervisionada-5" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br></p>
<p>Um outro exemplo seria a classifica√ß√£o de imagem/v√≠deo, utilizando um algoritmo de rede neural, por exemplo, usando uma <strong>C</strong>onvolutional <strong>N</strong>eural <strong>N</strong>etwork - <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a>. Foram utilizados diversas imagens de pessoas ‚Äúcom‚Äù e ‚Äúsem‚Äù m√°scara. Em que ‚Äúcom‚Äù representa detec√ß√£o da m√°scara na face da pessoa e ‚Äúsem‚Äù a n√£o detec√ß√£o.</p>
<p><br></p>
<iframe data-external="1" src="https://www.youtube.com/embed/Zt_Fr7YbU1c" width="50%" height="50%" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="aprendizagem-n√£o-supervisionada" class="slide level2">
<h2>Aprendizagem n√£o-supervisionada</h2>
<p><br></p>
<p>Nesse tipo de aprendizagem, os algoritmos trabalham sobre dados n√£o rotulados, por exemplo, em uma trarefa de agrupamento.</p>
<p><br></p>
<p>Os algoritmos verificam se as inst√¢ncias observadas poder√£o ser arranjadas de alguma maneira, por exemplo, usando alguma m√©trica de dist√¢ncia, formando grupos (<em>clusters</em>).</p>
<p><br></p>
<p>A ideia √© maximizar a dist√¢ncia entre os clusters e minimizar a dist√¢ncia entre os elementos no interior do grupo. Em outras palavras, o que se quer √© tornar os grupos mais diferentes poss√≠veis e tornar os elementos dos grupos o mais parecido poss√≠vel.</p>
<p><br></p>
<p>Aqui, por n√£o haver r√≥tulos, um problema comum √© determinar a quantidade de grupos ideal que muitas vezes s√£o obtidos de forma subjetiva ou por heur√≠sticas. A quantidade de grupos √© um dilema!</p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="aprendizagem-n√£o-supervisionada-1" class="slide level2">
<h2>Aprendizagem n√£o-supervisionada</h2>
<p><br></p>

<img data-src="gifs/kmeans.gif" width="900" class="r-stretch quarto-figure-center"><p>Ap√≥s a detec√ß√£o dos grupos, √© preciso analisar o resultado de modo a tentar extrair informa√ß√µes coerentes de modo a saber o que cada grupo representa no problema em quest√£o.</p>
</section>
<section id="aprendizagem-semi-supervisionada" class="slide level2">
<h2>Aprendizagem semi-supervisionada</h2>
<p><br></p>
<p>A aprendizagem semi-supervisionada √© uma abordagem na √°rea de aprendizagem de m√°quina, em que um algoritmo utiliza tanto dados rotulados quanto n√£o rotulados para treinamento. Por exemplo, algoritmos que propagam r√≥tulos, como o <em>Label Propagation</em>, em que r√≥tulos conhecidos s√£o propagados para dados n√£o rotulados com base em sua sua proximidade no espa√ßo de caracter√≠sticas.</p>
<p><br></p>
<p>Uma outra abordagem seria misturar modelos (<em>Model Blending</em>), em que diferentes modelos s√£o treinados em diferentes partes do conjunto de dados, por exemplo, um modelo para a parte roturada e um para a parte n√£o rotulada.</p>
<p><br></p>

<img data-src="gifs/hum.gif" style="width:50.0%" class="r-stretch"></section>
<section id="aprendizagem-por-refor√ßo" class="slide level2">
<h2>Aprendizagem por refor√ßo</h2>
<p><br></p>
<p>Nesse tipo de aprendizagem, n√£o <strong>h√° uma fonte externa de exemplos</strong>. O agente (modelo) aprende aprende com sua pr√≥pria experi√™ncia, por tentativas e erros, em que voc√™ dever√° definir uma medida de sucesso, e eventualmente recompensar os acertos. No v√≠deo abaixo, veja um joguinho que criei em R, em que o carrinho aprendeu a desviar de obst√°culos aleat√≥rios que aparecem em sua frente. Utilizou-se uma rede neural cuja a sa√≠da poderia ser (‚Äúparado‚Äù, ‚Äúpara cima‚Äù ou ‚Äúpara baixo‚Äù). Veja o c√≥digo clicando <a href="https://github.com/prdm0/desviando_obstaculos"><strong>aqui</strong></a>.</p>
<p><br></p>
<iframe data-external="1" src="https://www.youtube.com/embed/9NXUtwGkkDw" width="50%" height="50%" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="dados-explora√ß√£o-e-tratamento" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: explora√ß√£o e tratamento</h2>
<p><br></p>
<p>Um dos passos mais importante no fluxo de trabalho (<em>workflow</em>) de um modelo de aprendizagem de m√°quina, consiste na prepara√ß√£o dos dados, em que realizamos transforma√ß√µes, inputa√ß√µes de valores ausentes, identifica√ß√£o de <em>outliers</em>, remo√ß√£o de vari√°veis altamente correlacionadas, entre outros.</p>
<p><br></p>
<p>Fazer uma <strong>an√°lise explorat√≥ria</strong> dos dados √© um passo importante para que se possa entender e detecatar poss√≠veis inconsist√™ncias na base de dados. N√£o adianta fazer uso de modelos muito sofisticados quando se tem uma base de dados cheia de problemas.</p>
<p><br></p>
<p>Normalmente trabalhamos com juntos de dados (tabelas) relacionais, em que cada linha √© uma observa√ß√£o e cada coluna representa um atributo do objeto/observa√ß√£o. A linha de uma base de dados relacional, sem sua a vari√°vel de interesse, lembre-se que denominamos <span class="math inline">Y</span> de <span class="red">r√≥tulo</span> ou <span class="red"><em>label</em></span>, fornece o vetor de caracter√≠sticas <span class="math inline">\bf{x}</span> que descreve uma dada observa√ß√£o.</p>
</section>
<section id="dados-explora√ß√£o-e-tratamento-1" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: explora√ß√£o e tratamento</h2>
<p>No artigo <a href="https://www.jstatsoft.org/article/view/v059i10">Tidy Data</a>, 2014, publicado no Journal of Statistical Sofware, o Hadley Wickham discute que o princ√≠pio de dados organizados est√£o intimamente relacionados com banco de dados relacional e mais pr√≥ximo do recioc√≠nio que empregamos na √°lgebra. Nesse artigo, ele define o que √© <span class="red">Tidy Data</span>, sendo essa uma maneira de mapear um conjunto de dados.</p>
<p><br></p>
<p>Segundo o artigo, um conjunto de dados √© <span class="red">bagun√ßado</span> ou <span class="red">arrumado</span>/<span class="red">tidy</span>, dependendo de como as linhas, colunas e tabelas s√£o combinadas com as observa√ß√µes, vari√°veis e tipos. Em dados arrumados (dados <em>tidy</em>), temos que:</p>
<p><br></p>
<ol type="1">
<li class="fragment">Cada vari√°vel forma uma coluna;</li>
<li class="fragment">Cada observa√ß√£o forma uma linha;</li>
<li class="fragment">Cada valor deve ter sua pr√≥pria c√©lula.</li>
</ol>
<p><br></p>
<p>Embora existam situa√ß√µes em que j√° podemos come√ßar a analisar uma base de dados real, essa √© a exce√ß√£o e n√£o a regra. Normalmente, nos deparamos com bases de dados que violam uma ou mais dessas regras. Sempre, que poss√≠vel, procure utilizar dados no formato <span class="red">Tidy</span>.</p>
</section>
<section id="dados-explora√ß√£o-e-tratamento-2" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: explora√ß√£o e tratamento</h2>
<p><br></p>

<img data-src="imgs/tidy-1.png" class="r-stretch quarto-figure-center"><p class="caption">Representa√ß√£o de uma base de dados no formato <em>tidy</em>.</p></section>
<section id="dados-explora√ß√£o-e-tratamento-3" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: explora√ß√£o e tratamento</h2>
<p><br></p>
<blockquote>
<p>‚ÄúAs fam√≠lias felizes s√£o todas iguais; toda fam√≠lia infeliz √© infeliz √† sua maneira.‚Äù ‚Äì <a href="https://en.wikipedia.org/wiki/Leo_Tolstoy">Leo Tolstoy</a></p>
</blockquote>
<blockquote>
<p>‚ÄúConjuntos de dados organizados s√£o todos iguais, mas todo conjunto de dados confuso √© confuso √† sua maneira.‚Äù ‚Äì <a href="https://hadley.nz/">Hadley Wickham</a></p>
</blockquote>
<p><br></p>

<img data-src="imgs/tidy-2.png" style="width:50.0%" class="r-stretch quarto-figure-center"><p class="caption">Trabalhar com a Tabela do lado esquerdo √© melhor que a Tabela do lado direito. Prefira, sempre que poss√≠vel, o formato tidy. N√£o permita-se ficar estressado t√£o facilmente. üòÉ</p></section>
<section id="dados-explora√ß√£o-e-tratamento-4" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: explora√ß√£o e tratamento</h2>
<p>A linguagem de programa√ß√£o R possue diversas ferramentas que permite manipular e explorar bases de dados. Enumero algumas:</p>
<ol type="1">
<li class="fragment"><a href="https://dplyr.tidyverse.org/">dplyr</a>: biblioteca que implementa √© uma gram√°tica de manipula√ß√£o de dados, fornecendo um conjunto consistente de verbos que ajudam a resolver os desafios mais comuns de manipula√ß√£o de dados;</li>
<li class="fragment"><a href="https://tidyr.tidyverse.org/">tidyr</a>: ferramentas para ajudar a criar dados organizados, em que cada coluna √© uma vari√°vel, cada linha √© uma observa√ß√£o e cada c√©lula cont√©m um √∫nico valor;</li>
<li class="fragment"><a href="https://ggplot2-book.org/">ggplot2</a>: um sistema para criar gr√°ficos ‚Äòdeclarativamente‚Äô, baseado no livro <a href="https://www.amazon.com.br/Grammar-Graphics-Leland-Wilkinson/dp/0387245448">The Grammar of Graphics</a>, de <a href="https://en.wikipedia.org/wiki/Leland_Wilkinson">Leland Wilkinson</a>;</li>
<li class="fragment"><a href="https://docs.ropensci.org/visdat/">visdat</a>: uma biblioteca √∫til para um visualiza√ß√£o explorat√≥ria preliminar de dados;</li>
<li class="fragment"><a href="https://github.com/rolkra/explore">explore</a>: biblioteca que apresenta algumas rotinas de an√°lise para realizar uma an√°lise explorat√≥ria nos dados.</li>
</ol>
<p>Todas essas bibliotecas est√£o muito bem documentadas. √â importante que voc√™s explorem as documentas dessas bibliotecas, pois eventualmente irei utizar alguma delas.</p>

<img data-src="gifs/ok.gif" class="r-stretch"></section>
<section id="dados-explora√ß√£o-e-tratamento-5" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: explora√ß√£o e tratamento</h2>
<p><br></p>
<p>No <a href="https://r4ds.had.co.nz/tidy-data.html">Cap√≠tulo 12</a>, do livro <a href="https://r4ds.had.co.nz/index.html">R for Data Science</a>, o autor aborda mais sobre o formato <em>Tidy</em> e como trabalhar com a biblioteca <a href="https://tidyr.tidyverse.org/">tidyr</a>. <a href="https://r4ds.had.co.nz/transform.html?q=dplyr#dplyr-basics">Aqui</a> o autor aborda de forma b√°sica o pacote <a href="https://dplyr.tidyverse.org/">dplyr</a>.</p>
<p><br></p>
<p>Durante o curso, na medida da necessidade de utiliza√ß√£o dessas ferramentas, durante a exposi√ß√£o de exemplos, abordaremos alguns conceitos. Voc√™ ter√° a oportunidade de tamb√©m explorar essas bibliotecas nos exerc√≠cios. Ok?!</p>
<p><br></p>

<img data-src="gifs/ok.gif" class="r-stretch"></section>
<section id="dados-explora√ß√£o-e-tratamento-6" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: explora√ß√£o e tratamento</h2>
<p><br></p>
<p>Muitas vezes, no processo de tratamento dos dados, tamb√©m estamos preocupados em <strong>remover atributos que n√£o s√£o significativo</strong> para a modelagem, em que nesse momento a experi√™ncia dos especialistas s√£o fundamentais.</p>
<p><br></p>
<p><strong>√â comum enriquercermos a base de dados com informa√ß√µes de outras bases de dados</strong>, em um sistema de gerenciamento de banco de dados relacional, em que as bases de dados est√£o relacionadas por uma chave. Nesse caso, buscamos por novos atributos para um mesmo objeto (para uma mesma linha da base), em que atributos cruzados devem ter um √∫nico valor, para cada objeto, respeitando a regra tr√™s de conjuntos de dados <em>tidy</em>.</p>
<p><br></p>
<p><strong>As vezes transformamos vari√°veis</strong>. Por exemplo, √© comum tomar o logaritmo de uma vari√°vel num√©rica que √© assim√©trica, se <span class="math inline">x \geq 1</span>, em que <span class="math inline">x</span> √© um atributo num√©rico qualquer.</p>
<p><br></p>
<p>Em diveras situa√ß√µes, tamb√©m √© comum a base de dados apresentar <strong>informa√ß√µes faltantes</strong>. Nos data frames de R, a falta de informa√ß√£o na base, normlamente ser√£o representadas por <code>NA</code>.</p>
</section>
<section id="dados-explora√ß√£o-e-tratamento-7" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: explora√ß√£o e tratamento</h2>
<p><br></p>
<p>Poder√° ser que um dado atributo apresente informa√ß√£o faltante, e normalmente n√£o optaremos em remover a observa√ß√£o e precisaremos imputar a informa√ß√£o, por exemplo:</p>
<p><br></p>
<ol type="1">
<li class="fragment">Tomando alguma medida de tend√™ncia central como m√©dia/moda/mediana dos valores que s√£o conhecidos para aquele atributo;</li>
<li class="fragment">Criar um novo valor que √© indica√ß√£o de valor faltante;</li>
<li class="fragment">Usar algoritmos como <span class="math inline">k</span>-nearest neighbors - <span class="math inline">k</span><strong>NN</strong> (<span class="math inline">k</span> vizinhos mais pr√≥ximos) para imputar valores coerentes;</li>
<li class="fragment">Interpolar os dados.</li>
</ol>
<p>Esses s√£o alguns exemplos de como podemos imputar observa√ß√µes faltantes. Muitas vezes n√£o podemos nos dar o luxo de percer observa√ß√µes de nossa base de dados.</p>

<img data-src="gifs/chapulin-colorado-no.gif" class="r-stretch"></section>
<section id="dados-explora√ß√£o-e-tratamento-8" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: explora√ß√£o e tratamento</h2>
<p><br></p>
<p>√â comum ser necess√°rio transformar os dados:</p>
<ol type="1">
<li class="fragment"><p>Pode ser necess√°rio transformar os tipos ou os valores dos atributos para tentar obter um melhor ajuste do modelo;</p></li>
<li class="fragment"><p>Pode-se discretizar valores cont√≠nuos ou transform√°-los em intervalos;</p></li>
<li class="fragment"><p>√â comum transformar atributos categ√≥ricos com <span class="math inline">p</span> categorias, em <span class="math inline">p</span> novos atributos bin√°rios.</p>
<ul>
<li class="fragment"><a href="https://en.wikipedia.org/wiki/One-hot">One-hot encoding</a></li>
<li class="fragment"><a href="https://en.wikipedia.org/wiki/Dummy_variable_(statistics)">Vari√°veis dummy</a></li>
</ul></li>
<li class="fragment"><p>Outra transforma√ß√£o muito comum √© a normaliza√ß√£o dos dados. Normalizar os dados √© muito √∫til quando os atributos num√©ricos possuem escalas muito diferentes.</p></li>
</ol>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="math display">X_{novo} = \frac{X - X_{min}}{X_{max} - X_{min}},</span> em que <span class="math inline">X_{novo} \in [0, 1].</span></p>
</div><div class="column" style="width:50%;">
<p><span class="math display">X_{novo} = Z = \frac{X - \mu}{\sigma^2},</span> em que <span class="math inline">\mathbb{E}(X) = \mu</span> √© a m√©dia dos dados e <span class="math inline">\mathrm{Var}(X) = \sigma^2</span>. Na pr√°tica, em um contexto de v.a., iids, usamos <span class="math inline">\overline{x}</span> como estimador de <span class="math inline">\mu</span> e <span class="math inline">S^2</span> (vari√¢ncia amostral) como estimador de <span class="math inline">\sigma^2</span>.</p>
</div>
</div>
</section>
<section id="dados-explora√ß√£o-e-tratamento-9" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: explora√ß√£o e tratamento</h2>
<p><br></p>
<p>Lembre-se, como citado anteriormente, tomar o logaritmo natural, ou mesmo na base 10 de vari√°veis num√©ricas muito assim√©tricas, poder√° ajudar um pouco, desde que seja possivel tomar o <span class="math inline">\log(\cdot)</span>.</p>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">rgamma</span>(<span class="dv">1000</span>, <span class="dv">2</span>, <span class="dv">2</span>) <span class="sc">|&gt;</span> </span>
<span id="cb1-3"><a href="#cb1-3"></a>  <span class="fu">hist</span>()</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img data-src="index_files/figure-revealjs/unnamed-chunk-2-1.png" width="960"></p>
</div>
</div>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="fu">rgamma</span>(<span class="dv">1000</span>, <span class="dv">2</span>, <span class="dv">2</span>) <span class="sc">|&gt;</span> </span>
<span id="cb2-3"><a href="#cb2-3"></a>  <span class="fu">log</span>() <span class="sc">|&gt;</span> <span class="fu">hist</span>()</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img data-src="index_files/figure-revealjs/unnamed-chunk-3-1.png" width="960"></p>
</div>
</div>
</div>
</div>
</section>
<section id="dados-explora√ß√£o-e-tratamento-10" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: explora√ß√£o e tratamento</h2>
<p><br></p>
<p>Anteriormente eu citei algumas bibliotecas √∫teis de R para explorar os dados, na fase de tratamento das observa√ß√µes. Por√©m, n√£o estranhe n√£o ter, at√© o momento, citado bibliotecas do <em>framework</em> <a href="https://www.tidymodels.org/packages/">tidymodels</a>, em especial o <a href="https://recipes.tidymodels.org/">recipes</a> que √© muito utilizado no <em>workflow</em> de aprendizagem de m√°quina, na fase de pr√©-processamento dos dados. Muitas dessas transforma√ß√µes s√£o aplicadas como receitas de pr√©-processamento com o pacote <a href="https://recipes.tidymodels.org/">recipes</a>.</p>
<p><br></p>
<p><img data-src="imgs/recipes.png"></p>
<p><br></p>
<p>O <a href="https://www.tidymodels.org/packages/">tidymodels</a> ser√° muito √∫til para n√≥s, mas, aos poucos, seu uso e explica√ß√µes mais detalhadas ser√£o apresentadas, apesar que em algumas situa√ß√µes mais simples, poderei n√£o utiliz√°-lo, para expor detalhes que eventualmente n√£o ser√° poss√≠vel ou estariam camuflados (<em>black box</em>) na utiliza√ß√£o do <a href="https://www.tidymodels.org/packages/">tidymodels</a>.</p>
<p><br></p>
<p>Para n√£o deixar de valar sobre o <a href="https://www.tidymodels.org/packages/">tidymodels</a>, explicarei, agora, a sua filosofia e como ele est√° dividido em outras bibliotecas que s√£o √∫teis em cada parte do processo de treinamento de um modelo de <em>machine learning</em>.</p>
<p><img data-src="gifs/ok-2.gif" style="width:20.0%"></p>
</section>
<section id="tidymodels" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>
<p>Diversas bibliotecas na linguagem R s√£o preparadas para trabalharem na √°rea de aprendizagem de m√°quina. V√°rias dessas bibliotecas vem sendo desenvolvidas h√° anos. Por exemplo,as bibliotecas <a href="https://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a>, <a href="https://cran.r-project.org/web/packages/ranger/index.html">ranger</a>, <a href="https://cran.r-project.org/web/packages/kknn/index.html">kknn</a>, <a href="https://cran.r-project.org/web/packages/xgboost/index.html">xgboost</a>, <a href="https://cran.r-project.org/web/packages/keras/index.html">keras</a>, <a href="https://cran.r-project.org/web/packages/rpart/index.html">rpart</a>, <a href="https://cran.r-project.org/web/packages/randomForest/index.html">randomForest</a>, entre diversos outros.</p>
<p><br></p>
<div class="cell">
<details>
<summary>O n√∫mero de pacotes abaixo √© o mais recente. Obtido automaticamente por webscraping.</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="fu">library</span>(xml2)</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="fu">library</span>(httr)</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="fu">library</span>(stringr)</span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a>numero_pacotes_r <span class="ot">&lt;-</span> httr<span class="sc">::</span><span class="fu">GET</span>(<span class="st">"https://cloud.r-project.org/web/packages/index.html"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb3-6"><a href="#cb3-6"></a>  xml2<span class="sc">::</span><span class="fu">read_html</span>() <span class="sc">|&gt;</span> </span>
<span id="cb3-7"><a href="#cb3-7"></a>  xml2<span class="sc">::</span><span class="fu">xml_find_all</span>(<span class="st">"//p[1]"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb3-8"><a href="#cb3-8"></a>  xml2<span class="sc">::</span><span class="fu">xml_text</span>() <span class="sc">|&gt;</span> </span>
<span id="cb3-9"><a href="#cb3-9"></a>  stringr<span class="sc">::</span><span class="fu">str_extract</span>(<span class="at">pattern =</span> <span class="st">"[0-9]+"</span>)</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><br></p>
<p>Atualmente, a linguagem R possui <span class="red">19892</span>, em que muitos deles s√£o preparados para para trabalharem em tarefas de aprendizagem de m√°quina, por√©m, cada com sua sintaxe espec√≠fica. Muitos implementam o mesmo modelo, uns com algumas varia√ß√µes, por√©m, o uso √© totalmente diferente, nomes de par√¢metros distintos, sa√≠das distintas, etc.</p>

<img data-src="gifs/chaves-isso.gif" class="r-stretch"></section>
<section id="tidymodels-1" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>
<p>Essas diferentes implementa√ß√µes torna confuso trabalhar e testar diferentes modelos ao mesmo tempo.</p>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Uma das primeiras ideias mais conhecidas de unifica√ß√£o de sintaxe do <em>workflow</em> de machine learning, na linguagem R, foi idealizada pelo estat√≠stico <a href="https://www.linkedin.com/in/max-kuhn-864a9110/">Max Khun</a>.</p>
<p><br></p>
<p>Ele criou a biblioteca <a href="https://cran.r-project.org/web/packages/caret/index.html">caret</a> - <strong>C</strong>lassification <strong>A</strong>nd <strong>RE</strong>gression <strong>T</strong>raining de R que √© muito bem desenvolvida e abrangente. Voc√™ poder√° estudar o <a href="https://cran.r-project.org/web/packages/caret/index.html">caret</a> clicando <a href="https://topepo.github.io/caret/">aqui</a>.</p>
<p><br></p>
<p>N√£o foi um trabalho simples, veja uma tabela com a quantidade de modelos que o <a href="https://cran.r-project.org/web/packages/caret/index.html">caret</a> suporta, clicando <a href="https://topepo.github.io/caret/available-models.html">aqui</a>. Ent√£o, ‚Äúpor baixo dos panos‚Äù, a ideia era unificar a entrada e sa√≠da. A biblioteca <strong>caret</strong> continua sendo mantida, apesar da exist√™ncia do <a href="https://www.tidymodels.org/">tidymodels</a>.</p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/max_kuhn.jpeg"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="tidymodels-2" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/max_kuhn.jpeg"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<p>Max Kuhn, atualmente, no momento de escrita desse mateiral, √© funcion√°rio da <a href="https://posit.co/">Posit Ltda</a> e foi contratado para estar a frente do desenvolvimento de uma vers√£o ‚Äúarrumada‚Äù (<em>tidy</em>) do <a href="https://cran.r-project.org/web/packages/caret/index.html">caret</a>, que √© o <a href="https://www.tidymodels.org/">tidymodels</a>.</p>
<p><br></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/tidymodels.png" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="tidymodels-3" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>

<img data-src="imgs/workflow_tidymodels.png" style="width:70.0%" class="r-stretch quarto-figure-center"><p class="caption">O <em>workflow</em> (<em>pipeline</em>) do treinamento de um modelo usando o <em>framework</em> <a href="https://www.tidymodels.org/">tidymodels</a>. Todos os pacotes (rsample, recipes, parsnip, tune, dails, yardstick) s√£o gerenciados pelo pacote <a href="https://www.tidymodels.org/">tidymodels</a>. Cada um desses pacotes fornece um conjunto de fun√ß√µes √∫teis em tarefas espec√≠ficas no workflow de machine learning.</p></section>
<section id="tidymodels-4" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>
<ol type="1">
<li class="fragment"><a href="https://rsample.tidymodels.org/">rsample</a>: respons√°vel pela reamostragem dos dados, parte importante para que possamos treinar um modelo de aprendizagem de m√°quina. √â nele que encontra-se fun√ß√µes para realizar reamostragem como bootstrap, <span class="math inline">k</span>-folds cross-validation, nested cross-validation, entre outras.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/rsample.png" style="width:10.0%"></p>
</figure>
</div>
<ol start="2" type="1">
<li class="fragment"><a href="https://recipes.tidymodels.org/">recipes</a>: apresenta diversas fun√ß√µes para transforma√ß√µes de vari√°veis como cria√ß√£o de vari√°veis dummy, normaliza√ß√£o de vari√°veis, inputa√ß√£o de dados pela m√©dia, mediana, <span class="math inline">k</span>NN, entre outras formas de imputa√ß√£o, transforma√ß√µes de vari√°veis categ√≥rias em num√©ricas, entre outras funcionalidades. Ele permite que possamos criar uma receita de transforma√ß√µes nos dados para que esses, ap√≥s transformados, possam entrar no modelo.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/recipes.png" style="width:10.0%"></p>
</figure>
</div>
<ol start="3" type="1">
<li class="fragment"><a href="https://parsnip.tidymodels.org/">parsnip</a>: √© o pacote que unifica as entradas e sa√≠das de diversos pacotes de aprendizagem de m√°quina de R. Ele possui os motores (engines) que s√£o as comunica√ß√µes com os algoritmos implementados em diversos pacotes de R que trabalham com tarefas de regress√£o e classifica√ß√£o, em aprendizagem de m√°quina.</li>
</ol>
</section>
<section id="tidymodels-5" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Os pacotes <a href="https://tune.tidymodels.org/">tune</a>, <a href="https://dials.tidymodels.org/">dails</a> e <a href="https://yardstick.tidymodels.org/">yardstick</a> tomar√° conta da parte de treino do modelo. Os pacote <a href="https://tune.tidymodels.org/">tune</a> e <a href="https://dials.tidymodels.org/">dails</a> s√£o respons√°veis pela ‚Äútunagem‚Äù dos eventuais hiperpar√¢metros, j√° o <a href="https://yardstick.tidymodels.org/">yardstick</a> √© respons√°vel pelas m√©tricas de avalia√ß√£o do modelo.</p>
<p><br></p>
<p>A biblioteca <a href="https://dials.tidymodels.org/">dails</a> est√° mais relacionada a cria√ß√£o dos <em>grids</em> para os eventuais hiperpar√¢metros do modelo. J√° o pacote <a href="https://tune.tidymodels.org/">tune</a>, utiliza-se da valida√ß√£o cruzada criada pelo pacote <a href="https://rsample.tidymodels.org/">rsample</a> para varrer as combina√ß√µes de hiperpar√¢metros criadas pelo <a href="https://dials.tidymodels.org/">dails</a>, i.e., o <a href="https://tune.tidymodels.org/">tune</a> est√° mais relacionado com a otimiza√ß√£o dos hiperpar√¢metros.</p>
</div><div class="column" style="width:25%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/tune.png"></p>
</figure>
</div>
</div><div class="column" style="width:25%;">
<p><img data-src="imgs/dails.png" data-fig-aling="right"> <img data-src="imgs/yardstick.png" data-fig-aling="right"></p>
</div>
</div>
</section>
<section id="tidymodels-6" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>
<p>Todo o fluxo de trabalho √© gerido pela biblioteca <a href="https://workflows.tidymodels.org/">workflows</a> de R. Em especial, as etapas de <em>feature engineering</em> e especifica√ß√£o do modelo.</p>
<p><br></p>

<img data-src="imgs/workflows_lib.png" class="r-stretch quarto-figure-center"></section>
<section id="tidymodels-7" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>

<img data-src="imgs/MachineLearning_tidymodels.png" class="r-stretch quarto-figure-center"><p class="caption">Perceba o papel da biblioteca <a href="https://workflows.tidymodels.org/">workflows</a> de R. Basicamente gostar√≠amos de ter uma automa√ß√£o da faze do tratamento das <em>features</em> realizada com o <a href="https://recipes.tidymodels.org/">recipes</a> com a modelagem.</p></section>
<section id="tidymodels-8" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>
<p>N√£o necessariamente iremos utilizar o <a href="https://www.tidymodels.org/">tidymodels</a> em todos os exemplos e exerc√≠cios. Por√©m, iremos explorar bastante, at√© o fim do curso, o treinamento de modelos usando o <a href="https://www.tidymodels.org/">tidymodels</a>. Por tanto, aos poucos, a medida em que exemplos s√£o apresentados e exerc√≠cios forem passados, o aprendizado do uso do <a href="https://www.tidymodels.org/">tidymodels</a> se dar√°.</p>
<p><br></p>
<p><img data-src="gifs/thumbs-up-nod.gif" style="width:25.0%"> <img data-src="gifs/teclado-anime.gif" style="width:25.0%"></p>
<p><br></p>
<p>Sempre que poss√≠vel, deveremos colocar as ‚Äúm√£os na massa‚Äù üçù para que possamos dominar e compreender uma ferramenta computacional. A pr√°tica √© importante!</p>
</section>
<section id="as-duas-culturas" class="slide level2">
<h2>As duas culturas</h2>
<p><br></p>
<p>Em <a href="https://projecteuclid.org/journalArticle/Download?urlId=10.1214%2Fss%2F1009213726">Breiman, L. (2001a). <strong>Statistical modeling: The two cultures</strong>. Statistical Science, 16(3), 199‚Äì231</a>, o Leo Breiman argumenta que existe duas culturas no uso de modelos estat√≠sticos, em especialmente na √°rea de modelos de regress√£o. Segundo eles, as culturas s√£o:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><span class="red">Data modeling culture</span>: nela, em geral, se assume que o modelo de regress√£o utilizado <span class="math inline">r(x)</span>, por exemplo, <span class="math inline">r(x) = \beta_0 + \sum_{i = 1}^d \beta_ix_i</span> √© correto. O principal objetivo dessa abordagem √© a interpreta√ß√£o dos par√¢metros que indexam o modelo <span class="math inline">r(x)</span>. Nesse tipo de cultura, a ideia tamb√©m √© construir intervalos aleat√≥rios e testar hip√≥teses para os <span class="math inline">\beta_i's</span>. Sob essa √≥tica, muitas suposi√ß√µes sob o modelo s√£o realizadas, em que formas para checar essas suposi√ß√µes s√£o desenvolvidas, uma vez que elas s√£o fundamentais para esse tipo de modelagem.</li>
</ol>
<p><br></p>
<ol start="2" type="1">
<li class="fragment"><span class="red">Algorithmic modeling culture</span>: essa √© a cultura que domina a comunidade de aprendizagem de m√°quina. Nessa abordagem, o principal objetivo s√£o as predi√ß√µes por meio de novas observa√ß√µes. N√£o se assume que o modelo utilizado √© o modelo correto. Nesse tipo de modelagem, muitas vezes os algoritmos n√£o envolve nenhuma estrutura probabil√≠stica. Muitas vezes, modelos n√£o bem especificado conduzem a boas predi√ß√µes.</li>
</ol>
</section>
<section id="as-duas-culturas-1" class="slide level2">
<h2>As duas culturas</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/paper_breiman.png" style="width:70.0%"></p>
<figcaption>Breiman, L. (2001a). Statistical modeling: The two cultures. Statistical Science, 16(3), 199‚Äì231.</figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/leo_breiman.png" style="width:60.0%"></p>
<figcaption>Leo, na √©poca em que era um jovem probabilista na Universidade da Calif√≥rina.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="as-duas-culturas-2" class="slide level2">
<h2>As duas culturas</h2>
<p><br></p>
<p>H√° diversos artigos interessantes que s√£o respostas ao artigo do Leo Breiman, como por exemplo, o artigo <a href="https://www.jstor.org/stable/2676682">Statistical Modeling: The Two Cultures: Comment</a> do David Cox e com coment√°rios do Brad Efron.</p>

<img data-src="imgs/david_cox.png" style="width:20.0%" class="r-stretch quarto-figure-center"><p class="caption"><a href="https://en.wikipedia.org/wiki/David_Cox_(statistician)">Sir David Cox.</a></p></section>
<section id="as-duas-culturas-3" class="slide level2">
<h2>As duas culturas</h2>
<p><br></p>
<p>Muito embora exista essa divis√£o entre as culturas, Breiman foi um estat√≠stico que desempenhou um grande trabalho para unir a √°rea de estat√≠stica com aprendizado de m√°quina. Por conta dessa grande import√¢ncia, um pr√™mio concedido em sua homenagem foi criado pela <a href="https://community.amstat.org/slds/awards/breiman-award">American Statistical Association</a>.</p>

<img data-src="imgs/breiman_residencia.png" style="width:30.0%" class="r-stretch quarto-figure-center"><p class="caption"><a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2Fss%2F1009213290">Leo Breiman trabalhando em sua resid√™ncia, em 1985.</a></p></section>
<section id="as-duas-culturas-4" class="slide level2">
<h2>As duas culturas</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:70%;">
<p>Leo Breiman, renomado estat√≠stico, contribuiu significativamente para o campo de aprendizagem de m√°quina. Ele √© conhecido por ter criado m√©todos populares e influentes para a √°rea. Entre tais m√©todos famosos, cito dois:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p><strong>Random forest</strong> (<strong>florestas aleat√≥rias</strong>): m√©todo que combina a previs√£o de v√°rios modelos de √°rvores de decis√£o (<em>decision tree</em>), que veremos mais a frente, por isso o termo ‚Äúfloresta‚Äù para problemas de regress√£o e classifica√ß√£o;</p></li>
<li class="fragment"><p><strong>Bootstrap aggregating</strong> (<strong>bagging</strong>): t√©cnica de aprendizagem <em>ensemble</em>, em que cria-se multiplos conjuntos de dados obtidos com reposi√ß√£o da amostra de treinamento. O modelo de aprendizagem de m√°quina √© treinado em cada conjunto de dados e as previs√µes de cada um dos modelos s√£o combinadas por meio da m√©dia (em problemas de regress√£o), ou por voto majorit√°rio, em problemas de classifica√ß√£o. O <em>bagging</em> √© utilizado para reduzir a vari√¢ncia e melhorar a estabilidade do modelo.</p></li>
</ol>
</div><div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/sign1357-gra-0003-m.jpg"></p>
</figure>
</div>
</div>
</div>
</section></section>
<section>
<section id="section-3" class="title-slide slide level1 title center" data-background-image="imgs/rawpixel/freight.jpg">
<h1></h1>
<div class="r-fit-text">
<p><span class="flow">Regress√£o / Parte I</span></p>
</div>
</section>
<section id="regress√£o" class="slide level2">
<h2>Regress√£o</h2>
<p><br></p>
<p>M√©todos de regress√£o surgiram h√° mais de dois s√©culos com Legendre (1805) e Gauss (1809), que exploraram o m√©todo dos m√≠nimos quadrados com o objetivo de prever √≥rbitas ao redor do Sol. Hoje em dia, o problema de estima√ß√£o de uma fun√ß√£o de regress√£o possui papel central em estat√≠stica.</p>
<p><br></p>
<blockquote>
<p>Apesar de as primeiras t√©cnicas para solucionar esse problema datarem de ao menos 200 anos, os avan√ßos computacionais recentes permitiram que novas metodologias fossem exploradas. Em particular, com a capacidade cada vez maior de armazenamento de dados, m√©todos com menos suposi√ß√µes sobre o verdadeiro estado da natureza ganham cada vez mais espa√ßo. Com isso, v√°rios desafios surgiram: por exemplo, m√©todos tradicionais n√£o s√£o capazes de lidar de forma satisfat√≥ria com bancos de dados em que h√° mais covari√°veis que observa√ß√µes, uma situa√ß√£o muito comum nos dias de hoje. Similarmente, s√£o frequentes as aplica√ß√µes em que cada observa√ß√£o consiste em uma imagem ou um documento de texto, objetos complexos que levam a an√°lises que requerem metodologias mais elaboradas. ‚Äì Izbick et al.</p>
</blockquote>
</section>
<section id="regress√£o-1" class="slide level2">
<h2>Regress√£o</h2>
<p><br></p>
<p>De forma geral, temos que o objetivo de um modelo de regress√£o √© determinar a rela√ß√£o entre uma vari√°vel aleat√≥ria (label) <span class="math inline">Y \in \mathbb{R}</span> e um vetor de covari√°veis (features) <span class="math inline">\mathbf{x} = (x_1, \cdots, x_d) \in \mathbb{R}^d</span>. Mais especificamente, busaca-se estimar</p>
<p><span class="math display">r(\bf{x}) := \mathbb{E}(Y|\bf{X} = \bf{x}),</span></p>
<p>sendo esta chamada de <span class="red">fun√ß√£o de regress√£o</span>. Temos que:</p>
<p><br></p>
<ol type="1">
<li class="fragment">Se <span class="math inline">Y</span> √© uma vari√°vel quantitativa, ent√£o estamos sob um problema de <span class="red">regress√£o</span>;</li>
<li class="fragment">Se <span class="math inline">Y</span> √© uma vari√°vel qualitativa, ent√£o teremos um problema de <span class="red">classifica√ß√£o</span>.</li>
</ol>
<p>Em aprendizagem de m√°quina, assumimos que n√£o temos meios de calcular <span class="math inline">r({\bf{x}})</span>, i.e., n√£o conhecemos a distribui√ß√£o condicional de <span class="math inline">{\bf{Y}\,|\,X}</span>. Portanto, n√£o temos meios de calcular</p>
<p><span class="math display">\mathbb{E}({\bf X}|Y = y) = \int x\,\mathrm{d}F_{\bf X}({\bf x} | Y = y).</span></p>
</section>
<section id="nota√ß√µes" class="slide level2">
<h2>Nota√ß√µes</h2>
<p><br></p>
<p>A vari√°vel <span class="math inline">Y</span> recebe frequentemente o nome de vari√°vel resposta, vari√°vel dependente, r√≥tulo ou <em>label</em>. J√° as observa√ß√µes contidas no vetor <span class="math inline">\bf{x} = (x_1, \cdots, x_d)</span>, s√£o, em geral, denominadas de vari√°veis explicativas, vari√°veis independentes, caracter√≠sticas, atributos, preditores, covari√°veis ou <em>features</em>.</p>
<p><br></p>
<p>A ideia, nessa primeira parte do curso, √© descrever algumas t√©cnicas para estimar (<strong>treinar</strong>, como √© dito em aprendizagem de m√°quina) <span class="math inline">r(\bf{x})</span>.</p>
<p><br></p>
<p>A menos quando dito o contr√°rio, assumiremos que nossa amostra s√£o i.i.d. (independentes e identicamente distribu√≠das), ou seja, <span class="math inline">(\bf{X}_1, Y_1), \cdots, (\bf{X}_n, Y_n)</span> s√£o i.i.d.</p>
<p><br></p>
<p>Denota-se por <span class="math inline">x_{i,j}</span> o valor da <span class="math inline">j</span>-√©sima covari√°vel na <span class="math inline">i</span>-√©sima amostra, com <span class="math inline">j = 1, \cdots, d</span> e <span class="math inline">i = 1, \cdots, n</span>.</p>
</section>
<section id="nota√ß√µes-1" class="slide level2">
<h2>Nota√ß√µes</h2>
<p><br></p>
<table style="width:50%;">
<caption>Nota√ß√£o utilizada nesse material para as vari√°veis envolvidas em um problema de regress√£o.</caption>
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Label</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">Y_1</span></td>
<td><span class="math inline">X_{1,1},\cdots, X_{1,d}\,\,\, (= \bf{X}_1)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\vdots</span></td>
<td><span class="math inline">\,\,\,\vdots\,\,\,\,\, \ddots\,\,\ \vdots</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">Y_n</span></td>
<td><span class="math inline">X_{n,1},\cdots, X_{n,d}\,\,\, (= \bf{X}_n)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="regress√£o-2" class="slide level2">
<h2>Regress√£o</h2>
<p>Nossa ideia √© construir uma boa estimativa <span class="math inline">g</span> da fun√ß√£o de regress√£o <span class="math inline">r(\bf{x}) := \mathbb{E}(Y\,|\,\bf{X} = \bf{x})</span>, para novas observa√ß√µes, i.e., queremos obter uma fun√ß√£o <span class="math inline">g</span>, tal que:</p>
<p><span class="math display">g: \mathbb{R}^d \rightarrow \mathbb{R},</span></p>
<p>de tal forma que <span class="math inline">g</span> possua um bom poder preditivo. Em aprendizagem de m√°quina, s√≥ estaremos interessados em obter uma fun√ß√£o <span class="math inline">g</span> que estime bem um n√∫mero real (em problemas de regress√£o), ou que classifique bem (em um problema de classifica√ß√£o), utilizando as <span class="math inline">d</span> covari√°veis. Ou seja, para <span class="math inline">m</span> novas observa√ß√µes, desejamos obter <span class="math inline">g</span>, que</p>
<p><span class="math display">g({\bf{x}}_{n + 1}) \approx y_{n + 1}, \cdots, g({\bf{x}}_{n + m}) \approx y_{n + m}.</span></p>
</section>
<section id="fun√ß√£o-de-risco" class="slide level2">
<h2>Fun√ß√£o de risco</h2>
<p><br></p>
<p>Para que possamos construir boas fun√ß√µes de predi√ß√£o, √© preciso que tenhamos um crit√©rio para medir o desempenho de uma dada fun√ß√£o <span class="math inline">g:\mathbb{R}^d \rightarrow \mathbb{R}</span>. Em contexto de regress√£o, usaremos o risco quadr√°tico, muito embora esta n√£o √© a √∫nica op√ß√£o. Denotaremos a fun√ß√£o de risco quadr√°tico por:</p>
<p><span class="math display">R_{pred}(g) = \mathbb{E}\left[({\bf Y} - g({\bf X}))^2\right],</span> em que <span class="math inline">(\bf X, Y)</span> s√£o observa√ß√µes novas que n√£o foram utilizadas para treinar/estimar <span class="math inline">g</span>. L√™-se <span class="math inline">R_{pred}(g)</span> como ‚Äúrisco preditivo de <span class="math inline">g</span>‚Äù. Note que, como <span class="math inline">\bf X</span> s√£o observa√ß√µes conhecidas e <span class="math inline">g(\cdot)</span> √© um modelo preditivo, portanto, <span class="math inline">g</span> √© conhecido, ent√£o, <span class="math inline">\widehat{\bf Y} = g(\bf X)</span> √© um estimador dos <em>labels</em>, i.e., de <span class="math inline">\bf Y</span>.</p>
<p><br></p>
<p>Diremos que <span class="math inline">L(g({\bf X}); {\bf Y}) = ({\bf Y} - g({\bf X}))^2</span> √© a <span class="red">fun√ß√£o de perda quadr√°tica</span>, as vezes chamado de perda <span class="math inline">L_2</span>. Outra fun√ß√µes como a <span class="red">fun√ß√£o de perda absoluta</span> denotada por <span class="math inline">L(g({\bf X}); {\bf Y}) = |{\bf Y} - g({\bf X})|</span>, as vezes chamada de perda <span class="math inline">L_1</span> poderiam ser consideradas.</p>
</section>
<section id="fun√ß√£o-de-risco-1" class="slide level2">
<h2>Fun√ß√£o de risco</h2>
<p><br></p>
<p>Em linhas gerais, seja <span class="math inline">L(\cdot)</span> uma fun√ß√£o qualquer, tal que <span class="math inline">\forall \, 0 &lt; u &lt; v</span>, de modo que:</p>
<p><br></p>
<ol type="i">
<li class="fragment"><span class="math inline">0 = L(0) \leq L(u) \leq L(v)</span>;</li>
<li class="fragment"><span class="math inline">0 = L(0) \leq L(-u) \leq L(-v)</span>.</li>
</ol>
<p><br></p>
<p>Qualquer fun√ß√£o <span class="math inline">L(\cdot)</span> que satisfaz as propriedades acima √© chamada de <a href="https://en.wikipedia.org/wiki/Loss_function">fun√ß√£o de perda</a>. Em especial, temos que:</p>
<p><br></p>
<ul>
<li class="fragment">Fun√ß√£o de perda quadr√°tica: <span class="math inline">L(u) = u^2</span>;</li>
<li class="fragment">Fun√ß√£o de perda absoluta: <span class="math inline">L(u) = |u|</span>;</li>
<li class="fragment">Fun√ß√£o de perda degrau: <span class="math inline">L(0) = 0</span>, se <span class="math inline">|u| &lt; \delta</span> e <span class="math inline">1</span> caso contr√°rio, para algum <span class="math inline">\delta &gt; 0</span>;</li>
</ul>
</section>
<section id="fun√ß√£o-de-risco-2" class="slide level2">
<h2>Fun√ß√£o de risco</h2>
<p><br></p>
<p>Normalmente considera-se a perda <span class="math inline">L_2</span>, uma vez que em modelos de regress√£o, minimizar <span class="math inline">R_{pred}(g)</span>, em <span class="math inline">g</span>, equivale a encontrar <span class="math inline">r({\bf x}) = \mathbb{E}({\bf X}|{\bf Y})</span>, i.e., equivale a estimar a fun√ß√£o de regress√£o.</p>
<p><br></p>
<p><span class="red">Teorema</span>: Suponha que definimos o risco de uma fun√ß√£o de predi√ß√£o <span class="math inline">g: \mathbb{R}^d \rightarrow \mathbb{R}</span> via fun√ß√£o perda quadr√°tica, i.e, <span class="math inline">R_{pred}(g) = \mathbb{E}\left[({\bf Y} - g({\bf X}))^2\right]</span>, em que <span class="math inline">\bf (X, Y)</span> s√£o novas observa√ß√µes que n√£o foram utilizadas para estimar <span class="math inline">g</span>. Suponha tamb√©m que estimamos o risco de um estimador de regress√£o <span class="math inline">r({\bf X})</span> via fun√ß√£o perda quadr√°tica <span class="math inline">R_{reg}(g) = \mathbb{E}\left[(r({\bf X}) - g({\bf X}))^2\right]</span>. Ent√£o,</p>
<p><span class="math display">R_{pred}(g) = R_{reg}(g) + \mathbb{E}\left[\mathbb{V}[{\bf Y} | {\bf X}]\right],</span></p>
<p>em que <span class="math inline">\mathbb{E}\left[\mathbb{V}[{\bf Y} | {\bf X}]\right]</span> √© a vari√¢ncia m√©dia do modelo que n√£o depende de <span class="math inline">g</span>. Portanto, estimar bem <span class="math inline">r({\bf x})</span> √© de fundamental import√¢ncia para criar uma boa fun√ß√£o de predi√ß√£o. Em especial, sob a √≥tica do risco quadr√°tico, a melhor fun√ß√£o de predi√ß√£o para <span class="math inline">\bf Y</span> √© a fun√ß√£o de regress√£o <span class="math inline">r({\bf x})</span>, de tal modo que:</p>
<p><span class="math display">\argmin_g R_{pred}(g) = \argmin_g R_{reg}(g) = r({\bf x}).</span></p>
</section>
<section id="fun√ß√£o-de-risco-3" class="slide level2">
<h2>Fun√ß√£o de risco</h2>
<p><br></p>
<p><strong>Lembre-se</strong>: <span class="math inline">r({\bf x}) = \mathbb{E}(Y | \bf{X} = \bf{x})</span> √© a nossa <span class="red">fun√ß√£o de regress√£o</span>.</p>
<p><br></p>
<p>A defini√ß√£o de risco preditivo <span class="math inline">R_{pred}</span>, que tamb√©m denotaremos simplesmente por <span class="math inline">R</span>, tem um apelo frequentista. Dessa forma, para um novo conjunto com <span class="math inline">m</span> novas observa√ß√µs, <span class="math inline">({\bf X}_{n+1}, Y_{n+1}), \cdots, ({\bf X}_{n+m}, Y_{n+m})</span>, temos que essa nova amostra √© i.i.d. √† amostra observada (utilizada no treinamento do modelo/na estima√ß√£o). Ent√£o, pela Lei dos Grandes N√∫meros, temos que um bom estimador para a fun√ß√£o para o risco preditivo √© dado por:</p>
<p><span id="eq-risco-correto"><span class="math display">\frac{1}{m}\sum_{i = 1}^m (Y_{n + i} - g(X_{n + i}))^2 \approx R(g) := \mathbb{E}\left[(Y - g({\bf X}))^2\right]. \tag{1}</span></span></p>
<p>Chamaremos a quantidade acima de <a href="https://pt.wikipedia.org/wiki/Erro_quadr%C3%A1tico_m%C3%A9dio">Erro Quadr√°tico M√©dio - EQM</a>. Em aprendizagem de m√°quina, normalmente estaremos no contexto em que temos muitas observa√ß√µes, e que portanto, poderemos fazer esse apelo frequentista.</p>
<p><br></p>
<p>Desejamos encontrar <span class="math inline">g</span> (encontrar m√©todos) que minimize de forma satisfat√≥ria <span class="math inline">R</span>, i.e., m√©todos que nos conduzam √† um risco baixo.</p>
</section>
<section id="fun√ß√£o-de-risco-4" class="slide level2">
<h2>Fun√ß√£o de risco</h2>
<p><br></p>
<p>Sendo assim, se <span class="math inline">R(g)</span> possui um valor baixo, ent√£o, temos que</p>
<p><span class="math display">g({\bf x}_{n+1}) \approx y_{n+1}, \cdots, g({\bf x}_{n+m}) \approx y_{n+m}.</span> <br> <img data-src="gifs/hum.gif" style="width:25.0%"></p>
</section>
<section id="regress√£o-linear" class="slide level2">
<h2>Regress√£o linear</h2>
<p><br></p>
<p>Nesse momento, vamos pensar um pouco em regress√£o linear. No caso mais simples, queremos prever o comportamento de uma vari√°vel de interesse <span class="math inline">Y</span> condicional a uma vari√°vel explicativa <span class="math inline">X</span> (regress√£o linear simples, i.e., <span class="math inline">d = 1</span>). O melhor preditor de <span class="math inline">Y</span> condicional em <span class="math inline">X</span> √© aquele que minimiza a fun√ß√£o de perda esperada, ou seja, √© aquele que resolve:</p>
<p><span class="math display">\argmin_g \mathbb{E}(L(Y - g)|X).</span></p>
<p>Para o caso da fun√ß√£o perda quadr√°tica (fun√ß√£o <span class="math inline">L_2</span>), o melhor preditor de <span class="math inline">Y</span> condicional √† <span class="math inline">X</span> √© a m√©dia condicional de <span class="math inline">Y</span> dado <span class="math inline">X</span>, i.e., <span class="math inline">r(X) = \mathbb{E}(Y|X)</span>. J√°, na situa√ß√£o em que considera-se a perda absoluta (fun√ß√£o <span class="math inline">L_1</span>), o melhor estimador √© a mediana condicional.</p>
<p><br></p>
<p><strong>Os modelos de regress√£o, em geral, fazem uso da fun√ß√£o de perda quadr√°tica.</strong></p>
</section>
<section id="regress√£o-linear-simples" class="slide level2">
<h2>Regress√£o linear simples</h2>
<p><br></p>
<p>No caso da regress√£o linear simples (<span class="math inline">d = 1</span>), temos que o modelo √© dado por:</p>
<p><span class="math display">g(x) = \beta_0 + \beta_1 x_{i,1} + \varepsilon_i, \,\, i = 1, \cdots, n,</span> em que <span class="math inline">\varepsilon_i</span> √© um erro aleat√≥rio. Na abordagem <em>data modeling culture</em>, v√°rias suposi√ß√µes poderem ser feitas para <span class="math inline">\varepsilon_i</span>.</p>
<p>Assumindo que a regress√£o linear simples √© o modelo <span class="math inline">g</span> que iremos utilizar, ent√£o, desejamos minimizar:</p>
<p><span class="math display">\argmin_{\beta} R(g_\beta) = \argmin_{\beta} \sum_{i = 1}^n(y_i - \beta_0 - \beta_1x_{i,1})^2.</span> Derivando em rela√ß√£o √† <span class="math inline">\beta</span> e igualando a zero, ap√≥s algumas manipula√ß√µes alg√©bricas, temos que:</p>
<p><span class="math display">\widehat{\beta} = \frac{\sum_{i = 1}^n (x_i - \overline{x})(y_i - \overline{y})}{\sum_{i=1}^n(x_i - \overline{x})^2} = r_{xy}\frac{s_y}{s_x},</span> em que <span class="math inline">s_x</span> e <span class="math inline">s_y</span> s√£o os desvio-padr√£o de <span class="math inline">x</span> e <span class="math inline">y</span>, respectivamente, e <span class="math inline">r_{xy}</span> √© o coeficiente de correla√ß√£o da amostra, em que <span class="math inline">-1 \leq r_{xy} \leq 1</span>.</p>
</section>
<section id="regress√£o-linear-simples-1" class="slide level2">
<h2>Regress√£o linear simples</h2>
<p><br></p>
<p><span class="math display">r_{xy} = \frac{\overline{xy} - \overline{x}\,\overline{y}}{\sqrt{(\overline{x^2} - \overline{x}^2)(\overline{y^2} - \overline{y}^2)}}.</span> O coeficiente de determina√ß√£o <span class="math inline">R^2</span> do modelo √© dado por <span class="math inline">r_{xy}^2</span>, quando o modelo √© linear e possue uma √∫nica vari√°vel independente (feature).</p>
<p><br></p>
<p>Portanto, temos que:</p>
<p><span class="math display">\widehat{\beta_0} = \overline{y} - \widehat{\beta}\overline{x},</span></p>
<p>Na <span class="red"><em>data modeling culture</em></span> (na estat√≠stica), normalmente assumimos que o <span class="math inline">\varepsilon_i</span> tem distribui√ß√£o normal e vari√¢ncia constante, <span class="math inline">\forall\, i = 1, \cdots, n</span>. Assume-se tamb√©m que <span class="math inline">\mathbb{E}(\varepsilon_i) = 0, \, \forall i</span>.</p>
</section>
<section id="regress√£o-linear-simples-2" class="slide level2">
<h2>Regress√£o linear simples</h2>
<p><br></p>
<p>Aqui n√£o iremos nos preocupar com essas suposi√ß√µes, uma vez que em <span class="red"><em>algorithmic modeling culture</em></span>, n√£o estamos preocupados com suposi√ß√µes nem interpreta√ß√µes, ok!?</p>
<p><br></p>

<img data-src="gifs/ok-2.gif" class="r-stretch"></section>
<section id="regress√£o-linear-multipla" class="slide level2">
<h2>Regress√£o linear multipla</h2>
<p><br></p>
<p>A fun√ß√£o de perda quadr√°tica (fun√ß√£o <span class="math inline">L_2</span>) tem algumas vantagens em rela√ß√£o a fun√ß√£o de perda absoluta. Listo algumas:</p>
<ol type="1">
<li class="fragment"><p>A fun√ß√£o de perda quadr√°tica penaliza mais os erros maiores, devido ao fato dos erros serem levado ao quadrado;</p></li>
<li class="fragment"><p>A fun√ß√£o de perda quadr√°tica √© mais sens√≠vel a presen√ßa de <a href="https://en.wikipedia.org/wiki/Outlier"><em>outlier</em></a>, que em compensa√ß√£o s√£o menos penalizados ao se considerar a fun√ß√£o de perda absoluta (fun√ß√£o <span class="math inline">L_1</span>);</p></li>
<li class="fragment"><p>Em situa√ß√µes em que o erro tem distribui√ß√£o normal, a estimativa de m√≠nimos quadrados √© a solu√ß√£o de m√°xima verossimilhan√ßa e √© a estimativa linear n√£o viesada e com menor vari√¢ncia. Portanto, gozamos de um estimador com √≥timas propriedades, muito embora ele tamb√©m √© um bom estimador mesmo quando a suposi√ß√£o de normalidade n√£o √© verificada;</p></li>
<li class="fragment"><p>A fun√ß√£o de perda quadr√°tica √© deferenci√°vel, j√° a fun√ß√£o de perda absoluta n√£o √©.</p></li>
</ol>
<p>Para o caso de regress√£o linear m√∫ltipla, i.e., quando <span class="math inline">d &gt; 1</span>, poderemos utilizar uma nota√ß√£o matricial para representar o modelo linear m√∫ltiplo de regress√£o.</p>
</section>
<section id="regress√£o-linear-multipla-1" class="slide level2">
<h2>Regress√£o linear multipla</h2>
<p><br></p>
<p>Considerando o modelo de regress√£o linear m√∫ltiplo, temos que:</p>
<p><span class="math display">Y = g({\bf X}) = \beta^{T}{\bf X} + \varepsilon,</span></p>
<p>em que <span class="math inline">Y</span> √© um vetor <span class="math inline">n \times 1</span>, <span class="math inline">{\bf X}</span> √© uma matriz fixa e conhecida com os atributos de dimens√£o <span class="math inline">n \times d</span>, em que a primeira coluna √© preenchida de 1, <span class="math inline">\beta = (\beta_0, \cdots, \beta_d)</span>. Na cultura de <em>machine learning</em>, iremos desconsiderar <span class="math inline">\varepsilon</span>, i.e., n√£o feremos suposi√ß√µes sobre <span class="math inline">\varepsilon</span>. Portanto, considere</p>
<p><span class="math display">g({\bf x}) = \beta^{T}{\bf X} = \beta_{0}x_0 + \beta_1x_{i,1} + \cdots + \beta_dx_{i,d},</span> em que <span class="math inline">x_0 \equiv 1</span>.</p>
</section>
<section id="regress√£o-linear-multipla-2" class="slide level2">
<h2>Regress√£o linear multipla</h2>
<p><br></p>
<p>O m√©todo dos m√≠nimos quadrados, para o caso de regress√£o linear m√∫ltipla (<span class="math inline">d &gt; 1</span>) √© dado por aquele que minimiza <span class="math inline">R(\beta^{T}{\bf X})</span>, i.e., minimiza:</p>
<p><span class="math display">\argmin_\beta \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1x_{i,1} - \cdots - \beta_dx_{i,d})^2.</span> Temos que</p>
<p><span class="math display">\widehat{\beta} = ({\bf X}^{T}{\bf X})^{-1}{\bf X}^{T}Y.</span></p>
<p>Portanto, a fun√ß√£o de regress√£o estimada √© dada por:</p>
<p><span class="math display">g({\bf x}) = \widehat{\beta}^{T}{\bf x}.</span></p>
</section>
<section id="regress√£o-linear-multipla-3" class="slide level2">
<h2>Regress√£o linear multipla</h2>
<p><br></p>
<p>Grande parte da literatura estat√≠stica √© voltada para justificar que o m√©todo de m√≠nimos quadrados sob um ponto de vista de um estimador de m√°xima verossimilhan√ßa, assim como tamb√©m para constru√ß√£o de testes de ader√™ncia, m√©todos para constru√ß√£o de intervalos de confian√ßa e teste de hip√≥tese para <span class="math inline">\beta_i</span> (par√¢metros que indexam o modelo), an√°lise de res√≠duos, entre outros.</p>
<p><br></p>
<p>Assumir que a verdadeira regress√£o <span class="math inline">r({\bf x}) = \mathbb{E}({\bf X}\,|\,Y)</span> √© uma suposi√ß√£o muito forte. Contudo, existe, na literatura, justificativas para o uso de m√©todos de m√≠nimos quadrados para estimar os coeficientes, mesmo quando a regress√£o real <span class="math inline">r({\bf x})</span> n√£o satisfaz a suposi√ß√£o de linearidade.</p>
<p><br></p>

<img data-src="gifs/chapulin-colorado-no.gif" class="r-stretch"></section>
<section id="regress√£o-linear-multipla-4" class="slide level2">
<h2>Regress√£o linear multipla</h2>
<p><br></p>
<p>O estimador de m√≠nimos quadrados <span class="math inline">\widehat{\beta} = ({\bf X}^{T}{\bf X})^{-1}{\bf X}^{T}Y</span> √© bom, por alguns motivos:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>√â igual ao estimador de m√°xima verossimilhan√ßa sob normalidade, linearidade e homoscedasticidade, portanto, consistente sob essas condi√ß√µes</p></li>
<li class="fragment"><p>√â <span class="red"><em>best linear unbiased prediction</em> - BLUE</span> sob linearidade e homoscedasticidade;</p></li>
<li class="fragment"><p>O m√©todo de m√≠nimos quadrados tem alguma garantia, mesmo sem assumir muitas suposi√ß√µes.</p></li>
</ol>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="m√≠nimos-quadrados-sem-suposi√ß√£o-de-linearidade" class="slide level2">
<h2>M√≠nimos quadrados sem suposi√ß√£o de linearidade</h2>
<p><br></p>
<p>Quando a suposi√ß√£o de linearidade falha, ou seja, quando a regress√£o verdadeira que desconhecemos <span class="math inline">r({\bf x})</span> n√£o √© linear, frequentemente existe um vetor <span class="math inline">\beta_{*}</span>, tal que <span class="math inline">g_{\beta_{*}}({\bf x}) = \beta_{*}^{T}{\bf x}</span> tem um bom poder preditivo. Nesses casos, o m√©trodo dos m√≠nimos quadrados <span class="math inline">\widehat{\beta}</span> tende a produzir estimadores com baixo risco. Isso se deve ao fato que <span class="math inline">\widehat{\beta}</span> converge para o melhor preditor linear (para o or√°culo <span class="math inline">\beta_{*}</span>) que √© dado por:</p>
<p><span class="math display">\beta_{*} = \argmin_\beta R(g_\beta) =  \argmin_\beta \mathbb{E}\left[(Y - \beta^{T}X)^2\right],</span> mesmo que a verdadeira regress√£o <span class="math inline">r({\bf x})</span> n√£o seja linear, em que <span class="math inline">({\bf X}, Y)</span> √© uma nova observa√ß√£o.</p>
<p><br></p>
<p><span class="red">Teorema</span>: Seja <span class="math inline">\beta_{*}</span> o melhor estimador linear e <span class="math inline">\widehat{\beta}</span> o estimador de m√≠nimos quadrados. Ent√£o,</p>
<p><span class="math display">\widehat{\beta}\overset{p}{\longrightarrow}  \beta_{*}\,\, \mathrm{e}\,\, R(g_{\widehat{\beta}})\overset{p}{\longrightarrow} R(g_{\beta_{*}}), </span> quando <span class="math inline">n \longrightarrow \infty</span>. Para uma demonstra√ß√£o, veja <a href="http://www.rizbicki.ufscar.br/AME.pdf" class="uri">http://www.rizbicki.ufscar.br/AME.pdf</a>, p√°gina. 29.</p>
</section>
<section id="m√≠nimos-quadrados-sem-suposi√ß√£o-de-linearidade-1" class="slide level2">
<h2>M√≠nimos quadrados sem suposi√ß√£o de linearidade</h2>
<p><br></p>
<p>Em palavras, o que o Teorema anterior diz √© que mesmo quando a regress√£o verdadeira n√£o √© linear, o estimador de m√≠nimos quadrados √© consistente para nos conduzir a um bom estimador <strong>linear</strong>, ou seja, ao menos conseguiremos o melhor estimador linear como uma aproxima√ß√£o √† <span class="math inline">r({\bf x})</span> que n√£o √© linear.</p>
<p><br></p>
<p>Isso n√£o quer dizer que voc√™ ter√° boas estimativas em todas as situa√ß√µes, muito embora o or√°culo <span class="math inline">\beta_{*}</span>, em muitas situa√ß√µes, ter√° bom poder preditivo. Em outras palavras, em situa√ß√µes que um problema, em sua natureza, n√£o linear, poderemos alcan√ßar boas estimativas por uma aproxima√ß√£o linear pelo m√©todo dos m√≠nimos quadrados.</p>
<p><br></p>

<img data-src="gifs/uau.gif" class="r-stretch"></section>
<section id="leia-mais-sobre-regress√£o-linear" class="slide level2">
<h2>Leia mais sobre regress√£o linear</h2>
<p><br></p>
<p>Caso voc√™ deseje ler um pouco mais sobre regress√£o linear sob homocedasticidade e sob heteroscedasticidades, leia o segundo Cap√≠tulo de minha disserta√ß√£o de mestrado intitulada <strong>Estimadores Intervalares sob Heteroscedasticidade de Forma Desconhecida via Bootstrap Duplo</strong>. Apesar do t√≠tulo, o segundo cap√≠tulo √© uma revis√£o do conceito de regress√£o linear √© apresentado de forma did√°tica. Clique <a href="pdf/regressao_linear.pdf">aqui</a> para ler.</p>
<p><br></p>

<img data-src="gifs/chapulin-colorado-no.gif" style="width:20.0%" class="r-stretch"></section>
<section id="predi√ß√£o-versus-infer√™ncia" class="slide level2">
<h2>Predi√ß√£o versus Infer√™ncia</h2>
<p><br></p>
<p><strong>Infer√™ncia</strong>: assume que o modelo linear √© correto. O principal objetivo consiste em interpretar os par√¢metros:</p>
<p><br></p>
<ul>
<li class="fragment">Quais s√£o os par√¢metros significantes?</li>
<li class="fragment">Qual o efeito do aumento da dose de um rem√©dio no paciente?</li>
</ul>
<p><br></p>
<p><strong>Predi√ß√£o</strong>: queremos criar <span class="math inline">g({\bf x})</span> com bom poder preditivo, mesmo que a especifica√ß√£o do modelo n√£o esteja correta. N√£o assume que a verdadeira regress√£o √© de fato linear! A interpreta√ß√£o aqui n√£o √© o foco. Tudo bem?!</p>
<p><br></p>

<img data-src="gifs/ok.gif" style="width:15.0%" class="r-stretch"></section>
<section id="ajustando-uma-regress√£o-linear-no-r" class="slide level2">
<h2>Ajustando uma regress√£o linear no R</h2>
<p><br></p>
<p>Caso voc√™ n√£o queira implementar o estimador de m√≠nimos quadrados <span class="math inline">\widehat{\beta} = ({\bf X}^{T}{\bf X})^{-1}{\bf X}^{T}Y</span>, voc√™ poder√° utilizar a famosa fun√ß√£o <code>lm</code>. Na verdade √© melhor que n√£o implemente o estimador <span class="math inline">\widehat{\beta}</span>, uma vez que a fun√ß√£o <code>lm</code>, assim como a fun√ß√£o <code>glmnet</code> do pacote <a href="https://glmnet.stanford.edu/articles/glmnet.html">glmnet</a>, utilizam-se de truques num√©ricos para um c√°lculo mais eficiente.</p>
<p><br></p>
<p>Falaremos do pacote <a href="https://glmnet.stanford.edu/articles/glmnet.html">glmnet</a>, um pouco mais a frente, quando abordarmos regress√£o penalizada. Certo!?</p>
<p><br></p>

<img data-src="gifs/bom.gif" class="r-stretch"></section>
<section id="ajustando-uma-regress√£o-linear-no-r-1" class="slide level2">
<h2>Ajustando uma regress√£o linear no R</h2>
<p><br></p>
<p>Considere o conjunto de dados de expectativa de vida versus PIB per Capita dispon√≠veis <a href="https://github.com/prdm0/dados/blob/main/dados_expectativa_renda.RData">aqui</a>. O comportamente entre as vari√°veis <code>LifeExpectancy</code> e <code>GDPercapita</code>, se fizermos um gr√°fico, n√£o √© linear.</p>
<p><br></p>
<p>Todavia, isso n√£o impede que possamos ajustar um modelo de regress√£o linear, muito embora o seu poder preditivo ser√° baixo.</p>
<p><br></p>
<p>Por√©m, como j√° sabemos, ao menos conseguiremos o melhor or√°culo, denotado por <span class="math inline">\beta_{*}</span>, i.e., o melhor estimador dentre os poss√≠veis estimadores lineares, como mostrado em teoremas anteriores.</p>
<p><br></p>
<p>E est√° tudo bem. Aqui n√£o estou querendo defender que voc√™ use uma aproxima√ß√£o linear para esse caso. Em breve, com um pequeno truque, poderemos ajustar uma regress√£o polinomial √† esses dados, e incorporaremos um pouco da tend√™ncia n√£o linar presente nos dados.</p>
</section>
<section id="ajustando-uma-regress√£o-linear-no-r-2" class="slide level2">
<h2>Ajustando uma regress√£o linear no R</h2>
<p><br></p>
<div class="cell" data-layout-align="center">
<details>
<summary>Veja o c√≥digo do gr√°fico</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb4-2"><a href="#cb4-2"></a></span>
<span id="cb4-3"><a href="#cb4-3"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="co"># Criando um arquivo tempor√°rio</span></span>
<span id="cb4-6"><a href="#cb4-6"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb4-7"><a href="#cb4-7"></a></span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="co"># Baixando um arquivo tempor√°rio</span></span>
<span id="cb4-9"><a href="#cb4-9"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb4-10"><a href="#cb4-10"></a></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="co"># Carregando os dados</span></span>
<span id="cb4-12"><a href="#cb4-12"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb4-13"><a href="#cb4-13"></a></span>
<span id="cb4-14"><a href="#cb4-14"></a>dados_expectativa_renda <span class="sc">|&gt;</span> </span>
<span id="cb4-15"><a href="#cb4-15"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> GDPercapita, <span class="at">y =</span> LifeExpectancy)) <span class="sc">+</span></span>
<span id="cb4-16"><a href="#cb4-16"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb4-17"><a href="#cb4-17"></a>  <span class="fu">labs</span>(</span>
<span id="cb4-18"><a href="#cb4-18"></a>    <span class="at">title =</span> <span class="st">"PIB per Capita versus Expectativa de Vida"</span>,</span>
<span id="cb4-19"><a href="#cb4-19"></a>    <span class="at">x =</span> <span class="st">"PIB per Capita"</span>,</span>
<span id="cb4-20"><a href="#cb4-20"></a>    <span class="at">y =</span> <span class="st">"Expectativa de Vida"</span></span>
<span id="cb4-21"><a href="#cb4-21"></a>  ) <span class="sc">+</span></span>
<span id="cb4-22"><a href="#cb4-22"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb4-23"><a href="#cb4-23"></a>  <span class="fu">theme</span>(</span>
<span id="cb4-24"><a href="#cb4-24"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>),</span>
<span id="cb4-25"><a href="#cb4-25"></a>    <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb4-26"><a href="#cb4-26"></a>  )</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="index_files/figure-revealjs/unnamed-chunk-5-1.png" width="1200" height="700"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="ajustando-uma-regress√£o-linear-no-r-3" class="slide level2">
<h2>Ajustando uma regress√£o linear no R</h2>
<p><br></p>
<p>Claramente, a reta de regress√£o (linha azul) do gr√°fico anterior n√£o tem um bom poder preditivo. O ajuste foi feito diretamente usando o pacote <a href="https://ggplot2.tidyverse.org/">ggplot2</a>, utilizando a fun√ß√£o <code>geom_smooth</code>, em que foi escolhido o m√©todo <code>"lm"</code>.</p>
<p><br></p>
<p>Poder√≠amos ter utilizado a fun√ß√£o <code>lm</code>:</p>
<p><br></p>
<div class="cell" data-layout-align="center">
<details>
<summary>Veja o c√≥digo do gr√°fico</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb5-2"><a href="#cb5-2"></a></span>
<span id="cb5-3"><a href="#cb5-3"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="co"># Criando um arquivo tempor√°rio</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb5-7"><a href="#cb5-7"></a></span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="co"># Baixando um arquivo tempor√°rio</span></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb5-10"><a href="#cb5-10"></a></span>
<span id="cb5-11"><a href="#cb5-11"></a><span class="co"># Carregando os dados</span></span>
<span id="cb5-12"><a href="#cb5-12"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb5-13"><a href="#cb5-13"></a></span>
<span id="cb5-14"><a href="#cb5-14"></a><span class="co"># Ajustando o modelo usando a fun√ß√£o lm</span></span>
<span id="cb5-15"><a href="#cb5-15"></a>ajuste <span class="ot">&lt;-</span> <span class="fu">lm</span>(LifeExpectancy <span class="sc">~</span> GDPercapita, <span class="at">data =</span> dados_expectativa_renda)</span>
<span id="cb5-16"><a href="#cb5-16"></a></span>
<span id="cb5-17"><a href="#cb5-17"></a>modelo <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb5-18"><a href="#cb5-18"></a>  novos_dados <span class="ot">&lt;-</span> tibble<span class="sc">::</span><span class="fu">tibble</span>(<span class="at">GDPercapita =</span> x)</span>
<span id="cb5-19"><a href="#cb5-19"></a>  <span class="fu">predict</span>(ajuste, <span class="at">newdata =</span> novos_dados)</span>
<span id="cb5-20"><a href="#cb5-20"></a>}</span>
<span id="cb5-21"><a href="#cb5-21"></a></span>
<span id="cb5-22"><a href="#cb5-22"></a>dados_expectativa_renda <span class="sc">|&gt;</span> </span>
<span id="cb5-23"><a href="#cb5-23"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> GDPercapita, <span class="at">y =</span> LifeExpectancy)) <span class="sc">+</span></span>
<span id="cb5-24"><a href="#cb5-24"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb5-25"><a href="#cb5-25"></a>  <span class="fu">labs</span>(</span>
<span id="cb5-26"><a href="#cb5-26"></a>    <span class="at">title =</span> <span class="st">"PIB per Capita versus Expectativa de Vida"</span>,</span>
<span id="cb5-27"><a href="#cb5-27"></a>    <span class="at">x =</span> <span class="st">"PIB per Capita"</span>,</span>
<span id="cb5-28"><a href="#cb5-28"></a>    <span class="at">y =</span> <span class="st">"Expectativa de Vida"</span></span>
<span id="cb5-29"><a href="#cb5-29"></a>  ) <span class="sc">+</span></span>
<span id="cb5-30"><a href="#cb5-30"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> modelo, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">size =</span> <span class="fl">1.2</span>) <span class="sc">+</span></span>
<span id="cb5-31"><a href="#cb5-31"></a>  <span class="fu">theme</span>(</span>
<span id="cb5-32"><a href="#cb5-32"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>),</span>
<span id="cb5-33"><a href="#cb5-33"></a>    <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb5-34"><a href="#cb5-34"></a>  )</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="index_files/figure-revealjs/unnamed-chunk-6-1.png" width="1200" height="700"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="matriz-esparsa" class="slide level2">
<h2>Matriz esparsa</h2>
<p><br></p>
<p>Para grandes bases de dados, em um problema real que voc√™ venha trabalhar, e se o custo computacional voc√™ considera elevado, poder√° utilizar o pacote <a href="https://cran.r-project.org/web/packages/biglm/index.html">biglm</a>.</p>
<p><br></p>
<p>Em situa√ß√µes em que h√° muitos zeros na sua matriz, poder√° utilizar representa√ß√£o <a href="https://en.wikipedia.org/wiki/Sparse_matrix">esparsa</a>.</p>
<p><br></p>
<p><span class="red">Matrizes esparsas</span> s√£o matrizes com muitas entradas iguais √† <span class="math inline">0</span>. Elas ocorrem naturalmente em diversas aplica√ß√µes, como por exemplo uma matriz de termos presentes em um documento, em que se o termo estiver no documento resebe 1, e zero, caso contr√°rio. Abaixo, <span class="math inline">{\bf X}</span> √© um exemplo de matriz esparsa.</p>
<p><br></p>
<p><span class="math display">
{\bf X} =
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 3 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 4 &amp; 0 \\
\end{bmatrix}
</span></p>
</section>
<section id="matriz-esparsa-1" class="slide level2">
<h2>Matriz esparsa</h2>
<p><br></p>
<p>Considere os textos:</p>
<ol type="1">
<li class="fragment"><span class="red">Texto 1</span>: ‚ÄúEu amo essa disciplina.‚Äù</li>
<li class="fragment"><span class="red">Texto 2</span>: ‚ÄúEu adoro meu professor.‚Äù</li>
<li class="fragment"><span class="red">Texto 3</span>: ‚ÄúEu serei muito bom em aprendizagem de m√°quina.‚Äù</li>
<li class="fragment"><span class="red">Texto 4</span>: ‚ÄúAdoro o departamento de estat√≠stica da UFPB.‚Äù</li>
</ol>
<p><br></p>
<table>
<colgroup>
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th>Textos</th>
<th>disciplina</th>
<th>amo</th>
<th>aprendizagem</th>
<th>m√°quina</th>
<th>estatistica</th>
<th>adoro</th>
<th>UFPB</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="red">Texto 1</span></td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td><span class="red">Texto 2</span></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td><span class="red">Texto 3</span></td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td><span class="red">Texto 4</span></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</section>
<section id="matriz-esparsa-2" class="slide level2">
<h2>Matriz esparsa</h2>
<p><br></p>
<p>A matriz com a ocorr√™ncia de determinados termos nos textos √© dada por:</p>
<p><span class="math display">
{\bf X} =
\begin{bmatrix}
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\
\end{bmatrix}
</span></p>
<p>A representa√ß√£o esparsa de <span class="math inline">{\bf X}</span>, aqui denotada por <span class="math inline">{\bf X_*}</span> √©:</p>
<p><span class="math display">
{\bf X_*} =
\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
2 &amp; 6 &amp; 1 \\
3 &amp; 3 &amp; 1 \\
3 &amp; 4 &amp; 1 \\
4 &amp; 5 &amp; 1 \\
4 &amp; 6 &amp; 1 \\
4 &amp; 7 &amp; 1 \\
\end{bmatrix},
</span> em que as duas primeiras colunas, s√£o as linhas e colunas de <span class="math inline">{\bf X}</span> com valor diferente de 0. A √∫ltima coluna representa o valor.</p>
</section>
<section id="regress√£o-linear-com-matriz-esparsa" class="slide level2">
<h2>Regress√£o linear com matriz esparsa</h2>
<p><br></p>
<p><strong>Exemplo</strong>: Ajuste de um modelo de regerss√£o linear m√∫ltiplo, em que <span class="math inline">{\bf X}</span> poder√° ter uma representa√ß√£o esparsa. Aqui n√£o estamos interessados em verificar qualidade de predi√ß√µes. Trata-se apenas de um exemplo de como utilizar uma representa√ß√£o esparsa para ajustar um modelo de regess√£o linear com algumas covari√°veis, em R.</p>
<p><br></p>
<div class="cell">
<details>
<summary>Estude o c√≥digo</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="fu">library</span>(Matrix)</span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="co"># Dados de exemplo</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb6-6"><a href="#cb6-6"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">0</span>)</span>
<span id="cb6-7"><a href="#cb6-7"></a>x3 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">7</span>)</span>
<span id="cb6-8"><a href="#cb6-8"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb6-9"><a href="#cb6-9"></a></span>
<span id="cb6-10"><a href="#cb6-10"></a><span class="co"># Criar data frame com as vari√°veis explicativas</span></span>
<span id="cb6-11"><a href="#cb6-11"></a>dados <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x1, x2, x3)</span>
<span id="cb6-12"><a href="#cb6-12"></a></span>
<span id="cb6-13"><a href="#cb6-13"></a><span class="co"># Converter o data frame para matriz esparsa</span></span>
<span id="cb6-14"><a href="#cb6-14"></a>X <span class="ot">&lt;-</span> <span class="fu">sparse.model.matrix</span>(<span class="sc">~</span> ., <span class="at">data =</span> dados)</span>
<span id="cb6-15"><a href="#cb6-15"></a></span>
<span id="cb6-16"><a href="#cb6-16"></a><span class="co"># Ajustar a regress√£o linear utilizando glmnet</span></span>
<span id="cb6-17"><a href="#cb6-17"></a>modelo <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> X, <span class="at">y =</span> y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> <span class="dv">0</span>)</span>
<span id="cb6-18"><a href="#cb6-18"></a></span>
<span id="cb6-19"><a href="#cb6-19"></a><span class="co"># Realizar previs√µes</span></span>
<span id="cb6-20"><a href="#cb6-20"></a>predicoes <span class="ot">&lt;-</span> <span class="fu">predict</span>(modelo, <span class="at">newx =</span> X)</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="erro-quadr√°tico-m√©dio" class="slide level2">
<h2>Erro quadr√°tico m√©dio</h2>
<p><br></p>
<p>Como exposto anteriormente, para avaliar o poder preditivo de uma modelo, i.e., a aprendizagem de um modelo, devemos avaliar a fun√ß√£o de risco, i.e., devemos avaliar <span class="math inline">R(g) := \mathbb{E}\left[L(g({\bf X}); Y)\right]</span>. Em particular, considere <span class="math inline">L = L_2</span> (fun√ß√£o perda quadr√°tica). Ent√£o, poder√≠amos ser levados a acreditar que o melhor estimador de <span class="math inline">R(g)</span>, utilizando a Lei dos Grandes N√∫meros seria:</p>
<p><span class="math display">\frac{1}{n}\sum_{i = 1}^n(Y_{i} - g({\bf X_{i}}))^2 \approx R(g) := \mathbb{E}\left[L_2(g({\bf X}); Y)\right].</span></p>
<p><br></p>
<p>Essa quantidade √© chamada, de <strong>E</strong>rro <strong>Q</strong>uadr√°tico <strong>M</strong>√©dio - <strong>EQM</strong>. Desejamos escolher o melhor mode, entre os modelos testados, que minimiza o EQM.</p>
<p><br></p>
<p>O apelo frequentista em utilizar a Lei dos Grandes N√∫meros na forma acima n√£o √© correto, uma vez que usamos as <span class="math inline">n</span> observa√ß√µes para treinar/ajustar o modelo <span class="math inline">g</span>.</p>
</section>
<section id="erro-quadr√°tico-m√©dio-1" class="slide level2">
<h2>Erro quadr√°tico m√©dio</h2>
<p><br></p>
<p>Por exemplo, no problema de PIB per Capita versus expectativa de vida, em que consideramos uma aproxima√ß√£o linear, n√£o poder√≠amos utilizar o EQM da forma acima, com as <span class="math inline">n</span> observa√ß√µes utilizadas para treinar o modelo. √â um detalhe sutil, mas que muitas pessoas cometem esse erro.</p>
<p><br></p>
<p>N√£o podemos utilizar as <span class="math inline">n</span> observa√ß√µes para estimar o risco <span class="math inline">R(g)</span> atrav√©s do EQM, uma vez que estamos utilizando o mesmo conjunto de dados para ajustar e avaliar <span class="math inline">g</span>.</p>
<p><br></p>
<p><strong>Qual o problema?</strong></p>
<p><br></p>
<ol type="1">
<li class="fragment">N√£o vale a Lei dos Grandes N√∫meros;</li>
<li class="fragment">Usamos os mesmos valores de <span class="math inline">{\bf x}</span> e <span class="math inline">y</span> para treinar e avaliar o modelo.</li>
</ol>
</section>
<section id="erro-quadr√°tico-m√©dio-2" class="slide level2">
<h2>Erro quadr√°tico m√©dio</h2>
<p><br></p>
<p>O que diz a Lei dos Grandes N√∫meros, em particular, a Lei Forte de Kolmogorov?</p>
<p><br></p>
<p><span class="red">Teorema</span> (<strong>Lei Forte de Kolmogorov</strong>): Sejam <span class="math inline">X_1, \cdots, X_n</span> uma sequ√™ncia de veri√°veis aleat√≥rias - v.a. i.i.d. e integr√°veis, i.e., com valor esperado limitado, tal que <span class="math inline">\mathbb{E}(X) = \mu\,\, \forall i</span>. Ent√£o,</p>
<p><span class="math display">\frac{X_1 + X_2 + \cdots + X_n}{n} \rightarrow \mu,</span></p>
<p>quase certamente, i.e., com probabilidade 1.</p>
<p><br></p>
<p>Note que se desejamos comparar diversos modelos, <span class="math inline">g_1({\bf x}), g_2({\bf x}), \cdots,</span> e se utilizarmos as mesmas <span class="math inline">n</span> oberva√ß√µes para calularmos <span class="math inline">R(g_1({\bf x})), R(g_2({\bf x})), \cdots</span>, os termos de cada uma das somas <strong>n√£o s√£o independentes</strong>. Lembre-se que desejamos obter <span class="math inline">\argmin_g R_{pred}(g)</span>.</p>
</section>
<section id="erro-quadr√°tico-m√©dio-3" class="slide level2">
<h2>Erro quadr√°tico m√©dio</h2>
<p><br></p>
<p>Portanto, nunca utilize as mesmas observa√ß√µes utilizadas para treinar o modelo, como aquelas que ser√£o utilizadas para se estimar <span class="math inline">R(g)</span>. Nunca! Isso √© um pecado mortal! Ok?!</p>
<p><br></p>

<img data-src="gifs/thumbs-up-nod.gif" style="width:20.0%" class="r-stretch"></section>
<section id="data-splitting" class="slide level2">
<h2>Data Splitting</h2>
<p><br></p>
<p>Corrigir o problema de depend√™ncia que h√° ao estimarmos o risco usando o EQM √© f√°cil. Uma abordagem muito utilizada √© utilizar <span class="red"><em>data splitting</em></span>, tamb√©m chamado de m√©todo <span class="red"><em>hold-out</em></span>. Algo como a segunda linha da imagem abaixo:</p>
<p><br></p>

<img data-src="imgs/train-and-test-1-min-1.webp" style="width:35.0%" class="r-stretch"></section>
<section id="data-splitting-1" class="slide level2">
<h2>Data Splitting</h2>
<p><br></p>
<p>Essa divis√£o √© feita de forma aleat√≥ria, algumas vezes estratificada de acordo com algumas vari√°v√°veis. A ideia de aleatorizar √© se livrar de problemas de conjunto de dados ordenados. Queremos que tanto no conjunto de treinamento <span class="red"><em>Training</em></span> quanto no conjunto <span class="red"><em>Testing</em></span>, na imagem, contenham a mesma diversidade de observa√ß√µes.</p>
<p><br></p>
<p>Ainda no exemplo de PIB per Capita versus Expectaitiva de Vida, n√£o quero correr o risco de ter no conjunto de treinamento apenas o pa√≠ses com maiores valores de PIB per Capita, caso o conjunto de dados tenha sido ordenado pela vari√°vel <code>GDPercapita</code>. Por isso, aleatorizar o conjunto de treinamento e teste √© sempre uma √≥tima ideia. Certo!?</p>
<p><br></p>

<img data-src="gifs/ok-2.gif" style="width:20.0%" class="r-stretch"></section>
<section id="data-splitting-2" class="slide level2">
<h2>Data Splitting</h2>
<p><br></p>
<p>O percentual de divis√£o dos dados normalmente √© emp√≠rico. Usa-se normalmente a propor√ß√£o de <span class="math inline">70\%</span> para treinamento e <span class="math inline">30\%</span> para teste <span class="math inline">(70\%, 30\%)</span>. Outros esquemas de divis√µes s√£o bastante utilizados, por exemplo, <span class="math inline">(80\%, 20\%)</span>, <span class="math inline">(99\%, 1\%)</span>, a depender da quantidade de observa√ß√µes (tamanho do conjunto de dados).</p>
<p><br></p>
<p>Portanto, utilizar o EQM sob o conjunto de dados de teste para avaliar <span class="math inline">g_1({\bf x}), g_2({\bf x}), \cdots,</span>, √© uma boa estrat√©gia, uma vez que agora n√£o teremos mais uma depend√™ncia no numerador do c√°lculo do EQM. Em nota√ß√£o matem√°tica, poder√≠amos escrever como j√° apresentado anteriormente, em <a href="#/fun√ß√£o-de-risco-3" class="quarto-xref">Equa√ß√£o&nbsp;1</a>, i.e.,</p>
<p><span class="math display">\frac{1}{m}\sum_{i = 1}^m (Y_{n + i} - g(X_{n + i}))^2 \approx R(g) := \mathbb{E}\left[(Y - g({\bf X}))^2\right].</span></p>
<p><br></p>
<p>Esse resultado valeria para qualquer outra fun√ß√£o de perda.</p>
</section>
<section id="data-splitting-3" class="slide level2">
<h2>Data Splitting</h2>
<p>Reescrevendo, suponha que o conjunto de dados total possua <span class="math inline">n</span> observa√ß√µes e que separamos aleatoriamente <span class="math inline">s &lt; n</span> observa√ß√µes para o conjunto de treinamento. Assim, temos, algo como:</p>
<p><br></p>
<p><span class="math display">\overbrace{(X_1, Y_1), (X_2, Y_2), \cdots, (X_s, Y_s)}^{70\%}, \,\,\, \overbrace{(X_{s + 1}, Y_{s + 1}), (X_{s + 2}, Y_{s + 2}), \cdots, (X_n, Y_n)}^{30\%}.</span></p>
<p><br></p>
<p>Ent√£o, temos que uma boa estimativa de <span class="math inline">R(g)</span> √© dada pelo EQM calculado sobre o conjunto de dados de teste, que nesse caso considerei o conjunto com <span class="math inline">30\%</span>, mas esse percentual poderia ser outro. Ent√£o, temos que um bom estimador √©:</p>
<p><span class="math display">\frac{1}{n - s}\sum_{i = s + 1}^n (Y_{i} - g(X_{i}))^2 \approx R(g) := \mathbb{E}\left[(Y - g({\bf X}))^2\right].</span></p>
</section>
<section id="data-splitting-4" class="slide level2">
<h2>Data Splitting</h2>
<p><br></p>
<p><strong>Agora voc√™ entende por que dividimos os dados em treinamento e teste?</strong></p>
<p><br></p>

<img data-src="gifs/yes.gif" style="width:25.0%" class="r-stretch"><p><br></p>
<p>Dividimos para obermos um bom estimador do risco utilizando o <a href="https://en.wikipedia.org/wiki/Mean_squared_error">EQM</a>. üéÅ</p>
</section>
<section id="data-splitting-5" class="slide level2">
<h2>Data Splitting</h2>
<p><br></p>
<p>Podemos argumentar que o procedimento de <em>data splitting</em>, em que dividimos o nosso conjunto de dados em treinamento e teste far√° com que venhamos perder muitas observa√ß√µes que poderiam ter sido utilizadas para treinar o modelo. E de certa forma isso √© verdade, principalmente quando termos um conjunto n√£o muito grande de observa√ß√µes.</p>
<p><br></p>
<p>Portanto, uma melhor abordagem, sendo esta uma varia√ß√£o do m√©todo de <em>data splitting</em> √© o procedimento de <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)"><em>cross-validation - cv (valida√ß√£o cruzada)</em></a>. Uma vers√£o mais geral de uma valida√ß√£o cruzada √© o <span class="red"><em>leave-one-out cross-validation</em></span>.</p>
<p><br></p>
<p>Em palavras, o procedimento consiste em tirar de fora uma √∫nica observa√ß√£o das <span class="math inline">n</span> observa√ß√µes da base de dados para ser o nosso conjunto de teste e treinar o modelo com as observa√ß√µes que permaneceram. Da√≠, calcula-se o <strong>risco observado</strong> (EQM, sob o conjunto de teste/valida√ß√£o). Na segunda itera√ß√£o, a observa√ß√£o que antes era de teste volta para perterncer ao conjunto de treinamento e uma nova observa√ß√£o √© removida para ser teste. Esse procedimento ocorre de forma iterativa at√© a retirada da √∫ltima observa√ß√£o como teste.</p>
</section>
<section id="leave-one-out-cross-validation" class="slide level2">
<h2>Leave-one-out cross-validation</h2>
<p><br></p>
<p>Observe a anima√ß√£o abaixo que ilustra o procedimento de <strong>l</strong>eave-<strong>o</strong>ne-<strong>o</strong>ut <strong>c</strong>ross-<strong>v</strong>alidation - LOOCV, em uma amostra de tamanho <span class="math inline">n = 8</span>. Ao fim, teremos <span class="math inline">n</span> modelos ajustados, em que calculamos as suas respectivas performances, i.e., com o risco observado, estimamos o risco de <span class="math inline">R(g)</span>.</p>
<p><br></p>

<img data-src="gifs/LOOCV.gif" class="r-stretch"></section>
<section id="leave-one-out-cross-validation-1" class="slide level2">
<h2>Leave-one-out cross-validation</h2>
<p><br></p>
<p>Vejo muitas pessoas que usam uma valida√ß√£o cruzada, por exemplo, <em>leave-one-out cross-validation</em> - LOOCV comparando com o m√©todo Jackknife e algumas inclusive dizendo ser a mesma coisa. N√£o, n√£o s√£o!</p>
<p><br></p>
<p>O algoritmo Jackknife √© um procedimento de estima√ß√£o, e que, por sua vez, deve estar dentro do conjunto de treinamento. Para haver algum Jackknife, a estimativa com <span class="math inline">n-1</span> observa√ß√µes deve estar dentro do conjunto de treinamento, em que dentro do treinamento teria a remo√ß√£o de um observa√ß√£o por vez. <strong>Consegue perceber a diferen√ßa sutil?</strong></p>
<p><br></p>

<img data-src="gifs/bean_01.gif" style="width:20.0%" class="r-stretch"></section>
<section id="leave-one-out-cross-validation-2" class="slide level2">
<h2>Leave-one-out cross-validation</h2>
<p><br></p>
<p>O m√©todo LOOCV foi proposto por Stones (1974), no artigo intitulado Cross-Validatory Choice and Assessment of Statistical Predictions, no Royal Statistical Society, S√©rie B. Clique <a href="https://www.jstor.org/stable/pdf/2984809.pdf?refreqid=excelsior%3A3071b86b3588905b095d44668025b005&amp;ab_segments=&amp;origin=&amp;initiator=&amp;acceptTC=1">aqui</a> se tiver curiosidade em ler o artigo.</p>
<p><br></p>
<p>Escrevendo o estimador do risco em um procedimento de LOOCV, temos que:</p>
<p><span class="math display">\widehat{R}(g) = \frac{1}{n}\sum_{i = 1}^n (Y_i - g_{-i}({\bf X}_i))^2,</span> em que <span class="math inline">g_{-i}(\bf{X}_i)</span>, representa o ajuste do modelo no conjunto de dados sem a <span class="math inline">i</span>-√©sima observa√ß√£o.</p>
<p><br></p>
<p>N√£o √© dif√≠cil perceber que a depender do valor de <span class="math inline">n</span>, o m√©todo LOOCV √© <strong>computacionalmente intensivo</strong>. O m√©todo requer que ajustemos <span class="math inline">n</span> modelos. Em algumas situa√ß√µes isso n√£o √© um grande problema, por√©m, em diversas outras pode ser impeditivo utilizar o LOOCV. ü§Ø</p>
</section>
<section id="leave-one-out-cross-validation-3" class="slide level2">
<h2>Leave-one-out cross-validation</h2>
<p><br></p>
<p>O m√©todo LOOCV converge assintotivamente para o AIC, por√©m, este, muitas vezes n√£o poderemos calcular diretamente, uma vez que n√£o conhecemos a distribui√ß√£o conjunta dos dados, i.e., n√£o conhecemos a estrutura probabil√≠stica.</p>
<p><br></p>

<img data-src="gifs/interesting-batman.gif" class="r-stretch"></section>
<section id="leave-one-out-cross-validation-4" class="slide level2">
<h2>Leave-one-out cross-validation</h2>
<p><br></p>
<p>Essa rela√ß√£o entre o LOOCV e o <em>Akaike Information Criterion</em> - AIC foi provada no paper Stone (1977) intitulado <strong>An Asymptotic Equivalence of Choice of Model by Cross-Validation and Akaike‚Äôs Criterion</strong> e publicado no <em>Journal of the Royal Statistical Society, Series B</em>. Clique <a href="https://iri.columbia.edu/~tippett/cv_papers/Stone1977.pdf">aqui</a>, se quiser ler o artigo.</p>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="imgs/loocn_aic.png"></p>
</div><div class="column" style="width:50%;">
<p><br> <br> <img data-src="gifs/thumbs-up-nod.gif"></p>
</div>
</div>
</section>
<section id="k-fold-cross-validation" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p><br></p>
<p>Uma alternativa ao LOOCV √© utilizar o m√©todo <span class="math inline">k</span>-<em>fold cross-validation</em>. Nessa abordagem, dividimos o conjunto de dados em <span class="math inline">k</span>-<em>folds</em> (lotes) disjuntos e com aproximadamente o mesmo tamanho. Dessa forma, temos <span class="math inline">L_1, \cdots, L_k \subset \{1, \cdots, n\}</span> s√£o, cada um, um conjunto de √≠ndices aleat√≥rios associados a cada um dos lotes. A ideia aqui √© construir <span class="math inline">k</span> estimadores da fun√ß√£o de regress√£o, denotados por <span class="math inline">\widehat{g}_{-1}, \cdots, \widehat{g}_{-k}</span>, em que <span class="math inline">\widehat{g}_{-j}</span> √© criado usando todas as observa√ß√µes do banco de dados, com exce√ß√£o daquelas do lote <span class="math inline">L_j</span>, utilizado para <strong>valida√ß√£o</strong>. O estimador do risco √© dado por:</p>
<p><span class="math display">\widehat{R}(g) = \frac{1}{n}\sum_{j=1}^k \sum_{i \in L_j}(Y_i - g_{-j}({\bf X}_i))^2.</span> Perceba que, que o LOOCV √© um caso particular do <span class="math inline">k</span>-<em>fold cross-validation</em>, quando fazemos <span class="math inline">k = n</span>. Em outras palavras, <span class="math inline">L_1, \cdots, L_k \subset \{1, \cdots, n\}</span> representam os √≠ndices aleat√≥rios do conjunto de treinamento nos <span class="math inline">k</span> lotes.</p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="k-fold-cross-validation-1" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p>A anima√ß√£o abaixo, ilustra o procedimento de <span class="math inline">3</span>-<em>fold cross-validation</em> (<span class="math inline">k = 3</span>), para uma amostra de tamanho <span class="math inline">n = 12</span> observa√ß√µes. Note que os valores que pertencem a cada um dos lotes s√£o aleat√≥rios. Portanto, o procedimento LOOCV √© deterministico, j√° o procedimento de <span class="math inline">k</span>-<em>fold cross-validation</em> √© randomizado.</p>
<p><br></p>

<img data-src="gifs/KfoldCV.gif" class="r-stretch"><p>Perceba que teremos agora apenas <span class="math inline">3</span> modelos. Para cada um desses lotes, calulamos o EQM com o conjunto de teste (parte <span class="trueblue">azul</span>) e treinamos o modelo com o conjunto de treinamento (parte <span class="truered">vermelha</span>).</p>
</section>
<section id="k-fold-cross-validation-2" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p><br></p>
<p>Muitos modelos mais sofisticados apresentam hiperpar√¢metros (par√¢metros de sintoniza√ß√£o) que n√£o dependem dos dados. √â muito comum os algoritmos de aprendizagem de m√°quina se utilizarem do procedimento de valida√ß√£o cruzada, para al√©m da estima√ß√£o do risco <span class="math inline">R(g)</span> no conjunto de valida√ß√£o.</p>
<p><br></p>
<p>Ao estimar <span class="math inline">k</span> modelos, normalmente faz-se um <em>grid</em> de poss√≠veis valores para esses hiperpar√¢metros em que ao final, escolhe-se como hiperpar√¢metro o modelo com menor EQM. Por fim, ajusta-se um modelo final, com todo o conjunto de treinamento usando o valor do hiperpar√¢metro que retornou o menor EQM no conjunto de valida√ß√£o.</p>
<p><br></p>
<p>Alias, utilizamos um procedimento de valida√ß√£o para selecionar a melhor combina√ß√£o de hiperpar√¢metros e √© ser√° risco preditivo observado sob o conjunto de teste que realmente ir√° nos fornecer uma estimativa v√°lida do risco preditivo <span class="math inline">R(g)</span>.</p>
<p><br></p>

<img data-src="gifs/ok.gif" class="r-stretch"></section>
<section id="k-fold-cross-validation-3" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p>O termo <span class="red">valida√ß√£o</span> refere-se √† parcela do conjunto de treinamento incial que dividimos em valida√ß√£o e treinamento, dentro de uma valida√ß√£o cruzada.</p>
<p><br></p>
<p>O conjunto <span class="red"><em>Testing</em></span> na segunda hierarquia da √°rvore ao lado, s√≥ usamos no final para avaliar o desempenho do modelo nesse conjunto. Isto √©, usamos o <span class="red"><em>Testing</em></span> para o c√°lculo do <strong>risco observado</strong>.</p>
<p><br></p>
<p>Perceba que o conjunto de treinamento (<span class="red"><em>Not Testing</em></span>) √© particionado em treinamento e valida√ß√£o. Poder√≠amos fazer uma √∫nica parti√ß√£o, mas o procedimento comumente utilizado √© particionar entre <span class="red"><em>Training</em></span> e <span class="red"><em>Validation</em></span> uzando algum procedimento de valida√ß√£o cruzada.</p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/validation-split.svg" style="width:64.0%"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="k-fold-cross-validation-4" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p><br></p>
<p>Em alguns livros o conjunto de treinamento √© denominado de <em>not-testing</em> (n√£o-teste). Isso, porqu√™ eles querem enfatizar o fato de que o conjunto <em>not-testing</em> √© utilizado para treinar/ensinar o modelo e jamais dever√° ser utilizado para avaliar o risco preditivo <span class="math inline">R(g)</span>.</p>
<p><br></p>
<p>Ao usar essa terminologia, os autores dos livros tentam enfatizar que o conjunto de treinamento √© usado exclusivamente para ensinar o modelo a aprender os padr√µes nos dados, enquanto o conjunto de teste √© usado para medir qu√£o bem o modelo generaliza esses padr√µes para dados n√£o vistos anteriormente.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="k-fold-cross-validation-5" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p>A imagem abaixo ilustra o procedimento <span class="math inline">k</span>-<em>fold cross-validation</em>, em que uma <span class="math inline">5</span>-<em>fold cross-validation</em> √© realizada dentro do conjunto de treinamento. Em cada <em>split</em>, o conjunto verde de observa√ß√µes (fold <span class="green">verde</span>) s√£o utilizados para treinar/ajustar o modelo e o conjunto <span class="trueblue">azul</span>, em cada um dos <em>splits</em> √© utilizado para avaliar o risco preditivo <span class="math inline">R(g)</span> (atrav√©s, por exemplo do EQM).</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="imgs/grid_search_cross_validation.png" width="800"> <img data-src="gifs/hum.gif"></p>
</div><div class="column" style="width:60%;">
<p>N√£o confunda os folds azuis com o conjunto de teste (<span class="trueblue">Test data</span>), este √∫ltimo utilizado por fim, depois do modelo pronto, para avaliar o desempenho do modelo treinado.</p>
<p>Note tamb√©m que a valida√ß√£o cruzada tamb√©m √© utilizada para o ajuste de hiperpar√¢metros, que s√£o par√¢metros de sintoniza√ß√£o que n√£o dependem dos dados para serem equalizados. Por exemplo, em uma regress√£o lasso, que veremos adiante, h√° o hiperpar√¢metro <span class="math inline">\lambda</span> que precisamos obter, normalmente por meio de um <span class="red"><em>grid search</em></span> (sequ√™ncia finita), por exemplo, <span class="math inline">\lambda \in [0.5, 1, 1.5, 2, 2.5]</span> de poss√≠veis valores. Cada <em>split</em> pode ser utilizado para avaliar um valor de <span class="math inline">\lambda</span>, dos poss√≠veis valores dispostos no grid. Aumentar√≠amos a quantidade de splits para mais valores de <span class="math inline">\lambda</span> na sequ√™ncia.</p>
</div>
</div>
</section>
<section id="resumindo-data-splitting-valida√ß√£o-cruzada-e-conjunto-de-valida√ß√£o" class="slide level2">
<h2>Resumindo: data splitting, valida√ß√£o cruzada e conjunto de valida√ß√£o</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="imgs/data_split_validation_cross.png"></p>
</div><div class="column" style="width:50%;">
<p>O simples procedimento de dividir o conjunto de dados em dois, uma parte para treinar o modelo e a outra parte (conjunto de teste) para estimar o risco <span class="math inline">R(g)</span> √© denominado de <span class="red"><em>data splitting</em></span> ou <span class="red"><em>hold-out method</em></span>. √â um procedimento mais simples, por√©m, pode n√£o ser √∫til em conjunto de dados n√£o muito grandes.</p>
<p>A segunda linha da ilustra√ß√£o, demonstra o procedimento de <em>cross-validation</em> (valida√ß√£o cruzada), procedimento mais utilizado nos treinamentos de modelos de aprendizagem de m√°quina.</p>
<p>A terceira linha √© uma abordagem tamb√©m utilizada, por√©m n√£o t√£o interessante quanto a valida√ß√£o cruzada. Nessa abordagem o banco de dados √© dividido aleatoriamente em tr√™s partes. Treina-se o modelo com a parte <span class="green">verde</span>, estima-se o risco com o conjunto de valida√ß√£o amarelo e testa-se o modelo com o conjunto de teste.</p>
</div>
</div>
</section>
<section id="resumindo-data-splitting-valida√ß√£o-cruzada-e-conjunto-de-valida√ß√£o-1" class="slide level2">
<h2>Resumindo: data splitting, valida√ß√£o cruzada e conjunto de valida√ß√£o</h2>
<p><br> A abordagem do conjunto de valida√ß√£o envolve <strong>dividir o conjunto de treinamento em duas partes</strong>: uma parte √© usada para treinar o modelo e a outra parte √© usada para avaliar o desempenho do modelo para uma dada combina√ß√£o. de hiperpar√¢metros. O conjunto de valida√ß√£o √© utilizado para avaliar os hiperpar√¢metros do modelo, como a taxa de aprendizado, o n√∫mero de camadas ocultas em uma rede neural, entre outros. Ap√≥s o ajuste dos hiperpar√¢metros, o modelo final √© treinado com o conjunto de treinamento completo e avaliado em um conjunto separado chamado conjunto de teste. Essa abordagem √© conhecida como divis√£o simples de treinamento/valida√ß√£o/teste.</p>
<p><br></p>
<p>Por outro lado, a valida√ß√£o cruzada <span class="math inline">k</span>-<em>fold</em> √© uma abordagem que visa obter uma estimativa mais robusta do desempenho do modelo. Nessa abordagem, o conjunto de treinamento √© dividido em <span class="math inline">k</span> subconjuntos (<em>folds</em>) de tamanho aproximadamente igual. O modelo √© treinado <span class="math inline">k</span> vezes, cada vez usando <span class="math inline">k-1</span> <em>folds</em> como conjunto de treinamento e <span class="math inline">1</span> <em>fold</em> como conjunto de valida√ß√£o. O desempenho do modelo √© ent√£o calculado como a m√©dia dos resultados obtidos em cada itera√ß√£o. Isso permite avaliar o modelo, em diferentes combina√ß√µes dos hiperpar√¢metros, de forma mais precisa, pois utiliza todos os dados para treinamento e valida√ß√£o, evitando a depend√™ncia de uma √∫nica divis√£o do conjunto de treinamento.</p>
</section>
<section id="resumindo-data-splitting-valida√ß√£o-cruzada-e-conjunto-de-valida√ß√£o-2" class="slide level2">
<h2>Resumindo: data splitting, valida√ß√£o cruzada e conjunto de valida√ß√£o</h2>
<p><br></p>
<p>A valida√ß√£o cruzada <span class="math inline">k</span>-<em>fold</em> √© particularmente √∫til quando o conjunto de dados √© limitado, pois aproveita ao m√°ximo os dados dispon√≠veis. Al√©m disso, <strong>ela permite verificar se o modelo √© est√°vel e se seu desempenho varia significativamente com diferentes divis√µes dos dados</strong>. √â importante ressaltar que a valida√ß√£o cruzada <span class="math inline">k</span>-<em>fold</em> pode ser computacionalmente mais cara do que a abordagem do conjunto de valida√ß√£o, uma vez que envolve treinar e avaliar o modelo v√°rias vezes.</p>
<p><br></p>

<img data-src="gifs/chapulin-colorado-no.gif" style="width:40.0%" class="r-stretch"></section>
<section id="resumindo-data-splitting-valida√ß√£o-cruzada-e-conjunto-de-valida√ß√£o-3" class="slide level2">
<h2>Resumindo: data splitting, valida√ß√£o cruzada e conjunto de valida√ß√£o</h2>
<p><br></p>
<p>Um outro detalhe que muitas vezes n√£o √© falado √© que apesar de temos duas tarefas de estima√ß√£o, uma envolvendo o conjunto de <strong>treinamento</strong>, em que treinamos o modelo e outra envolvendo o conjunto de <strong>teste</strong>, em que queremos estimar o risco <span class="math inline">R(g)</span>, de modo a poder selecionar o melhor modelo, a segunda tarefa √© bem mais f√°cil. √â por isso que o conjunto de treinamento tende a ser menor que o conjunto de teste.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="resumindo-data-splitting-valida√ß√£o-cruzada-e-conjunto-de-valida√ß√£o-4" class="slide level2">
<h2>Resumindo: data splitting, valida√ß√£o cruzada e conjunto de valida√ß√£o</h2>
<p><br></p>
<p>No <a href="https://www.tidymodels.org/">tidymodels</a>, utilizamos a biblioteca <a href="https://rsample.tidymodels.org/">rsample</a> para realizar procedimentos de valida√ß√£o cruzada:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p><code>initial_split()</code>: √∫til para uma divis√£o inicial dos dados, i.e., para aplica√ß√£o do <em>hold-out</em>;</p></li>
<li class="fragment"><p><code>loo_cv()</code>: se desejar realizar um procedimento de <em>leave-one-out cross-validation</em>;</p></li>
<li class="fragment"><p><code>vfold_cv()</code>: para um procedimento de <span class="math inline">k</span>-<em>folds cross-validation</em>. Podemos inclusive realizar v√°rias repeti√ß√µes de valida√ß√£o cruzada o que poder√° remelhorar ainda mais a sele√ß√£o da melhor combina√ß√£o de hiperpar√¢metros. O n√∫mero de repeti√ß√µes de valida√ß√£o cruzada poder√° ser especificado no argumento <code>repeats</code> que por padr√£o √© <code>1L</code>;</p></li>
<li class="fragment"><p><code>bootstraps()</code>: se for desejado utilizar um procedimento de <em>bootstrap</em> n√£o-param√©trico ao inv√©s de uma valida√ß√£o cruzada. O papel do <em>bootstrap</em> √© o mesmo da valida√ß√£o cruzada, por√©m, a sele√ß√£o das amostras de treinamento √© de mesmo tamanho da amostra de treinamento original e o procedimento √© feito com reposi√ß√£o, i.e., os mesmos dados podem aparecer nas amostras. O conjunto de avalia√ß√£o √© definido como as linhas dos dados originais que n√£o foram inclu√≠dos na amostra bootstrap. Isso geralmente √© chamado de amostra <span class="red"><em>out-of-bag</em></span> - OOB. Podemos alterar o n√∫mero de amostras <em>bootstrap</em> modificando o argumento <code>times</code> que por padr√£o √© <code>25L</code>.</p></li>
</ol>
<p><br></p>
<p><strong>Lembre-se que qualquer procedimento de valida√ß√£o cruzada que voc√™ escolher deve ser realizado no conjunto de treinamento</strong>! üìå</p>
</section>
<section id="balan√ßo-vi√©s-e-vari√¢ncia" class="slide level2">
<h2>‚öñÔ∏è Balan√ßo vi√©s e vari√¢ncia</h2>
<p><br></p>
<p>A ideia de precis√£o e exatid√£o est√£o ligadas ao vi√©s e vari√¢ncia do modelo <span class="math inline">g</span>, em que precis√£o est√° ligado a ideia de vari√¢ncia pequena e exatid√£o est√° ligada a ideia de baixo vi√©s. A ideia √© termos um estimador pr√≥ximo o que ilustra o item <span class="red">d</span>. Muitas vezes temos um estimador nas situa√ß√µes <span class="red">b</span> e <span class="red">c</span>. O ideal √© o balan√ßo de vi√©s e vari√¢ncia, que seria o estimador ilustrado pelo item <span class="red">d</span>.</p>

<img data-src="imgs/precisao_exatidao.png" style="width:55.0%" class="r-stretch quarto-figure-center"></section>
<section id="balan√ßo-vi√©s-e-vari√¢ncia-1" class="slide level2">
<h2>‚öñÔ∏è Balan√ßo vi√©s e vari√¢ncia</h2>
<p><br></p>
<p>Um grande apelo para o uso do risco quadr√°tico, i.e., risco que utiliza a fun√ß√£o de perda <span class="math inline">L_2</span> √© sua interpretabilidade. Temos que o risco quadr√°tico <span class="math inline">R(g)</span> condicional a um novo <span class="math inline">{\bf x}</span> poder√° ser decomposto por:</p>
<p><span id="eq-decomposicao-risco-l2"><span class="math display">\mathbb{E}\left[(Y - \widehat{g}({\bf X}))^2| {\bf X} = {\bf x}\right] = \underbrace{\mathbb{V}[Y | {\bf X = x}]}_{\mathrm{i - Vari√¢ncia\,\, intr√≠nseca}} + \overbrace{(r({\bf x}) - \mathbb{E}[\widehat{g}({\bf x})])^2}^{\mathrm{ii - Vi√©s\, ao\, quadrado\, do\, modelo}} + \underbrace{\mathbb{V}[\widehat{g}({\bf x})]}_{\mathrm{iii - Vari√¢ncia\, do\, modelo}}. \tag{2}</span></span> <strong>Temos que</strong>:</p>
<p><br></p>
<p>i - √â a vari√¢ncia intr√≠nseca da vair√°vel resposta (<em>label</em>), que n√£o depende da fun√ß√£o <span class="math inline">\widehat{g}</span> escolhida e, assim, n√£o poder√° ser reduzida. Na verdade, poderemos reduzir <span class="math inline">i</span>, se incluirmos mais <em>features</em> (covari√°veis/vari√°veis explicativas) ao nosso modelo;</p>
<p>ii - √â o vi√©s ao quadrado do estimador <span class="math inline">\widehat{g}</span> (vi√©s ao quadrado do modelo);</p>
<p>iii - √â a vari√¢ncia do estimador <span class="math inline">\widehat{g}</span>.</p>
</section>
<section id="balan√ßo-vi√©s-e-vari√¢ncia-2" class="slide level2">
<h2>‚öñÔ∏è Balan√ßo vi√©s e vari√¢ncia</h2>
<p><br></p>
<p>Assim, lembre-se que uma escolha adequada de <span class="math inline">\widehat{g}</span> nos garante que conseguiremos reduzir o risco preditivo <span class="math inline">R(g)</span>, pois a escolha apropriada implica em escolhermos um estimador de <span class="math inline">\widehat{g}</span> com balan√ßo entre v√≠es e vari√¢ncia.</p>
<p><br></p>
<p>Modelos com muitos par√¢metros possuem vi√©s relativamente baixo, por√©m, tendem a ter vari√¢ncia muito alta, em geral, uma vez que precisamos estimar muitos par√¢metros. J√° modelos com poucos par√¢metros, normalmente tendem a ter vari√¢ncia baixa, acompanhados normalmente de um alto vi√©s.</p>
<p><br></p>
<p>Geralmente, modelos com muitos par√¢metros nos levam a termos <span class="red"><em>overffiting</em></span> (super-ajuste), o que n√£o √© bom pois s√£o acompanhados de alta vari√¢ncia. J√° modelos muito simplistas nos conduzem √† um ajuste muito ruim (<span class="red"><em>underffiting</em></span> ou sub-ajuste). Entendeu!?</p>

<img data-src="gifs/mais_ou_menos.gif" class="r-stretch"></section>
<section id="balan√ßo-vi√©s-e-vari√¢ncia-3" class="slide level2">
<h2>‚öñÔ∏è Balan√ßo vi√©s e vari√¢ncia</h2>
<p><br></p>
<div class="cell">
<details>
<summary>Estude o c√≥digo</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="fu">library</span>(tibble)</span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb7-5"><a href="#cb7-5"></a></span>
<span id="cb7-6"><a href="#cb7-6"></a><span class="co"># Fun√ß√£o de regress√£o verdadeira. Na pr√°tica √© desconhecida.</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>regressao_verdadeira <span class="ot">&lt;-</span> <span class="cf">function</span>(x)</span>
<span id="cb7-8"><a href="#cb7-8"></a>  <span class="dv">45</span> <span class="sc">*</span> <span class="fu">tanh</span>(x<span class="sc">/</span><span class="fl">1.9</span> <span class="sc">-</span> <span class="dv">7</span>) <span class="sc">+</span> <span class="dv">57</span></span>
<span id="cb7-9"><a href="#cb7-9"></a></span>
<span id="cb7-10"><a href="#cb7-10"></a>observacoes_regressao_real <span class="ot">&lt;-</span> <span class="cf">function</span>(n, <span class="at">desvio_padrao =</span> <span class="fl">0.2</span>) {</span>
<span id="cb7-11"><a href="#cb7-11"></a>  <span class="co"># Permitindo que o mesmo x possa ter dois pontos de y, como ocorre na </span></span>
<span id="cb7-12"><a href="#cb7-12"></a>  <span class="co"># pratica</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>  seq_x <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">17.5</span>, <span class="at">length.out =</span> n), <span class="at">size =</span> n, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb7-14"><a href="#cb7-14"></a>  </span>
<span id="cb7-15"><a href="#cb7-15"></a>  step <span class="ot">&lt;-</span> <span class="cf">function</span>(x)</span>
<span id="cb7-16"><a href="#cb7-16"></a>    <span class="fu">regressao_verdadeira</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> 1L, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> desvio_padrao)</span>
<span id="cb7-17"><a href="#cb7-17"></a>  </span>
<span id="cb7-18"><a href="#cb7-18"></a>  tibble<span class="sc">::</span><span class="fu">tibble</span>(<span class="at">y =</span> purrr<span class="sc">::</span><span class="fu">map_vec</span>(<span class="at">.x =</span> seq_x, <span class="at">.f =</span> step), <span class="at">x =</span> seq_x)</span>
<span id="cb7-19"><a href="#cb7-19"></a>}</span>
<span id="cb7-20"><a href="#cb7-20"></a></span>
<span id="cb7-21"><a href="#cb7-21"></a><span class="co"># Usaremos uma regress√£o polinomial para tentar ajustar √† regress√£o -------</span></span>
<span id="cb7-22"><a href="#cb7-22"></a>regressao_polinomial <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">n =</span> 30L, <span class="at">desvio_padrao =</span> <span class="dv">4</span>, <span class="at">grau =</span> 1L) {</span>
<span id="cb7-23"><a href="#cb7-23"></a>  </span>
<span id="cb7-24"><a href="#cb7-24"></a>  dados <span class="ot">&lt;-</span> <span class="fu">observacoes_regressao_real</span>(<span class="at">n =</span> n, <span class="at">desvio_padrao =</span> desvio_padrao)</span>
<span id="cb7-25"><a href="#cb7-25"></a>    </span>
<span id="cb7-26"><a href="#cb7-26"></a>  iteracoes <span class="ot">&lt;-</span> <span class="cf">function</span>(tibble_data, grau) {</span>
<span id="cb7-27"><a href="#cb7-27"></a>      x <span class="ot">&lt;-</span> tibble_data<span class="sc">$</span>x</span>
<span id="cb7-28"><a href="#cb7-28"></a>      iteracoes <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="at">X =</span> 2L<span class="sc">:</span>grau, <span class="at">FUN =</span> <span class="cf">function</span>(i) x<span class="sc">^</span>i)</span>
<span id="cb7-29"><a href="#cb7-29"></a>      </span>
<span id="cb7-30"><a href="#cb7-30"></a>      result <span class="ot">&lt;-</span> <span class="fu">cbind</span>(tibble_data, <span class="fu">do.call</span>(cbind, iteracoes))</span>
<span id="cb7-31"><a href="#cb7-31"></a>      <span class="fu">colnames</span>(result)[(<span class="fu">ncol</span>(tibble_data) <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span><span class="fu">ncol</span>(result)] <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">"x"</span>, 2L<span class="sc">:</span>grau)</span>
<span id="cb7-32"><a href="#cb7-32"></a>      </span>
<span id="cb7-33"><a href="#cb7-33"></a>      <span class="fu">as_tibble</span>(result)</span>
<span id="cb7-34"><a href="#cb7-34"></a>  }  </span>
<span id="cb7-35"><a href="#cb7-35"></a>  </span>
<span id="cb7-36"><a href="#cb7-36"></a>  <span class="cf">if</span>(grau <span class="sc">&gt;=</span> 2L)</span>
<span id="cb7-37"><a href="#cb7-37"></a>    dados <span class="ot">&lt;-</span> <span class="fu">iteracoes</span>(dados, <span class="at">grau =</span> grau)</span>
<span id="cb7-38"><a href="#cb7-38"></a>  </span>
<span id="cb7-39"><a href="#cb7-39"></a>  ajuste <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> y <span class="sc">~</span> ., <span class="at">data =</span> dados)</span>
<span id="cb7-40"><a href="#cb7-40"></a>  dados<span class="sc">$</span>y_chapeu <span class="ot">&lt;-</span> <span class="fu">predict</span>(ajuste, <span class="at">new.data =</span> dados)</span>
<span id="cb7-41"><a href="#cb7-41"></a>  </span>
<span id="cb7-42"><a href="#cb7-42"></a>  dados <span class="sc">|&gt;</span> </span>
<span id="cb7-43"><a href="#cb7-43"></a>    dplyr<span class="sc">::</span><span class="fu">relocate</span>(y_chapeu, <span class="at">.before =</span> x)</span>
<span id="cb7-44"><a href="#cb7-44"></a>}</span>
<span id="cb7-45"><a href="#cb7-45"></a></span>
<span id="cb7-46"><a href="#cb7-46"></a>plotando <span class="ot">&lt;-</span> <span class="cf">function</span>(dados){</span>
<span id="cb7-47"><a href="#cb7-47"></a>  dados <span class="sc">|&gt;</span>  </span>
<span id="cb7-48"><a href="#cb7-48"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y_chapeu)) <span class="sc">+</span></span>
<span id="cb7-49"><a href="#cb7-49"></a>    <span class="fu">geom_point</span>()</span>
<span id="cb7-50"><a href="#cb7-50"></a>}</span>
<span id="cb7-51"><a href="#cb7-51"></a></span>
<span id="cb7-52"><a href="#cb7-52"></a>mc_ajustes <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">mc =</span> 100L, <span class="at">n =</span> 50L, <span class="at">desvio_padrao =</span> <span class="dv">5</span>, <span class="at">grau =</span> 1L){</span>
<span id="cb7-53"><a href="#cb7-53"></a></span>
<span id="cb7-54"><a href="#cb7-54"></a>  p <span class="ot">&lt;-</span> </span>
<span id="cb7-55"><a href="#cb7-55"></a>    <span class="fu">ggplot</span>(<span class="at">data =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb7-56"><a href="#cb7-56"></a>      <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">17.5</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">110</span>)) <span class="sc">+</span>      </span>
<span id="cb7-57"><a href="#cb7-57"></a>      <span class="fu">ylab</span>(<span class="st">"Valores estimados"</span>)</span>
<span id="cb7-58"><a href="#cb7-58"></a>  </span>
<span id="cb7-59"><a href="#cb7-59"></a>  df <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb7-60"><a href="#cb7-60"></a>  <span class="cf">for</span>(i <span class="cf">in</span> 1L<span class="sc">:</span>mc){</span>
<span id="cb7-61"><a href="#cb7-61"></a>    df <span class="ot">&lt;-</span> <span class="fu">regressao_polinomial</span>(<span class="at">n =</span> n, <span class="at">desvio_padrao =</span> desvio_padrao, <span class="at">grau =</span> grau)</span>
<span id="cb7-62"><a href="#cb7-62"></a>    p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y_chapeu))</span>
<span id="cb7-63"><a href="#cb7-63"></a>  }</span>
<span id="cb7-64"><a href="#cb7-64"></a>  p <span class="sc">+</span> </span>
<span id="cb7-65"><a href="#cb7-65"></a>    <span class="fu">stat_function</span>(<span class="at">fun =</span> regressao_verdadeira, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">size=</span> <span class="fl">1.4</span>) <span class="sc">+</span></span>
<span id="cb7-66"><a href="#cb7-66"></a>    <span class="fu">labs</span>(</span>
<span id="cb7-67"><a href="#cb7-67"></a>      <span class="at">title =</span> <span class="st">"Regress√£o Polinomial"</span>,</span>
<span id="cb7-68"><a href="#cb7-68"></a>      <span class="at">subtitle =</span> <span class="fu">paste</span>(<span class="st">"Grau: "</span>, grau)</span>
<span id="cb7-69"><a href="#cb7-69"></a>    ) <span class="sc">+</span></span>
<span id="cb7-70"><a href="#cb7-70"></a>    <span class="fu">theme</span>(</span>
<span id="cb7-71"><a href="#cb7-71"></a>      <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>),</span>
<span id="cb7-72"><a href="#cb7-72"></a>      <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb7-73"><a href="#cb7-73"></a>    )</span>
<span id="cb7-74"><a href="#cb7-74"></a>}</span>
<span id="cb7-75"><a href="#cb7-75"></a></span>
<span id="cb7-76"><a href="#cb7-76"></a><span class="co"># Fixando uma semente</span></span>
<span id="cb7-77"><a href="#cb7-77"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb7-78"><a href="#cb7-78"></a></span>
<span id="cb7-79"><a href="#cb7-79"></a>p1 <span class="ot">&lt;-</span> <span class="fu">mc_ajustes</span>(<span class="at">grau =</span> <span class="dv">1</span>, <span class="at">n =</span> <span class="dv">100</span>, <span class="at">desvio_padrao =</span> <span class="dv">10</span>)</span>
<span id="cb7-80"><a href="#cb7-80"></a>p2 <span class="ot">&lt;-</span> <span class="fu">mc_ajustes</span>(<span class="at">grau =</span> <span class="dv">7</span>, <span class="at">n =</span> <span class="dv">100</span>, <span class="at">desvio_padrao =</span> <span class="dv">10</span>)</span>
<span id="cb7-81"><a href="#cb7-81"></a>p3 <span class="ot">&lt;-</span> <span class="fu">mc_ajustes</span>(<span class="at">grau =</span> <span class="dv">70</span>, <span class="at">n =</span> <span class="dv">100</span>, <span class="at">desvio_padrao =</span> <span class="dv">10</span>)</span>
<span id="cb7-82"><a href="#cb7-82"></a>p4 <span class="ot">&lt;-</span> <span class="fu">mc_ajustes</span>(<span class="at">grau =</span> <span class="dv">200</span>, <span class="at">n =</span> <span class="dv">100</span>, <span class="at">desvio_padrao =</span> <span class="dv">10</span>)</span>
<span id="cb7-83"><a href="#cb7-83"></a></span>
<span id="cb7-84"><a href="#cb7-84"></a>p <span class="ot">&lt;-</span> ((p1 <span class="sc">|</span> p2) <span class="sc">/</span> (p3 <span class="sc">|</span> p4)) <span class="sc">+</span> <span class="fu">plot_annotation</span>(<span class="at">tag_levels =</span> <span class="st">"A"</span>)</span>
<span id="cb7-85"><a href="#cb7-85"></a></span>
<span id="cb7-86"><a href="#cb7-86"></a><span class="fu">ggsave</span>(p, <span class="at">file =</span> <span class="st">"imgs/vies_variancia.png"</span>, <span class="at">device =</span> <span class="st">"png"</span>, <span class="at">width =</span> <span class="dv">40</span>, <span class="at">height =</span> <span class="dv">30</span>, <span class="at">units =</span> <span class="st">"cm"</span>)</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>

<img data-src="imgs/vies_variancia.png" style="width:60.0%" class="r-stretch quarto-figure-center"></section>
<section id="balan√ßo-vi√©s-e-vari√¢ncia-4" class="slide level2">
<h2>‚öñÔ∏è Balan√ßo vi√©s e vari√¢ncia</h2>
<p><br></p>
<p>Experimente de forma interativa altera a complefixade do modelo.</p>
<p><br></p>
 <iframe id="example1" src="https://pedro-rafael.shinyapps.io/shiny_apps/" style="border: none; width: 100%; height: 90%" frameborder="0"></iframe>
<p><br></p>
<p>Para ampliar a aplica√ß√£o, clique <a href="https://pedro-rafael.shinyapps.io/shiny_apps/">aqui</a>.</p>
</section>
<section id="tuning-parameters" class="slide level2">
<h2>üéõ Tuning Parameters</h2>
<p><br></p>
<p>No exemplo anterior, note que os par√¢metros dos modelos s√£o os coeficientes <span class="math inline">\beta</span>‚Äôs que indexam a regress√£o polinomial. Por√©m, perceba que √° um <strong>par√¢metro de sintoniza√ß√£o</strong> (<span class="red"><em>tuning parameter</em></span>) que √© o valor de <span class="math inline">p</span>, isto √©, qual o grau do polin√¥mio que iremos utilizar.</p>
<p><br></p>
<p>Normalmente a escolha √© feita realizando um <span class="red"><em>grid search</em></span> por meio de um <span class="red"><em>cross-validation</em></span>.</p>
<p><br></p>
<p>No exemplo anterior, fizemos uma simula√ß√£o e observamos que ao considerar graus nem muito grandes nem muito pequenos, aparentemente teremos escolhas razo√°veis.</p>
<p><br></p>

<img data-src="gifs/interesting-batman.gif" class="r-stretch"></section>
<section id="tuning-parameters-1" class="slide level2">
<h2>üéõ Tuning Parameters</h2>
<p><br></p>
<p><span class="red">Exemplo</span>: Consedere os dados de expectativa de vida versus PIB per Capita, dispon√≠veis <a href="https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData">aqui</a>. Selecione o melhor estimador <span class="math inline">g</span> da classe <span class="math inline">\mathbb{G}</span>, em que</p>
<p><span class="math display">\mathbb{G} = \left\{g(x)\,\,:\,\, \beta_0 + \sum_{i = 1}^p \beta_i x^i\,\, \text{para } p \in \{1, 2, \cdots,11\} \right\}.</span> Note que selecionar o melhor polin√¥mio √© uma busca em <span class="math inline">p</span>. Devemos utilizar o erro quadr√°tico m√©dio - EQM sob o conjunto de valida√ß√£o, uma vez que sabemos que apenas em um conjunto de valida√ß√£o ou em novas observa√ß√µes o estimador do risco pelo EQM √© consistente, pela Lei dos Grandes N√∫meros.</p>
<p><br></p>
<p>Vamos utilizar a biblioteca <a href="https://rsample.tidymodels.org/index.html">rsample</a> para a tarefa de valida√ß√£o cruzada. Leia a documenta√ß√£o da biblioteca, em especial, a da fun√ß√£o <code>vfold_cv</code>, respons√°vel por construir a valida√ß√£o cruzada. Na verdade ela faz a divis√£o da base de dados em <span class="math inline">v</span> <em>splits</em> de tamanho aproximadamente iguais. Por padr√£o, <span class="math inline">v = 10</span>. Esse √© o procedimento de <span class="math inline">k</span>-<em>fold cross-validation</em> que apresentamos aqui, em que <span class="math inline">v = k</span>.</p>
</section>
<section id="tuning-parameters-2" class="slide level2">
<h2>üéõ Tuning Parameters</h2>
<p><br></p>
<p>Algumas observa√ß√µes gerais a respetio da biblioteca <a href="https://rsample.tidymodels.org/index.html">rsample</a>, que s√£o √∫teis para resolver esse problema:</p>

<img data-src="imgs/rsample.png" class="r-stretch quarto-figure-center"><ol type="1">
<li class="fragment">Para realizar uma <strong>primeira divis√£o</strong> do conjunto de dados (<em>data splitting/hold-out</em>), utiliza-se a fun√ß√£o <code>initial_split</code>;</li>
<li class="fragment">Para acessar o conjunto de <strong>treinamento</strong> dos dados, usamos a fun√ß√£o <code>training</code>;</li>
<li class="fragment">Para acessar o conjunto de <strong>teste</strong>, usamos a fun√ß√£o <code>testing</code>;</li>
<li class="fragment">Para constuir todas as divis√µes da valida√ß√£o cruzada, entre treinamento e valida√ß√£o, no conjunto de treinamento inicial, usamos a fun√ß√£o <code>vfold_cv</code> j√° mencionada;</li>
<li class="fragment">Para acessar o conjunto de <strong>treinamento</strong> de um <em>split</em> da valida√ß√£o cruzada, usamos a fun√ß√£o <code>analysis</code>;</li>
<li class="fragment">Para acessar o conjunto de <strong>valida√ß√£o</strong>, utilizamos a fun√ß√£o <code>assessment</code>.</li>
</ol>
</section>
<section id="tuning-parameters-3" class="slide level2">
<h2>üéõ Tuning Parameters</h2>

<img data-src="imgs/rsample.png" style="width:5.0%" class="r-stretch quarto-figure-center"><p><br> Note que realizar uma valida√ß√£o cruzada √© importante para podemos selecionar o melhor polin√¥mio, i.e., o melhor valor de <span class="math inline">p</span>. Caso venhamos negligenciar esse aspecto da an√°lise, iremos cair na fal√°cia de acreditarmos que quanto maior o grau do polin√¥mio, maior ser√° o poder preditivo do modelo. Isso n√£o √© verdade e voc√™ dever√° selecionar o melhor modelo dentro de um esquema de valida√ß√£o cruzada.</p>
<p><br></p>
<p>No mundo de aprendizagem de m√°quina, muitos chamam o processo de encontrar o melhor hiperpar√¢metro de <span class="red">‚Äútunagem‚Äù</span>. Em v√°rios modelos, podemos ter mais de um.</p>
</section>
<section id="tuning-parameters-4" class="slide level2">
<h2>üéõ Tuning Parameters</h2>
<p><br></p>
<p>As Figuras abaixo, mostram a avalia√ß√£o dos polin√¥mios da classe <span class="math inline">\mathbb{G}</span>, usando o risco estimado <span class="math inline">\widehat{R}(g)</span> pelo erro quadr√°tico m√©dio - EQM. Por√©m, a Figura <span class="red">A</span> aprenseta a avalia√ß√£o dos modelos, usando simplesmente o conjunto de treinamento e a Figura <span class="red">B</span> aprensenta a avalia√ß√£o do grau do polin√¥mio considerando uma valida√ß√£o cruzada dentro do conjunto de treinamento.</p>
<p><br> <img data-src="imgs/avaliacao_risco.png" data-fig-align="center"></p>
<p><br></p>
<p>A mensagem equivocada passada pela Figura <span class="red">A</span> √© que supostamente aumentar a complexidade do modelo seria uma uma boa alternativa e nos conduzir√≠amos √† bons modelos preditivos. Mas sempre se lembre do equil√≠brio que temos que ter entre vi√©s e vari√¢ncia. A Figura <span class="red">B</span> mostra que um polin√¥mio com grau pr√≥ximo √† <span class="math inline">p = 8</span> √© a melhor alternativa.</p>
</section>
<section id="tuning-parameters-5" class="slide level2">
<h2>üéõ Tuning Parameters</h2>
<p><br></p>
<p>Observe o <em>dashboard</em> interativo! Sabemos que para que tenhamos uma boa estimativa do risco preditivo, devemos utilizar novas observa√ß√µes. No <em>dashboard</em>, √© poss√≠vel observar que a forma errada (usando o conjunto de treinamento para avaliar o risco), sugere que sempre ser√° bom adicionar mais par√¢metros ao modelo, levando a <em>overfitting</em>. Perceba que usando a forma correta (usando valida√ß√£o cruzada), o EQM (risco estimado) sugere que n√£o podemos aumentar muito a quantidade de par√¢metros.</p>
<p><br></p>
 <iframe id="example1" src="https://pedro-rafael.shinyapps.io/shiny_apps/#section-avalia%C3%A7%C3%A3o-do-risco-preditivo" style="border: none; width: 100%; height: 90%" frameborder="0"></iframe>
</section>
<section id="tuning-parameters-6" class="slide level2">
<h2>üéõ Tuning Parameters</h2>
<p><br></p>
<p>Abaixo voc√™ poder√° acessar o c√≥digo que soluciona o problema. O par√¢metro <code>errado = FALSE</code> da fun√ß√£o valida√ß√£o no c√≥digo que segue, conduz a solu√ß√£o correta (usando a valida√ß√£o cruzada), que sempre voc√™ dever√° considerar na pr√°tica.</p>
<p><br></p>
<div class="cell">
<details>
<summary>Estude o c√≥digo da solu√ß√£o de exemplo</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="fu">library</span>(rsample)</span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="fu">library</span>(yardstick)</span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="fu">library</span>(tibble)</span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="fu">library</span>(purrr)</span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb8-7"><a href="#cb8-7"></a></span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="co"># Lendo dados</span></span>
<span id="cb8-9"><a href="#cb8-9"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb8-10"><a href="#cb8-10"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb8-12"><a href="#cb8-12"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb8-13"><a href="#cb8-13"></a></span>
<span id="cb8-14"><a href="#cb8-14"></a>dados <span class="ot">&lt;-</span> </span>
<span id="cb8-15"><a href="#cb8-15"></a>  dados_expectativa_renda <span class="sc">|&gt;</span> </span>
<span id="cb8-16"><a href="#cb8-16"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>CountryName) <span class="sc">|&gt;</span> </span>
<span id="cb8-17"><a href="#cb8-17"></a>  dplyr<span class="sc">::</span><span class="fu">rename</span>(<span class="at">y =</span> LifeExpectancy, <span class="at">x =</span> GDPercapita)</span>
<span id="cb8-18"><a href="#cb8-18"></a>  </span>
<span id="cb8-19"><a href="#cb8-19"></a>iteracoes <span class="ot">&lt;-</span> <span class="cf">function</span>(tibble_data, grau) {</span>
<span id="cb8-20"><a href="#cb8-20"></a>  x <span class="ot">&lt;-</span> tibble_data<span class="sc">$</span>x</span>
<span id="cb8-21"><a href="#cb8-21"></a>  iteracoes <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="at">X =</span> 2L<span class="sc">:</span>grau, <span class="at">FUN =</span> <span class="cf">function</span>(i) x<span class="sc">^</span>i)</span>
<span id="cb8-22"><a href="#cb8-22"></a>  </span>
<span id="cb8-23"><a href="#cb8-23"></a>  result <span class="ot">&lt;-</span> <span class="fu">cbind</span>(tibble_data, <span class="fu">do.call</span>(cbind, iteracoes))</span>
<span id="cb8-24"><a href="#cb8-24"></a>  <span class="fu">colnames</span>(result)[(<span class="fu">ncol</span>(tibble_data) <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span><span class="fu">ncol</span>(result)] <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">"x"</span>, 2L<span class="sc">:</span>grau)</span>
<span id="cb8-25"><a href="#cb8-25"></a>  </span>
<span id="cb8-26"><a href="#cb8-26"></a>  <span class="fu">as_tibble</span>(result)</span>
<span id="cb8-27"><a href="#cb8-27"></a>}  </span>
<span id="cb8-28"><a href="#cb8-28"></a></span>
<span id="cb8-29"><a href="#cb8-29"></a>regressao_polinomial <span class="ot">&lt;-</span> <span class="cf">function</span>(dados, <span class="at">grau =</span> 1L) {</span>
<span id="cb8-30"><a href="#cb8-30"></a>  <span class="cf">if</span>(grau <span class="sc">&gt;=</span> 2L)</span>
<span id="cb8-31"><a href="#cb8-31"></a>    dados <span class="ot">&lt;-</span> <span class="fu">iteracoes</span>(dados, <span class="at">grau =</span> grau)</span>
<span id="cb8-32"><a href="#cb8-32"></a>  </span>
<span id="cb8-33"><a href="#cb8-33"></a>  <span class="fu">lm</span>(<span class="at">formula =</span> y <span class="sc">~</span> ., <span class="at">data =</span> dados)</span>
<span id="cb8-34"><a href="#cb8-34"></a>}</span>
<span id="cb8-35"><a href="#cb8-35"></a></span>
<span id="cb8-36"><a href="#cb8-36"></a><span class="co"># Divis√£o dos dados</span></span>
<span id="cb8-37"><a href="#cb8-37"></a>divisao_inicial <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(dados)</span>
<span id="cb8-38"><a href="#cb8-38"></a>treinamento <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(divisao_inicial)</span>
<span id="cb8-39"><a href="#cb8-39"></a>teste <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">testing</span>(divisao_inicial) <span class="co"># Teste final</span></span>
<span id="cb8-40"><a href="#cb8-40"></a></span>
<span id="cb8-41"><a href="#cb8-41"></a><span class="co"># v-folds cross-validation</span></span>
<span id="cb8-42"><a href="#cb8-42"></a>validacao <span class="ot">&lt;-</span> <span class="cf">function</span>(dados, <span class="at">grau =</span> 1L, <span class="at">errado =</span> <span class="cn">FALSE</span>, ...){</span>
<span id="cb8-43"><a href="#cb8-43"></a>  </span>
<span id="cb8-44"><a href="#cb8-44"></a>  <span class="co"># Todas as divis√µes da validacao cruzada</span></span>
<span id="cb8-45"><a href="#cb8-45"></a>  cv <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">vfold_cv</span>(dados, ...)</span>
<span id="cb8-46"><a href="#cb8-46"></a>  </span>
<span id="cb8-47"><a href="#cb8-47"></a>  hiper <span class="ot">&lt;-</span> <span class="cf">function</span>(i){</span>
<span id="cb8-48"><a href="#cb8-48"></a>    treino <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">analysis</span>(cv<span class="sc">$</span>splits[[i]]) <span class="co"># Treinamento</span></span>
<span id="cb8-49"><a href="#cb8-49"></a>    validacao <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">assessment</span>(cv<span class="sc">$</span>splits[[i]]) <span class="co"># Validaca√ß√£o</span></span>
<span id="cb8-50"><a href="#cb8-50"></a>    ajuste <span class="ot">&lt;-</span> <span class="fu">regressao_polinomial</span>(<span class="at">dados =</span> treino, <span class="at">grau =</span> grau)</span>
<span id="cb8-51"><a href="#cb8-51"></a>    </span>
<span id="cb8-52"><a href="#cb8-52"></a>    <span class="cf">if</span>(errado){</span>
<span id="cb8-53"><a href="#cb8-53"></a>      df_treino <span class="ot">&lt;-</span> <span class="fu">iteracoes</span>(treino, <span class="at">grau =</span> grau)</span>
<span id="cb8-54"><a href="#cb8-54"></a>      df_treino <span class="ot">&lt;-</span> df_treino <span class="sc">|&gt;</span> dplyr<span class="sc">::</span><span class="fu">mutate</span>(<span class="at">y_chapeu =</span> <span class="fu">predict</span>(ajuste, <span class="at">newdata =</span> df_treino))</span>
<span id="cb8-55"><a href="#cb8-55"></a>      yardstick<span class="sc">::</span><span class="fu">rmse</span>(<span class="at">data =</span> df_treino, <span class="at">truth =</span> y, <span class="at">estimate =</span> y_chapeu)<span class="sc">$</span>.estimate</span>
<span id="cb8-56"><a href="#cb8-56"></a>    } <span class="cf">else</span> {</span>
<span id="cb8-57"><a href="#cb8-57"></a>      df_validacao <span class="ot">&lt;-</span> <span class="fu">iteracoes</span>(validacao, <span class="at">grau =</span> grau)</span>
<span id="cb8-58"><a href="#cb8-58"></a>      df_validacao <span class="ot">&lt;-</span> df_validacao <span class="sc">|&gt;</span> dplyr<span class="sc">::</span><span class="fu">mutate</span>(<span class="at">y_chapeu =</span> <span class="fu">predict</span>(ajuste, <span class="at">newdata =</span> df_validacao))</span>
<span id="cb8-59"><a href="#cb8-59"></a>      yardstick<span class="sc">::</span><span class="fu">rmse</span>(<span class="at">data =</span> df_validacao, <span class="at">truth =</span> y, <span class="at">estimate =</span> y_chapeu)<span class="sc">$</span>.estimate</span>
<span id="cb8-60"><a href="#cb8-60"></a>    }</span>
<span id="cb8-61"><a href="#cb8-61"></a>  }</span>
<span id="cb8-62"><a href="#cb8-62"></a>  purrr<span class="sc">::</span><span class="fu">map_dbl</span>(<span class="at">.x =</span> <span class="fu">seq_along</span>(cv<span class="sc">$</span>splits), <span class="at">.f =</span> hiper) <span class="sc">|&gt;</span> </span>
<span id="cb8-63"><a href="#cb8-63"></a>    <span class="fu">mean</span>()</span>
<span id="cb8-64"><a href="#cb8-64"></a>}</span>
<span id="cb8-65"><a href="#cb8-65"></a></span>
<span id="cb8-66"><a href="#cb8-66"></a>plot_avaliacao <span class="ot">&lt;-</span> <span class="cf">function</span>(dados, <span class="at">errado =</span> <span class="cn">FALSE</span>){</span>
<span id="cb8-67"><a href="#cb8-67"></a>  <span class="co"># Testando iterativamente, v√°rios valores de p:</span></span>
<span id="cb8-68"><a href="#cb8-68"></a>  p <span class="ot">&lt;-</span> <span class="fu">seq</span>(1L<span class="sc">:</span>11L)</span>
<span id="cb8-69"><a href="#cb8-69"></a>  risco <span class="ot">&lt;-</span> purrr<span class="sc">::</span><span class="fu">map_dbl</span>(<span class="at">.x =</span> p, <span class="at">.f =</span> \(p) <span class="fu">validacao</span>(<span class="at">dados =</span> dados, <span class="at">grau =</span> p, <span class="at">errado =</span> errado))</span>
<span id="cb8-70"><a href="#cb8-70"></a>  df_risco <span class="ot">&lt;-</span> tibble<span class="sc">::</span><span class="fu">tibble</span>(<span class="at">p =</span> p, <span class="at">risco =</span> risco)</span>
<span id="cb8-71"><a href="#cb8-71"></a>  </span>
<span id="cb8-72"><a href="#cb8-72"></a>  <span class="co"># Plotando</span></span>
<span id="cb8-73"><a href="#cb8-73"></a>  df_risco <span class="sc">|&gt;</span> </span>
<span id="cb8-74"><a href="#cb8-74"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> p, <span class="at">y =</span> risco, <span class="at">color =</span> risco)) <span class="sc">+</span></span>
<span id="cb8-75"><a href="#cb8-75"></a>    <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb8-76"><a href="#cb8-76"></a>    <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> p) <span class="sc">+</span></span>
<span id="cb8-77"><a href="#cb8-77"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">breaks =</span> p) <span class="sc">+</span></span>
<span id="cb8-78"><a href="#cb8-78"></a>    <span class="fu">labs</span>(</span>
<span id="cb8-79"><a href="#cb8-79"></a>      <span class="at">title =</span> <span class="st">"Valiando o risco estimado para diversos graus do polin√¥mio"</span>,</span>
<span id="cb8-80"><a href="#cb8-80"></a>      <span class="at">subtitle =</span> <span class="st">"EQM no conjunto de valida√ß√£o"</span></span>
<span id="cb8-81"><a href="#cb8-81"></a>    ) <span class="sc">+</span></span>
<span id="cb8-82"><a href="#cb8-82"></a>    <span class="fu">theme</span>(</span>
<span id="cb8-83"><a href="#cb8-83"></a>      <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">18</span>, <span class="at">face =</span> <span class="st">"bold"</span>),</span>
<span id="cb8-84"><a href="#cb8-84"></a>      <span class="at">plot.subtitle =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">16</span>),</span>
<span id="cb8-85"><a href="#cb8-85"></a>      <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>), </span>
<span id="cb8-86"><a href="#cb8-86"></a>      <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">14</span>, <span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb8-87"><a href="#cb8-87"></a>    )</span>
<span id="cb8-88"><a href="#cb8-88"></a>}</span>
<span id="cb8-89"><a href="#cb8-89"></a></span>
<span id="cb8-90"><a href="#cb8-90"></a><span class="co"># Avaliac√£o errada versus correta</span></span>
<span id="cb8-91"><a href="#cb8-91"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb8-92"><a href="#cb8-92"></a>grafico <span class="ot">&lt;-</span> </span>
<span id="cb8-93"><a href="#cb8-93"></a>  <span class="fu">plot_avaliacao</span>(dados, <span class="at">errado =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> </span>
<span id="cb8-94"><a href="#cb8-94"></a>  <span class="fu">plot_avaliacao</span>(dados, <span class="at">errado =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb8-95"><a href="#cb8-95"></a>  <span class="fu">plot_annotation</span>(<span class="at">tag_levels =</span> <span class="fu">c</span>(<span class="st">"A"</span>, <span class="st">"B"</span>))</span>
<span id="cb8-96"><a href="#cb8-96"></a></span>
<span id="cb8-97"><a href="#cb8-97"></a><span class="fu">ggsave</span>(grafico, <span class="at">file =</span> <span class="st">"imgs/avaliacao_risco.png"</span>, <span class="at">device =</span> <span class="st">"png"</span>, <span class="at">width =</span> <span class="dv">50</span>, <span class="at">height =</span> <span class="dv">20</span>, <span class="at">units =</span> <span class="st">"cm"</span>, <span class="at">limitsize =</span> F)</span>
<span id="cb8-98"><a href="#cb8-98"></a></span>
<span id="cb8-99"><a href="#cb8-99"></a>plot_bar <span class="ot">&lt;-</span> <span class="cf">function</span>(grau){</span>
<span id="cb8-100"><a href="#cb8-100"></a>  ruim <span class="ot">&lt;-</span> <span class="fu">validacao</span>(dados, <span class="at">errado =</span> <span class="cn">TRUE</span>, <span class="at">grau =</span> grau)</span>
<span id="cb8-101"><a href="#cb8-101"></a>  bom <span class="ot">&lt;-</span> <span class="fu">validacao</span>(dados, <span class="at">errado =</span> <span class="cn">FALSE</span>, <span class="at">grau =</span> grau)</span>
<span id="cb8-102"><a href="#cb8-102"></a>  df <span class="ot">&lt;-</span> tibble<span class="sc">::</span><span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="st">"Errado"</span>, <span class="st">"Certo"</span>), <span class="at">y =</span> <span class="fu">c</span>(<span class="fu">log</span>(ruim), <span class="fu">log</span>(bom)))</span>
<span id="cb8-103"><a href="#cb8-103"></a>  </span>
<span id="cb8-104"><a href="#cb8-104"></a>  df <span class="sc">|&gt;</span> </span>
<span id="cb8-105"><a href="#cb8-105"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb8-106"><a href="#cb8-106"></a>    <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>) <span class="sc">+</span></span>
<span id="cb8-107"><a href="#cb8-107"></a>    <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">label =</span> y), <span class="at">vjust =</span> <span class="dv">0</span>)</span>
<span id="cb8-108"><a href="#cb8-108"></a>}</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>

<img data-src="gifs/mr-bean-pivot.gif" class="r-stretch"></section>
<section id="exerc√≠cios" class="slide level2">
<h2>üìö Exerc√≠cios</h2>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Explique resumidamente o que √© aprendizagem supervisionada e n√£o-supervisionada. Cite um problema de aprendizagem supervisionada e um outro de aprendizagem n√£o-supervisionada.</p>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Considere o conjunto de dados de Expectativa de vida versus PIB per Capita, dispon√≠vel <a href="https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData">aqui</a>. Considere a fun√ß√£o <span class="math inline">g</span>, da seguinte forma:</p>
<p><span class="math display">g(x) = \beta_0 + \sum_{i = 1}^p \beta_i x^i,</span> com <span class="math inline">p \in \{1, 2, ..., 50\}</span>. Utilizando o erro quadr√°tico m√©dio observado, sem fazer nenhuma estrat√©gia de divis√£o dos dados, implemente um c√≥digo em R para checar qual o melhor modelo.</p>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Explique qual o motivo que faz com que o Erro Quadr√°tico M√©dio - EQM para avaliar o desempenho de um modelo √© ruim quando n√£o adotamos nenhuma estrat√©gia de divis√£o do conjunto de dados em treinamento e teste.</p>
</section>
<section id="exerc√≠cios-1" class="slide level2">
<h2>üìö Exerc√≠cios</h2>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Com suas palavras, explique o dilema de balan√ßo entre v√≠es e vari√¢ncia.</p>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Refa√ßa o exerc√≠cio do polin√¥mio, utilizando a estrat√©gia de <span class="red"><em>data splitting</em></span>, em que divide-se o conjunto de dados em treinamento e teste. Utilize o conjunto de teste para calcular a estimativa do risco, usando o EQM.</p>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Ainda considerando o exerc√≠cio do polin√¥mio, implemente uma estrat√©gia de <span class="red"><em>leave-one-out cross-validation</em></span> e selecione o melhor modelo minimizando a fun√ß√£o de risco.</p>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Por fim, considerando o exerc√≠cio do polin√¥mio, rafa√ßa-o utilizando um procedimento de <span class="red"><span class="math inline">k</span>-fold cross-validation</span>. Considere <span class="math inline">k = 5</span>. <strong>Dica</strong>: considere utiliza a biblioteca <a href="https://rsample.tidymodels.org/">rsample</a>.</p>

<img data-src="imgs/rsample.png" class="r-stretch"></section>
<section id="melhor-subconjunto-de-covari√°veis" class="slide level2">
<h2>Melhor subconjunto de covari√°veis</h2>
<p><br></p>
<p>O estimador de m√≠nimos quadrados - EMQ, na presen√ßa de muitas <em>features</em> (covari√°veis), i.e., quando temos <span class="math inline">d</span> grande, possui um baixo poder preditivo devido <em>overfitting</em> (super-ajuste). Isso, porqu√™ haver√° muitos par√¢metros a serem estimados, e portanto, a fun√ß√£o de regress√£o estimada <span class="math inline">\widehat{r}({\bf x})</span> ter√° baixo poder preditivo.</p>
<p><br></p>
<p>Isso se deve ao fato do balan√ßo de vi√©s e vari√¢ncia. Havendo muitos par√¢metros, como j√° tinhamos visto, a vari√¢ncia do modelo ser√° muito alta.</p>
<p><br></p>
<p><strong>Portanto, deveremos buscar meios de encontrar o melhor (ao menos um bom) comjunto de covari√°veis.</strong></p>
<p><br></p>

<img data-src="gifs/chapulin-colorado-no.gif" style="width:20.0%" class="r-stretch"></section>
<section id="melhor-subconjunto-de-covari√°veis-1" class="slide level2">
<h2>Melhor subconjunto de covari√°veis</h2>
<p><br></p>
<p>A ideia para resolver esse problema √© retirar algumas covari√°veis do modelo de regress√£o, com o objetivo de diminuir a vari√¢ncia de <span class="math inline">\widehat{g}</span>.</p>
<p><br></p>
<p>Voc√™ poder√° entender que estamos em busca de um estimador <span class="math inline">\widehat{g}</span> de <span class="math inline">g</span> um <strong>pouco</strong> mais viesado. Trata-se de uma troca em que desejamos reduzir substancialmente a variabilidade do estiamdor do modelo em troca de um pouco mais de vi√©s.</p>
<p><br></p>

<img data-src="gifs/kiko.gif" class="r-stretch"></section>
<section id="melhor-subconjunto-de-covari√°veis-2" class="slide level2">
<h2>Melhor subconjunto de covari√°veis</h2>
<p><br></p>
<p>Matematicamente, uma maneira de fazer isso, √© buscar a estimativa para</p>
<p><span id="eq-risco-penalizacao"><span class="math display">\widehat{\beta}_{L_0} = \argmin_{\beta_0 \in \mathbb{R}, \beta \in \mathbb{R}^d}\sum_{k = 1}^n\overbrace{\left(y_k - \beta_0 - \sum_{i = 1}^d \beta_i x_{k,i}\right)^2}^{n \times EQM} + \lambda \,\,\underbrace{\sum_{i = 1}^d \mathbb{I}(\beta_i \neq 0)}_{\text{Penaliza√ß√£o}}. \tag{3}</span></span></p>
<p>Note que a penaliza√ß√£o <span class="math inline">\sum_{i = 1}^d \mathbb{I}(\beta_i \neq 0)</span> nos conduz na dire√ß√£o de modelos com poucas covari√°veis, quando <span class="math inline">\lambda</span> √© um valor alto. Em particualr, quando <span class="math inline">\lambda \to \infty</span>, for√ßamos a retirada de todas as covari√°veis <span class="math inline">\beta_i</span>‚Äôs, i.e., a solu√ß√£o para o problema seria <span class="math inline">\widehat{\beta}_{L_0} \equiv (\overline{y}, {\bf 0})</span>. Note que n√£o h√° penaliza√ß√£o para o intercepto <span class="math inline">\beta_0</span>.</p>
<p><br> No outro extremo, para <span class="math inline">\lambda = 0</span>, temos o estimador de m√≠nimos quadrados, em que nenhuma penaliza√ß√£o ser√° considerada.</p>
<p><br> <span class="red">N√£o h√° uma forma f√°cil de minimizar <span class="math inline">\widehat{\beta}_{L_0}</span>.</span></p>
</section>
<section id="melhor-subconjunto-de-covari√°veis-3" class="slide level2">
<h2>Melhor subconjunto de covari√°veis</h2>
<p><br></p>
<p>Poder√≠amos, ingenuamente, pensar em ajustar todas as combina√ß√µes poss√≠veis de par√¢metros e utilizar algum crit√©rio, por exemplo, o EQM em novas observa√ß√µes para escolher o melhor modelo de todas as combina√ß√µes poss√≠veis. Isto √©, escolher o melhor modelo entre todas as <span class="math inline">2^d</span> combina√ß√µes poss√≠veis de modelos em <span class="math inline">\mathbb{G}</span>.</p>
<p><br></p>
<p>Se <span class="math inline">\widehat{\lambda} = \frac{2}{n}\widehat{\sigma}^2</span>, estimar <span class="math inline">\widehat{\beta}_{L_0}</span> equivale uma busca entre <span class="math inline">2^d</span> modelos da classe <span class="math inline">\mathbb{G}</span>:</p>
<p><br></p>
<span class="math display">\begin{align*}
\mathbb{G} = \{
&amp;g({\bf x}) = \widehat{\beta}_0, \\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_1x_1,\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_2x_2,\\
&amp;\cdots\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_dx_d,\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_2x_2,\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_3x_3,\\
&amp;\cdots\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_dx_d,\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_2x_2 + \widehat{\beta}_3x_3,\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_2x_2 + \widehat{\beta}_dx_d,\\
&amp;\cdots\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_2x_2 + \widehat{\beta}_3x_3 + \cdots + &amp;\widehat{\beta}_dx_d
\}.
\end{align*}</span>
</section>
<section id="melhor-subconjunto-de-covari√°veis-4" class="slide level2">
<h2>Melhor subconjunto de covari√°veis</h2>
<p><br></p>
<p>Utilizar <span class="math inline">\lambda = \frac{2}{n}\widehat{\sigma}^2</span> √© o mesmo que utilizar o crit√©rio AIC para determinar o melhor modelo, em que dado</p>
<p><span class="math display">\widehat{R}(g) = \frac{1}{m}\sum_{k = 1}^m \underbrace{(\widetilde{Y}_k - g({\bf \widetilde{X}}_k))^2}_{W_k},</span> em que <span class="math inline">(\widetilde{{\bf X}}_1, \widetilde{Y}_1), \cdots, (\widetilde{{\bf X}}_m, \widetilde{Y}_m)</span>, representa o conjunto de teste, i.e., calculado com base em <span class="math inline">m</span> observa√ß√µes em um conjundo de dados n√£o utilizados para treinar o modelo, independentemente da estrat√©gia de divis√£o utilizada, em que</p>
<p><span class="math display">\widehat{\sigma}^2 = \frac{1}{m}\sum_{k = 1}^m (W_k - \overline{W})^2,</span> com <span class="math inline">\overline{W} = \frac{1}{m}\sum_{k=1}^m W_k</span>.</p>
</section>
<section id="melhor-subconjunto-de-covari√°veis-5" class="slide level2">
<h2>Melhor subconjunto de covari√°veis</h2>
<p><br></p>
<p>Temos que <span class="math inline">\widehat{R}(g)</span>, calculado em novas observa√ß√µes (oboserva√ß√µes n√£o utilizadas no treinamento), pode se valer do Teorema Central do Limite, uma vez que <span class="math inline">\widehat{R}(g)</span> √© calculado em uma sequ√™ncia de vari√°veis aleat√≥rias i.i.d.‚Äôs. Ent√£o:</p>
<p><span class="math display">\widehat{R}(g) \sim \text{Normal}\left(R(g), \frac{1}{m}\mathbb{V}[W_1]\right).</span></p>
<p>Portanto, um intervalo aleat√≥rio de aproximadamente <span class="math inline">95\%</span> de confian√ßa para o erro preditivo <span class="math inline">R(g)</span> poder√° ser calculado como:</p>
<p><span class="math display">\widehat{R}(g) \pm 1,645 \sqrt{\frac{1}{m}\widehat{\sigma}^2}.</span></p>
<p>O c√°lculo de um intervalo de confian√ßa poder√° ser √∫til para entendermos como est√° variando o risco preditivo do nosso modelo. Gostamos de ter modelos com intervalo de amplitude pequena. O intervalo poder√° ser utilizado para fornecer insight de como escolher a divis√£o de treinamento e teste. Por exemplo, pode-se escolher o menor valor de <span class="math inline">m</span> de modo que a amplitude seja a menor poss√≠vel.</p>
</section>
<section id="melhor-subconjunto-de-covari√°veis-6" class="slide level2">
<h2>Melhor subconjunto de covari√°veis</h2>
<p><br></p>
<p>Por que essa seria uma escolha ing√™nua? Pense na situa√ß√£o em que temos <span class="math inline">d = 30</span>, i.e., trinta covari√°veis. Ter√≠amos portanto <span class="math inline">2^{30}</span> modelos para ajustar, ou seja, um bilh√£o e setenta e tr√™s milh√µes, setecentos e quarenta e um mil, oitocentos e vinte e quatro modelos para ajustar. √â um a quantidade absurda de modelos para serem estimados!</p>
<p><br></p>
<p>Se <span class="math inline">d = 100</span>, ter√≠amos que estimar uma quantidade de modelos que a quantidade estimada de estrelas no universo. Alias, seriam mais modelos para ajustar que a quantidade de √°tomos no universo.</p>
<p><br></p>

<img data-src="gifs/side-eyeing-chloe-chloe.gif" class="r-stretch"></section>
<section id="regress√£o-stepwise" class="slide level2">
<h2>Regress√£o <em>Stepwise</em></h2>
<p><br> Devido a impossibilidade de experimentar uma grande quantidade de modelos (<span class="math inline">2^d</span>), existe uma s√©rie de algoritmos (heur√≠sticas), que visam reduzir a quantidade de modelos avaliados. Um dos mais conhecidos √© o <span class="red">forward stepwise</span>. Trata-se de um algoritmo sequencial, que em cada passo, apenas uma vari√°vel √© adicionada:</p>
<p><br></p>
<p>1 - Para <span class="math inline">j = 1, \cdots, d</span>, ajuste a regress√£o de <span class="math inline">Y</span> na <span class="math inline">j</span>-√©sima vari√°vel <span class="math inline">X_j</span>. Seja <span class="math inline">\widehat{R}(g_j)</span> o risco estimado desta fun√ß√£o. Ent√£o,</p>
<p><span class="math display">\widehat{j} = \argmin_j \widehat{R}(g_j)\,\,\,\,\,\, \text{e}\,\,\,\,\,\, S = \{\widehat{j}\}.</span> 2 - Para cada <span class="math inline">j \in S^c</span>, ajuste a regress√£o <span class="math inline">Y = \beta_jX_j + \sum_{s \in S}\beta_sX_S</span>, em que <span class="math inline">\widehat{R}(g_j)</span> √© o risco estimado desta fun√ß√£o. Defina</p>
<p><span class="math display">\widehat{j} = \argmin_{j \in S^c} \widehat{R}(g_j)\,\,\,\,\,\, \text{e atualize}\,\,\,\,\,\, S \leftarrow \{S \cup \widehat{j}\}.</span></p>
<p>3 - Repita os passos anteriores at√© que todas as vari√°veis estejam em <span class="math inline">S</span> ou at√© quando n√£o seja mais poss√≠vel ajustar o modelo de regress√£o.</p>
<p>4 - Selecione o modelo com menor risco estimado.</p>
</section>
<section id="regress√£o-stepwise-1" class="slide level2">
<h2>Regress√£o <em>Stepwise</em></h2>
<p><br></p>
<p>Utilizar o algoritmo de sele√ß√£o de vari√°veis <span class="red"><em>foward stepwise</em></span>, ao inv√©s de buscarmos o melhor ajuste entre <span class="math inline">2^d</span> poss√≠veis modelos, que muitas vezes √© imposs√≠vel, precisaremos investigar apenas <span class="math inline">1 + d(d + 1)/2</span> modelos. Reduzimos a complexidade da sele√ß√£o que antes era um problema exponencial. Melhor, n√£o?!</p>
<p><br></p>

<img data-src="gifs/bom.gif" class="r-stretch"></section>
<section id="penaliza√ß√£o" class="slide level2">
<h2>Penaliza√ß√£o</h2>
<p><br></p>
<p>Quando temos modelos que envolve <span class="math inline">d</span> par√¢metros e que temos controle sobre eles (conhecemos muito bem cada um deles), acrescentar algum tipo de penaliza√ß√£o √† fun√ß√£o objetivo poder√° ser √∫til. A penaliza√ß√£o √© uma <span class="red">medida de complexidade</span>, em que √© √∫til para equilibrar o modelo, de modo a tentar buscar um equilibrio entre vi√©s e vari√¢ncia, discutidos anteriormente. Assim, sob novas observa√ß√µes, desejamos estimar o risco <span class="math inline">R(g)</span>, por</p>
<p><br></p>
<p><span class="math display">R(g) \approx EQM(g) + \mathcal{P}(g).</span></p>
<p><br></p>
<p>Desejamos minimiar <span class="math inline">R(g)</span>, mas n√£o a custa de muitos par√¢metros, pois assim ter√≠amos <span class="red">overfitting</span>. Portanto, para muitos par√¢metros temos que ter EQM baixo, por√©m, <span class="math inline">\mathcal{P}(g)</span> deve ser alto. J√° em modelos viesados, quando temos poucos par√¢metros, o EQM normalmente √© alto, mas a complexidade <span class="math inline">\mathcal{P}(g)</span> deve ser baixo, pois temos um modelo mais simplista.</p>
</section>
<section id="aic-e-bic" class="slide level2">
<h2>AIC e BIC</h2>
<p><br></p>
<p>Existem diversas penaliza√ß√µes, em que o AIC (<strong>A</strong>kaike <strong>I</strong>nformation <strong>C</strong>riterion) e BIC (<strong>B</strong>ayesian <strong>I</strong>nformation <strong>C</strong>riterion) s√£o as mais conhecidas. Com base nesses crit√©rios, temos que</p>
<p><br></p>
<div class="{columns}">
<div class="column" style="width:30%;">
<ol type="1">
<li class="fragment"><span class="red">AIC</span>: <span class="math display">EQM + \frac{2}{n\,d\, \widehat{\sigma}^2}.</span></li>
<li class="fragment"><span class="red">BIC</span>: <span class="math display">EQM + \frac{\log(n)}{n\, d\, \widehat{\sigma}^2}.</span></li>
</ol>
</div>
<div class="column" style="width:70%;">
<p><span class="math display">\widehat{\sigma}^2 = \frac{1}{m}\sum_{k = 1}^m (W_k - \overline{W})^2.</span></p>
</div>
</div>
<p><br></p>
<p>Aqui, <span class="math inline">d</span> √© a quantidade de par√¢metros no modelo e <span class="math inline">\widehat{\sigma}^2</span> √© uma estimativa da vari√¢ncia do erro, que para um conjunto de teste suficientemente grande, poder√° ser considerado o estimador de <span class="math inline">\widehat{\sigma}^2</span> conforme descrito anteriormente. Segundo <a href="https://hastie.su.domains/ISLR2/ISLRv2_corrected_June_2023.pdf">James, Gareth, et al.&nbsp;An introduction to statistical learning. Ed. 2, p. 233</a>, assume-se o modelo com todos os preditores para o c√°lculo de <span class="math inline">\widehat{\sigma}^2</span>.</p>
</section>
<section id="lasso" class="slide level2">
<h2>Lasso</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p>O lasso foi desenvolvido pelo Robert Tibshirani, em um artigo publicado no artigo <a href="https://www.jstor.org/stable/2346178">Regression Shrinkage and Selection via the Lasso</a>.</p>
<p><img data-src="imgs/lasso.png"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="imgs/P55268-Robert-Tibshirani.jpg" style="width:60.0%"></p>
</div>
</div>
</section>
<section id="lasso-1" class="slide level2">
<h2>Lasso</h2>
<p><br></p>
<p>O lasso tem como objetivo encontrar um estimador de uma regress√£o linear que possui risco menor que o de m√≠nimos quadrados, possuindo duas grandes vantagens, em rela√ß√£o ao <em>stepwise</em>:</p>
<p><br></p>
<ol type="1">
<li class="fragment">Sua solu√ß√£o √© mais r√°pida, ainda que <em>stepwise</em> seja consideravalmente mais r√°pido do que avaliar <span class="math inline">2^d</span> modelos;</li>
<li class="fragment">O lasso √© capaz de selecionar automaticamente as vari√°veis mais relevantes para o modelo, reduzindo a dimensionalidade dos dados.</li>
</ol>
<p><br></p>
<p>A segunda vantagem ocorre, uma vez que ele realiza uma penaliza√ß√£o que leva √† estimativas de alguns coeficientes <span class="math inline">\beta_i</span> igual a zero, eliminando as vari√°veis menos importantes.</p>
<p><br></p>

<img data-src="gifs/uau.gif" class="r-stretch"></section>
<section id="lasso-2" class="slide level2">
<h2>Lasso</h2>
<p><br></p>
<p>No lasso, ao inv√©s de reduzir a vari√¢ncia do estimador de m√≠nimos quadrados usando a complexidade (<span class="math inline">L_0 = \sum_{i = 1}^d\mathbb{I}(\beta_i) \neq 0</span>) em <a href="#/melhor-subconjunto-de-covari√°veis-2" class="quarto-xref">Equa√ß√£o&nbsp;3</a>, usa-se a penaliza√ß√£o <span class="math inline">L_1 = \sum_{i = 1}^d|\beta_i|</span>. No lasso, buscamos:</p>
<p><span class="math display">\widehat{\beta}_{L_1,\lambda} = \argmin_{\beta_0 \in \mathbb{R}, \beta \in \mathbb{R}^d}\sum_{k = 1}^n\overbrace{\left(y_k - \beta_0 - \sum_{i = 1}^d \beta_i x_{x,i}\right)^2}^{n \times EQM} + \lambda \,\,\underbrace{\sum_{j = 1}^d|\beta_j|}_{\text{Penaliza√ß√£o}},</span> em que <span class="math inline">\lambda</span> √© um <em>tuning parameter</em>. Perceba que quando <span class="math inline">\lambda = 0</span>, ca√≠mos no caso do modelo de regress√£o por m√≠nimos quadrados sem penaliza√ß√£o. J√°, quando <span class="math inline">\lambda \rightarrow \infty</span>, temos um modelo em que todas as vari√°veis s√£o removidas, uma vez que a primeira parte do modelo torna-se insignificante.</p>
</section>
<section id="lasso-3" class="slide level2">
<h2>Lasso</h2>
<p><br> <br></p>
<p>Quando <span class="math inline">\lambda</span> √© grande, temos que</p>
<p><span class="math display">\sum_{k = 1}^n \left(y_k - \beta_0 - \sum_{j = 1}^d \beta_j x_{k,j}\right)^2 + \lambda \sum_{j = 1}^d |\beta_j| \approx \lambda \sum_{j = 1}^d |\beta_j|,</span> e portanto, <span class="math inline">\widehat{\beta}_1 = 0, \cdots, \widehat{\beta}_d = 0</span>.</p>
<p><br></p>
<p>A escolha de <span class="math inline">\lambda</span>, em geral, √© feita utilizado algum m√©todo de valida√ß√£o cruzada.</p>
</section>
<section id="lasso-4" class="slide level2">
<h2>Lasso</h2>
<p><br></p>
<p>O lasso √© extremamente r√°pido, e nos √∫ltimos anos, diversos algoritmos foram constru√≠dos para fazer essa tar√©fa de forma eficiente. O LARS foi um dos primeiros algoritmos desenvolvidos em 2010. Para detalhes, ler <a href="https://www.jstor.org/stable/2699986">Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of statistics, 1189‚Äì1232</a>.</p>
<p><br></p>
<p>No R, a regress√£o lasso poder√° ser feita usando a biblioteca <a href="https://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a>, assim:</p>
<p><br></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb9-2"><a href="#cb9-2"></a>ajuste <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">1</span>)</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>

<img data-src="gifs/teclado-anime.gif" style="width:21.0%" class="r-stretch"></section>
<section id="ridge" class="slide level2">
<h2>Ridge</h2>
<p><br></p>
<p>Uma alternativa que surgiu antes do lasso √© a <span class="red">regress√£o ridge</span>. Ela foi proposta no artigo <a href="https://www.jstor.org/stable/1267351">Hoerl, A. E. &amp; Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1), 55‚Äì67</a>. Aqui, utiliza-se como medida de complexidade a norma <span class="math inline">L_2</span>, em que, o estimador √© dado por:</p>
<p><span class="math display">\widehat{\beta}_{L_2,\lambda} = \argmin_{\beta_0 \in \mathbb{R}, \beta \in \mathbb{R}^d}\sum_{k = 1}^n\overbrace{\left(y_k - \beta_0 - \sum_{i = 1}^d \beta_i x_{x,i}\right)^2}^{n \times EQM} + \lambda \,\,\underbrace{\sum_{j = 1}^d\beta_j^2}_{\text{Penaliza√ß√£o}}.</span> Diferentemente do lasso, a regress√£o ridge possui solu√ß√£o anal√≠tica, dada por:</p>
<p><span class="math display">\widehat{\beta}_{L_2,\lambda} = ({\bf X}^{T}{\bf X} + \lambda\mathbb{\bf I}_0)^{-1}{\bf X}^{T}Y,</span> em que <span class="math inline">\mathbb{\bf I}_0</span> √© a matriz identidade <span class="math inline">(d + 1) \times (d + 1)</span> com <span class="math inline">\mathbb{\bf I}_0(1,1) = 0</span>.</p>
<p><br></p>
</section>
<section id="ridge-1" class="slide level2">
<h2>Ridge</h2>
<p><br></p>
<p>A regress√£o ridge poder√° ter uma vari√¢ncia menor que a regress√£o lasso, por√©m seu vi√©s poder√° ser maior. Outra caracter√≠stica da regress√£o ridge √© que ela possue uma √∫nica solu√ß√£o, enquanto a regress√£o lasso poder√° ter multiplas solu√ß√µes. Os autores tamb√©m demonstram que a regress√£o ridge lida melhor com multicolinearidade.</p>
<p><br></p>
<p>No R, tamb√©m poderemos utilizar a biblioteca <a href="https://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a>:</p>
<p><br></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb10-2"><a href="#cb10-2"></a>ajuste <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>)</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>

<img data-src="gifs/teclado-anime.gif" style="width:21.0%" class="r-stretch"></section>
<section id="elastic-net" class="slide level2">
<h2>Elastic Net</h2>
<p><br></p>
<p>Nesse tipo de modelo de gress√£o, combina-se as penaliza√ß√µes da regress√£o ridge com a utilizada na regress√£o lasso, herdando os benef√≠cios do uso de cada um dos m√©todos isoladamente, melhorando a estabilidade das estimativas do lasso, em situa√ß√µes de multicolinearidade entre as vari√°veis e tamb√©m permitindo a sele√ß√£o autom√°tica de vari√°veis.</p>
<p><span class="math display">(1-\alpha)\widehat{\beta}_{L_2,\lambda} + \alpha\widehat{\beta}_{L_1,\lambda},</span> em que <span class="math inline">0 \leq \alpha \leq1</span>. Em R, basta especificar para a fun√ß√£o <code>glmnet</code> um valor de <span class="math inline">\alpha</span> diferente de 0 e 1.</p>
<p><br></p>

<img data-src="gifs/teclado-anime.gif" style="width:21.0%" class="r-stretch"></section>
<section id="exemplo-emq-ridge-lasso-e-elastic-net" class="slide level2">
<h2>Exemplo: EMQ, Ridge, Lasso e Elastic Net</h2>
<p><br></p>
<p><span class="red">Exemplo</span>: Considere uma base de dados simulada, com <span class="math inline">n = 500</span> observa√ß√µes, de tal forma que</p>
<p><span class="math display">Y = 3X_{1} - 2X_2 + X_3 + -3X_4 + X_5 + \sum_{i = 6}^{20}0X_i + \varepsilon,</span> em que <span class="math inline">\varepsilon \sim \text{Normal}(0, 0.5^2)</span> e <span class="math inline">X_i \sim \text{Normal}(0, 1)</span>, independentes, com <span class="math inline">i = 1, \cdots, 20</span>. Desejamos ajustar quatro modelos de regress√£o. Para o caso do Estimador de M√≠nimos Quadrados - EMQ e do modelo Ridge, que n√£o tem sele√ß√£o autom√°tica de vari√°veis, usaremos <strong>todas</strong> as vari√°veis. Desejamos avaliar o risco estimado de cada uma das regress√µes.</p>

<img data-src="gifs/teclado-anime.gif" style="width:21.0%" class="r-stretch"></section>
<section id="exemplo-emq-ridge-lasso-e-elastic-net-1" class="slide level2">
<h2>Exemplo: EMQ, Ridge, Lasso e Elastic Net</h2>
<p><br></p>
<div class="cell">
<details>
<summary>Solu√ß√£o utilizando o tidymodels</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="fu">library</span>(tibble)</span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="fu">library</span>(purrr)</span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb11-6"><a href="#cb11-6"></a></span>
<span id="cb11-7"><a href="#cb11-7"></a><span class="co"># Removendo poss√≠veis conflitos de pacotes --------------------------------</span></span>
<span id="cb11-8"><a href="#cb11-8"></a>tidymodels<span class="sc">::</span><span class="fu">tidymodels_prefer</span>()</span>
<span id="cb11-9"><a href="#cb11-9"></a></span>
<span id="cb11-10"><a href="#cb11-10"></a><span class="co"># Fun√ß√£o para gerar os dados ----------------------------------------------</span></span>
<span id="cb11-11"><a href="#cb11-11"></a>gerando_dados <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">n =</span> 300L){</span>
<span id="cb11-12"><a href="#cb11-12"></a>  regressao <span class="ot">&lt;-</span> <span class="cf">function</span>(i){</span>
<span id="cb11-13"><a href="#cb11-13"></a>    x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> 5L)</span>
<span id="cb11-14"><a href="#cb11-14"></a>    y <span class="ot">&lt;-</span> <span class="dv">3</span><span class="sc">*</span>x[1L] <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>x[2L] <span class="sc">+</span> x[3L] <span class="sc">-</span> <span class="dv">3</span><span class="sc">*</span>x[4L] <span class="sc">+</span> x[5L] <span class="sc">+</span> <span class="fu">rnorm</span>(1L, <span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb11-15"><a href="#cb11-15"></a>    <span class="fu">tibble</span>(</span>
<span id="cb11-16"><a href="#cb11-16"></a>      <span class="at">y =</span> y,</span>
<span id="cb11-17"><a href="#cb11-17"></a>      <span class="at">x1 =</span> x[1L],</span>
<span id="cb11-18"><a href="#cb11-18"></a>      <span class="at">x2 =</span> x[2L],</span>
<span id="cb11-19"><a href="#cb11-19"></a>      <span class="at">x3 =</span> x[3L],</span>
<span id="cb11-20"><a href="#cb11-20"></a>      <span class="at">x4 =</span> x[4L],</span>
<span id="cb11-21"><a href="#cb11-21"></a>      <span class="at">x5 =</span> x[5L]</span>
<span id="cb11-22"><a href="#cb11-22"></a>    )</span>
<span id="cb11-23"><a href="#cb11-23"></a>  }</span>
<span id="cb11-24"><a href="#cb11-24"></a>  dados <span class="ot">&lt;-</span> purrr<span class="sc">::</span><span class="fu">map</span>(<span class="at">.x =</span> 1L<span class="sc">:</span>n, <span class="at">.f =</span> regressao) <span class="sc">|&gt;</span> </span>
<span id="cb11-25"><a href="#cb11-25"></a>    purrr<span class="sc">::</span><span class="fu">list_rbind</span>()</span>
<span id="cb11-26"><a href="#cb11-26"></a>  </span>
<span id="cb11-27"><a href="#cb11-27"></a>  parte_esparsa <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, n, <span class="dv">15</span>)</span>
<span id="cb11-28"><a href="#cb11-28"></a>  </span>
<span id="cb11-29"><a href="#cb11-29"></a>  dados <span class="ot">&lt;-</span> <span class="fu">cbind</span>(dados, parte_esparsa)</span>
<span id="cb11-30"><a href="#cb11-30"></a>  <span class="fu">colnames</span>(dados) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"y"</span>, <span class="fu">paste0</span>(<span class="st">"x"</span>, 2L<span class="sc">:</span><span class="fu">ncol</span>(dados)))</span>
<span id="cb11-31"><a href="#cb11-31"></a>  <span class="fu">as_tibble</span>(dados)</span>
<span id="cb11-32"><a href="#cb11-32"></a>}</span>
<span id="cb11-33"><a href="#cb11-33"></a></span>
<span id="cb11-34"><a href="#cb11-34"></a>dados <span class="ot">&lt;-</span> <span class="fu">gerando_dados</span>(<span class="at">n =</span> <span class="dv">500</span>)</span>
<span id="cb11-35"><a href="#cb11-35"></a></span>
<span id="cb11-36"><a href="#cb11-36"></a><span class="co"># Divis√£o inicial da base -------------------------------------------------</span></span>
<span id="cb11-37"><a href="#cb11-37"></a>hod_out <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(dados, <span class="at">prop =</span> <span class="fl">0.7</span>)</span>
<span id="cb11-38"><a href="#cb11-38"></a>treinamento <span class="ot">&lt;-</span> <span class="fu">training</span>(hod_out)</span>
<span id="cb11-39"><a href="#cb11-39"></a>teste <span class="ot">&lt;-</span> <span class="fu">testing</span>(hod_out)</span>
<span id="cb11-40"><a href="#cb11-40"></a></span>
<span id="cb11-41"><a href="#cb11-41"></a><span class="co"># Setando o modelo (set engine) -------------------------------------------</span></span>
<span id="cb11-42"><a href="#cb11-42"></a>modelo_eqm <span class="ot">&lt;-</span> </span>
<span id="cb11-43"><a href="#cb11-43"></a>  <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="dv">0</span>, <span class="at">mixture =</span> <span class="dv">0</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-44"><a href="#cb11-44"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-45"><a href="#cb11-45"></a>  <span class="fu">set_engine</span>(<span class="st">"glmnet"</span>)</span>
<span id="cb11-46"><a href="#cb11-46"></a>  </span>
<span id="cb11-47"><a href="#cb11-47"></a>modelo_ridge <span class="ot">&lt;-</span> </span>
<span id="cb11-48"><a href="#cb11-48"></a>  <span class="fu">linear_reg</span>(<span class="at">penalty =</span> tune<span class="sc">::</span><span class="fu">tune</span>(), <span class="at">mixture =</span> <span class="dv">0</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-49"><a href="#cb11-49"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-50"><a href="#cb11-50"></a>  <span class="fu">set_engine</span>(<span class="st">"glmnet"</span>)</span>
<span id="cb11-51"><a href="#cb11-51"></a></span>
<span id="cb11-52"><a href="#cb11-52"></a>modelo_lasso <span class="ot">&lt;-</span> </span>
<span id="cb11-53"><a href="#cb11-53"></a>  parsnip<span class="sc">::</span><span class="fu">linear_reg</span>(<span class="at">penalty =</span> tune<span class="sc">::</span><span class="fu">tune</span>(), <span class="at">mixture =</span> <span class="dv">1</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-54"><a href="#cb11-54"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-55"><a href="#cb11-55"></a>  parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"glmnet"</span>)</span>
<span id="cb11-56"><a href="#cb11-56"></a>  </span>
<span id="cb11-57"><a href="#cb11-57"></a>modelo_elastic <span class="ot">&lt;-</span> </span>
<span id="cb11-58"><a href="#cb11-58"></a>  parsnip<span class="sc">::</span><span class="fu">linear_reg</span>(<span class="at">penalty =</span> tune<span class="sc">::</span><span class="fu">tune</span>(), <span class="at">mixture =</span> tune<span class="sc">::</span><span class="fu">tune</span>()) <span class="sc">|&gt;</span> </span>
<span id="cb11-59"><a href="#cb11-59"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-60"><a href="#cb11-60"></a>  parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"glmnet"</span>)</span>
<span id="cb11-61"><a href="#cb11-61"></a></span>
<span id="cb11-62"><a href="#cb11-62"></a><span class="co"># Criando workflows -------------------------------------------------------</span></span>
<span id="cb11-63"><a href="#cb11-63"></a>all_wf <span class="ot">&lt;-</span> </span>
<span id="cb11-64"><a href="#cb11-64"></a>  <span class="fu">workflow_set</span>(</span>
<span id="cb11-65"><a href="#cb11-65"></a>    <span class="at">preproc =</span> <span class="fu">list</span>(y <span class="sc">~</span> . ),</span>
<span id="cb11-66"><a href="#cb11-66"></a>    <span class="at">models =</span> <span class="fu">list</span>(<span class="at">eqm =</span> modelo_eqm, <span class="at">ridge =</span> modelo_ridge, <span class="at">lasso =</span> modelo_lasso, <span class="at">elastic =</span> modelo_elastic), </span>
<span id="cb11-67"><a href="#cb11-67"></a>    <span class="at">cross =</span> <span class="cn">TRUE</span></span>
<span id="cb11-68"><a href="#cb11-68"></a>  )</span>
<span id="cb11-69"><a href="#cb11-69"></a></span>
<span id="cb11-70"><a href="#cb11-70"></a><span class="co"># Valida√ß√£o cruzada -------------------------------------------------------</span></span>
<span id="cb11-71"><a href="#cb11-71"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb11-72"><a href="#cb11-72"></a>cv <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">vfold_cv</span>(treinamento, <span class="at">v =</span> 5L)</span>
<span id="cb11-73"><a href="#cb11-73"></a></span>
<span id="cb11-74"><a href="#cb11-74"></a><span class="co"># Setando a m√©trica -------------------------------------------------------</span></span>
<span id="cb11-75"><a href="#cb11-75"></a>metrica <span class="ot">&lt;-</span> yardstick<span class="sc">::</span><span class="fu">metric_set</span>(rmse)</span>
<span id="cb11-76"><a href="#cb11-76"></a></span>
<span id="cb11-77"><a href="#cb11-77"></a><span class="co"># Tunagem dos hiperpar√¢metros ---------------------------------------------</span></span>
<span id="cb11-78"><a href="#cb11-78"></a><span class="co"># A semente (seed = 0) faz com que dentro da valida√ß√£o cruzada para cada modelo</span></span>
<span id="cb11-79"><a href="#cb11-79"></a><span class="co"># a semente seja sempre a mesma.</span></span>
<span id="cb11-80"><a href="#cb11-80"></a>tunagem <span class="ot">&lt;-</span> </span>
<span id="cb11-81"><a href="#cb11-81"></a>  all_wf <span class="sc">|&gt;</span> </span>
<span id="cb11-82"><a href="#cb11-82"></a>  <span class="fu">workflow_map</span>(</span>
<span id="cb11-83"><a href="#cb11-83"></a>    <span class="at">seed =</span> <span class="dv">0</span>, </span>
<span id="cb11-84"><a href="#cb11-84"></a>    <span class="at">verbose =</span> <span class="cn">TRUE</span>,</span>
<span id="cb11-85"><a href="#cb11-85"></a>    <span class="at">resamples =</span> cv,</span>
<span id="cb11-86"><a href="#cb11-86"></a>    <span class="at">grid =</span> <span class="dv">50</span>,</span>
<span id="cb11-87"><a href="#cb11-87"></a>    <span class="at">metrics =</span> metrica</span>
<span id="cb11-88"><a href="#cb11-88"></a>  )</span>
<span id="cb11-89"><a href="#cb11-89"></a></span>
<span id="cb11-90"><a href="#cb11-90"></a><span class="co"># Rank dos melhores modelos -----------------------------------------------</span></span>
<span id="cb11-91"><a href="#cb11-91"></a>modelos_rank <span class="ot">&lt;-</span> tunagem <span class="sc">|&gt;</span> <span class="fu">rank_results</span>()</span>
<span id="cb11-92"><a href="#cb11-92"></a></span>
<span id="cb11-93"><a href="#cb11-93"></a>melhor_eqm <span class="ot">&lt;-</span> </span>
<span id="cb11-94"><a href="#cb11-94"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-95"><a href="#cb11-95"></a>  <span class="fu">extract_workflow_set_result</span>(<span class="st">"formula_eqm"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-96"><a href="#cb11-96"></a>  <span class="fu">select_best</span>(<span class="st">"rmse"</span>)</span>
<span id="cb11-97"><a href="#cb11-97"></a></span>
<span id="cb11-98"><a href="#cb11-98"></a>melhor_ridge <span class="ot">&lt;-</span> </span>
<span id="cb11-99"><a href="#cb11-99"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-100"><a href="#cb11-100"></a>  <span class="fu">extract_workflow_set_result</span>(<span class="st">"formula_ridge"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-101"><a href="#cb11-101"></a>  <span class="fu">select_best</span>(<span class="st">"rmse"</span>)</span>
<span id="cb11-102"><a href="#cb11-102"></a></span>
<span id="cb11-103"><a href="#cb11-103"></a>melhor_lasso <span class="ot">&lt;-</span> </span>
<span id="cb11-104"><a href="#cb11-104"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-105"><a href="#cb11-105"></a>  <span class="fu">extract_workflow_set_result</span>(<span class="st">"formula_lasso"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-106"><a href="#cb11-106"></a>  <span class="fu">select_best</span>(<span class="st">"rmse"</span>)</span>
<span id="cb11-107"><a href="#cb11-107"></a></span>
<span id="cb11-108"><a href="#cb11-108"></a>melhor_elastic <span class="ot">&lt;-</span> </span>
<span id="cb11-109"><a href="#cb11-109"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-110"><a href="#cb11-110"></a>  <span class="fu">extract_workflow_set_result</span>(<span class="st">"formula_elastic"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-111"><a href="#cb11-111"></a>  <span class="fu">select_best</span>(<span class="st">"rmse"</span>)</span>
<span id="cb11-112"><a href="#cb11-112"></a></span>
<span id="cb11-113"><a href="#cb11-113"></a>finalizando_eqm <span class="ot">&lt;-</span> </span>
<span id="cb11-114"><a href="#cb11-114"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-115"><a href="#cb11-115"></a>  <span class="fu">extract_workflow</span>(<span class="st">"formula_eqm"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-116"><a href="#cb11-116"></a>  <span class="fu">finalize_workflow</span>(melhor_eqm) <span class="sc">|&gt;</span> </span>
<span id="cb11-117"><a href="#cb11-117"></a>  <span class="fu">last_fit</span>(<span class="at">split =</span> hod_out)</span>
<span id="cb11-118"><a href="#cb11-118"></a></span>
<span id="cb11-119"><a href="#cb11-119"></a>finalizando_ridge <span class="ot">&lt;-</span> </span>
<span id="cb11-120"><a href="#cb11-120"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-121"><a href="#cb11-121"></a>  <span class="fu">extract_workflow</span>(<span class="st">"formula_ridge"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-122"><a href="#cb11-122"></a>  <span class="fu">finalize_workflow</span>(melhor_ridge) <span class="sc">|&gt;</span> </span>
<span id="cb11-123"><a href="#cb11-123"></a>  <span class="fu">last_fit</span>(<span class="at">split =</span> hod_out)</span>
<span id="cb11-124"><a href="#cb11-124"></a></span>
<span id="cb11-125"><a href="#cb11-125"></a>finalizando_lasso <span class="ot">&lt;-</span> </span>
<span id="cb11-126"><a href="#cb11-126"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-127"><a href="#cb11-127"></a>  <span class="fu">extract_workflow</span>(<span class="st">"formula_lasso"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-128"><a href="#cb11-128"></a>  <span class="fu">finalize_workflow</span>(melhor_lasso) <span class="sc">|&gt;</span> </span>
<span id="cb11-129"><a href="#cb11-129"></a>  <span class="fu">last_fit</span>(<span class="at">split =</span> hod_out)</span>
<span id="cb11-130"><a href="#cb11-130"></a></span>
<span id="cb11-131"><a href="#cb11-131"></a>finalizando_elastic <span class="ot">&lt;-</span> </span>
<span id="cb11-132"><a href="#cb11-132"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-133"><a href="#cb11-133"></a>  <span class="fu">extract_workflow</span>(<span class="st">"formula_elastic"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-134"><a href="#cb11-134"></a>  <span class="fu">finalize_workflow</span>(melhor_elastic) <span class="sc">|&gt;</span> </span>
<span id="cb11-135"><a href="#cb11-135"></a>  <span class="fu">last_fit</span>(<span class="at">split =</span> hod_out)</span>
<span id="cb11-136"><a href="#cb11-136"></a></span>
<span id="cb11-137"><a href="#cb11-137"></a><span class="co"># Visualizando as m√©tricas</span></span>
<span id="cb11-138"><a href="#cb11-138"></a>finalizando_eqm <span class="sc">|&gt;</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb11-139"><a href="#cb11-139"></a>finalizando_ridge <span class="sc">|&gt;</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb11-140"><a href="#cb11-140"></a>finalizando_lasso <span class="sc">|&gt;</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb11-141"><a href="#cb11-141"></a>finalizando_elastic <span class="sc">|&gt;</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb11-142"><a href="#cb11-142"></a></span>
<span id="cb11-143"><a href="#cb11-143"></a><span class="co"># Visualizando predi√ß√µes:</span></span>
<span id="cb11-144"><a href="#cb11-144"></a>finalizando_eqm <span class="sc">|&gt;</span> <span class="fu">collect_predictions</span>()</span>
<span id="cb11-145"><a href="#cb11-145"></a>finalizando_ridge <span class="sc">|&gt;</span> <span class="fu">collect_predictions</span>()</span>
<span id="cb11-146"><a href="#cb11-146"></a>finalizando_lasso <span class="sc">|&gt;</span> <span class="fu">collect_predictions</span>()</span>
<span id="cb11-147"><a href="#cb11-147"></a>finalizando_elastic <span class="sc">|&gt;</span> <span class="fu">collect_predictions</span>()</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="exerc√≠cios-2" class="slide level2">
<h2>üìö Exerc√≠cios</h2>
<p><br> <span class="red">Exerc√≠cio</span>: Utilizando os dados de vinho vermelhoüç∑, dispon√≠veis <a href="https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009">aqui</a>, fa√ßa uma pequena an√°lise explorat√≥ria dos dados. No <a href="https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009">link</a> do Kaggle voc√™ consegue uma explica√ß√£o sobre o que significa cada uma das vari√°veis.</p>

<img data-src="gifs/mr-bean-test.gif" width="211" class="r-stretch"></section>
<section id="exerc√≠cios-3" class="slide level2">
<h2>üìö Exerc√≠cios</h2>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Utilizando os dados de vinho vermelhoüç∑, dispon√≠veis <a href="https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009">aqui</a>, obtenha o melhor modelo de regress√£o linar para modelar a qualidade do vinho, considerando:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><span class="math inline">g_1</span> - M√©todo dos m√≠nimos quadrados;</li>
<li class="fragment"><span class="math inline">g_2</span> - Regress√£o ridge;</li>
<li class="fragment"><span class="math inline">g_3</span> - Regressao lasso;</li>
<li class="fragment"><span class="math inline">g_4</span> - Elastic Net.</li>
</ol>
<p><br></p>
<p>Voc√™ dever√° selecionar o melhor modelo de cada uma das classes de modelos de regress√£o e construir uma tabela com o risco estimado <span class="math inline">\widehat{R}(g_i),\,\, i = 1, \cdots, 4</span>, em que aqui <span class="math inline">g_i</span> representa o modelo geral n√£o estimado. Ao fim, construa quatro gr√°ficos mostrando o ajuste de cada um dos modelos.</p>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Se voc√™ utilizou o <a href="https://www.tidymodels.org/">tidymodels</a> para resolver o exerc√≠cio anterior, rafa√ßa usando a biblioteca <a href="https://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a>. Caso contr√°rio, resolva-o utilizando o <a href="https://www.tidymodels.org/">tidymodels</a>.</p>
</section>
<section id="exerc√≠cios-4" class="slide level2">
<h2>üìö Exerc√≠cios</h2>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Considere agora o conjunto de dados de despesas m√©dicas, dispon√≠vel <a href="https://www.kaggle.com/datasets/mirichoi0218/insurance">aqui</a>, refa√ßa o mesmo exerc√≠cio dos dados de vinho vermelho, em que aqui, o objetivo √© prever a vari√°vel <code>charges</code>. Perceba que algumas vari√°veis s√£o qualitativas, e port√£o, voc√™ dever√° transform√°-las em <span class="red"><em>dummy</em></span>. Indique os melhores cen√°rios dos quatro modelos e informe qual modelo voc√™ utilizaria. Explique!</p>
<p><br></p>

<img data-src="gifs/bean_simple.gif" class="r-stretch"></section>
<section id="m√©todos-n√£o-param√©tricos" class="slide level2">
<h2>M√©todos n√£o-param√©tricos</h2>
<p><br></p>
<p>M√©todos param√©tricos podem impor muitas limita√ß√µes na solu√ß√£o de um problema de regress√£o, i.e., em problemas que desejamos estimar a fun√ß√£o de regress√£o <span class="math inline">r({\bf x})</span>. Por exemplo, nem sempre o melhor estimador linear √© um bom estimador para <span class="math inline">r({\bf x})</span>.</p>
<p><br></p>
<p>M√©todos param√©tricos s√£o muitas vezes simplistas e restritivos, em que normalmente abrimos m√£os para se ter um estimador um pouco mais viesado, em detrimento da diminui√ß√£o da vari√¢ncia do modelo. Por exemplo, nas regress√µes penalizadas que vimos anteriormente (<strong>ridge</strong>, <strong>lasso</strong> e <strong>elastic-net</strong>), penalizamos modelos com muitas covari√°veis o que naturalmente aumentar√° o vi√©s, na maioria das vezes.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="m√©todos-n√£o-param√©tricos-1" class="slide level2">
<h2>M√©todos n√£o-param√©tricos</h2>
<p><br></p>
<p>Em situa√ß√µes em que temos muitos dados (<span class="math inline">n</span> grande), os modelos n√£o-param√©tricos possuem, em geral, boa peformance, uma vez que apesar de que nessa classe de modelos existir um aumento da vari√¢ncia, por√©m, seguida de uma redu√ß√£o de vi√©s, a vari√¢ncia do modelo n√£o aumenta muito.</p>
<p><br></p>
<p>De um lado, nos modelos de regress√£o que vimos at√© o momento, introduzimos uma penaliza√ß√£o para diminuir a vari√¢ncia do modelo frente ao m√©todo dos m√≠nimos quadrados (quando n√£o usamos penaliza√ß√£o) em troca de um aumento no v√≠es. Aqui, em modelos n√£o param√©tricos, desejamos fazer a troca oposta, i.e., diminuir o vi√©s, em troca de um ganho na vari√¢ncia no modelo.</p>
<p><br></p>
<p>Qualquer abordagem param√©trica tr√°s consigo a possibilidade de que a forma funcional <span class="math inline">g</span> para estimar <span class="math inline">f</span> seja muito diferente da verdadeira, claro, se o modelo resultante n√£o se ajustar bem aos dados, isto √© <span class="math inline">\widehat{g}</span> n√£o tem boa capacidade preditiva, muito embora, tamb√©m √© poss√≠vel que tenhamos <span class="math inline">\widehat{g}</span> com boa capacidade preditiva, por√©m, n√£o represente a estrutura real de <span class="math inline">f</span> (sempre desconhecida).</p>
</section>
<section id="m√©todos-n√£o-param√©tricos-2" class="slide level2">
<h2>M√©todos n√£o-param√©tricos</h2>
<p><br> <br></p>
<p>Isso n√£o √© regra, por√©m ajuda a entender um pouco o dilema entre essas classes de modelos (param√©tricos e n√£o-param√©tricos).</p>
<p><br></p>

<img data-src="imgs/vies_variancia_nao_parametrico.png" class="r-stretch quarto-figure-center"></section>
<section id="k-vizinhos-mais-pr√≥ximo-pr√≥ximos" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais pr√≥ximo pr√≥ximos</h2>
<p><br></p>
<p>Tamb√©m chamado de <span class="red"><span class="math inline">k</span>-nearest neighbours - <span class="math inline">k</span>NN</span>, o <span class="math inline">k</span>NN √© um dos m√©todos mais populares na comunidade de aprendizagem de m√°quina. O m√©todo foi formulado em uma sequ√™ncia de dois artigos:</p>
<p><br></p>
<p>1 - Benedetti, J. K. (1977). <strong>On the nonparametric estimation of regression functions</strong>. Journal of the Royal Statistical Society. Series B (Methodological), 248‚Äì253;</p>
<p><br></p>
<p>2 - Stone, C. J. (1977). <strong>Consistent nonparametric regression</strong>. The Annals of Statistics, 595‚Äì 620.</p>
<p><br></p>
<p>A ideia do m√©todo √© estimar a fun√ß√£o de regress√£o <span class="math inline">r({\bf x})</span>, para um dado <span class="math inline">{\bf x}</span> com base nas respostas <span class="math inline">Y</span> dos <span class="math inline">k</span>-vizinhos mais pr√≥ximos de <span class="math inline">{\bf x}</span>.</p>
</section>
<section id="k-vizinhos-mais-pr√≥ximo-pr√≥ximos-1" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais pr√≥ximo pr√≥ximos</h2>
<p><br></p>
<p>Formalmente, temos que</p>
<p><span class="math display">g({\bf x^*}) = \frac{1}{k}\sum_{i \in \mathcal{N}_{\bf x^*}}y_i,</span> em que <span class="math inline">\mathcal{N}_{\bf x}</span> √© o conjunto √≠ndices das <span class="math inline">k</span> observa√ß√µes mais pr√≥ximas de <span class="math inline">{\bf x}</span>, i.e,</p>
<p><span class="math display">\mathcal{N}_{\bf x^*} = \{i \in \{1, \cdots, n\}\, : \, d({\bf x}_i, {\bf x^*}) \leq d_{\bf x^*}^k\},</span> em que <span class="math inline">d_{\bf x^*}^k</span> √© a dist√¢ncia do <span class="math inline">k</span>-√©simo vizinho mais pr√≥ximo de <span class="math inline">\bf{x^*}</span> em <span class="math inline">\bf{x}</span>. Portanto, o valor da regress√£o no ponto <span class="math inline">{\bf x^*},</span> i.e., o valor de <span class="math inline">r({\bf x^*}) = \mathbb{E}(Y|{\bf X} = {\bf x^*})</span> √© estimado pela m√©dia de <span class="math inline">Y_{N_{\bf x^*}}</span>. Ou seja, estimamos por:</p>
<p><span class="math display">\overline{Y}_{N_{\bf x^*}} = \frac{1}{k} \sum_{i \in \mathcal{N}_{\bf x^*}}y_i.</span></p>
</section>
<section id="k-vizinhos-mais-pr√≥ximo-pr√≥ximos-2" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais pr√≥ximo pr√≥ximos</h2>
<p><br> <!-- http://leg.ufpr.br/~lucambio/MSM/MSM03.html --> Para determinar <span class="math inline">d_{\bf x^*}^k</span>, poderemos utilizar alguma m√©trica de dist√¢ncia e assim mensurarmos a proximidade. Entre elas:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><span class="red">Dist√¢ncia Euclidiana</span> ou dist√¢ncia <span class="math inline">L_2</span>: <span class="math inline">d(x^a, x^b) = \sqrt{(x_1^a - x_1^b)^2 + \cdots + (x_d^a - x_d^b)^2}</span>;</li>
</ol>
<p><br></p>
<ol start="2" type="1">
<li class="fragment"><span class="red">Dist√¢ncia de Manhattan</span>, City Block ou dist√¢ncia <span class="math inline">L_1</span>: <span class="math inline">d(x^a, x^b) = \sqrt{|x_1^a - x_1^b| + \cdots + |x_d^a - x_d^b|}</span>;</li>
</ol>
<p><br></p>
<ol start="2" type="1">
<li class="fragment"><span class="red">Dist√¢ncia de Mahalanobis</span>: <span class="math inline">d(x^a, x^b) = \sqrt{(x^a - x^b)^{T}S^{-1}(x^a - x^b)}</span>, em que <span class="math inline">S</span> √© a matriz de covari√¢ncia, em que na diagonal princial temos as vari√¢ncias e fora dela as covari√¢ncias entre os pontos. Lembre-se que se <span class="math inline">x</span> e <span class="math inline">y</span> s√£o vetores de dados quaisquer, a interdepend√™ncia linear entre eles poder√° ser estimada como:</li>
</ol>
<p><br></p>
<p><span class="math display">\mathrm{cov}(x, y) = \frac{1}{n-1}\sum_{i = 1}^n (x_i - \overline{x})(y_i - \overline{y}).</span></p>
</section>
<section id="k-vizinhos-mais-pr√≥ximo-pr√≥ximos-3" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais pr√≥ximo pr√≥ximos</h2>
<p><br></p>
<p>Voc√™ poder√° utilizar qualquer outra medida de dist√¢ncia al√©m das que foram citadas acima.</p>
<p><br></p>
<p>√â importante perceber que <strong>o m√©todo</strong> <span class="math inline">k</span>NN n√£o faz uma ‚Äúcompress√£o‚Äù dos dados como a regress√£o linear que estudamos. L√°, temos uma equa√ß√£o que utilizamos para estimar o valor de <span class="math inline">Y</span>, ap√≥s as estima√ß√£o dos coeficientes do modelo de regress√£o, ou seja, n√£o precisamos mais dos dados para estimar novas observa√ß√µes. J√° no <span class="math inline">k</span>NN, precisamos sempre nos recorrer aos dados para fazer novas predi√ß√µes, ou seja, sempre que desejarmos calcular <span class="math inline">r({\bf x}) = \mathbb{E}(Y|{\bf X} = {\bf x})</span> deveremos sempre fazer uma nova consulta aos dados para calcular a m√©dia dos vizinhos mais pr√≥ximos. O <span class="math inline">k</span>NN n√£o possui coeficientes para interpretar. Diferentemente da regress√£o linear, o <span class="math inline">k</span>NN √© um pouco mais ‚Äú<em>black box</em>‚Äù.</p>
<p><br></p>

<img data-src="gifs/giphy.gif" class="r-stretch"></section>
<section id="k-vizinhos-mais-pr√≥ximo-pr√≥ximos-4" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais pr√≥ximo pr√≥ximos</h2>
<p><br> O valor da constante <span class="math inline">k</span> √© um hiperpar√¢metro do <span class="math inline">k</span>NN e dever√° ser obtido por valida√ß√£o cruzada. Perceba que se <span class="math inline">k = n</span> temos um modelo muito viesado, por√©m com vari√¢ncia pequena. Isso, porqu√™ para <span class="math inline">k = n</span> basicamente iremos tirar uma m√©dia dos dados. Para <span class="math inline">k = 1</span>, teremos um <em>overfitting</em>, uma vez que o estimador ir√° interpolar os dados.</p>
<p><br> <span class="red">Exemplo</span>: Considere novamente o conjunto de dados de expectativa de vida versus PIB per Capita dispon√≠veis <a href="https://github.com/prdm0/dados/blob/main/dados_expectativa_renda.RData">aqui</a>. Utilizando o <a href="https://www.tidymodels.org/">tidymodels</a>, vamos construir uma fu√ß√£o que retorne um gr√°fico com as estimativas do <span class="math inline">k</span>NN. A fun√ß√£o receber√° como argumentos o conjunto de dados e o valor de <span class="math inline">k</span>. Voc√™ tamb√©m poderia utilizar outras bibliotecas, como por exemplo a <a href="https://cran.r-project.org/web/packages/FNN/index.html">FNN</a>, ou a <a href="https://github.com/KlausVigo/kknn">KKNN</a>, esta √∫tlima √© a que √© utilizada internamente na biblioteca <a href="https://parsnip.tidymodels.org/">parsnip</a> do <a href="https://www.tidymodels.org/">tidymodels</a>.</p>
<p><br></p>

<img data-src="imgs/parsnip.png" class="r-stretch"></section>
<section id="k-vizinhos-mais-pr√≥ximo-pr√≥ximos-5" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais pr√≥ximo pr√≥ximos</h2>
<p><br></p>
<p>Abrindo apenas um par√™ntese, o <a href="https://www.tidymodels.org/">tidymodels</a> refere-se a um conjunto de pacotes que s√£o √∫teis para o tratamento, treinamento, tunagem e avalia√ß√£o de modelos de aprendizagem de m√°quina, em que o <a href="https://parsnip.tidymodels.org/">parsnip</a> implementa as <em>engines</em> (algoritmos/motores) que iremos utilizar para treinar um modelo. Na verdade, os algoritmos est√£o implementados em pacotes de terceiros e n√£o precisamente no <a href="https://parsnip.tidymodels.org/">parsnip</a>. Por√©m, o <a href="https://parsnip.tidymodels.org/">parsnip</a> unifica a sintaxe de diversos algoritmos implementados em pacotes separados.</p>
<p><br></p>

<img data-src="gifs/thumbs-up-nod.gif" class="r-stretch"></section>
<section id="k-vizinhos-mais-pr√≥ximo-pr√≥ximos-6" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais pr√≥ximo pr√≥ximos</h2>
<p><br></p>
<p>Observe que na imagem abaixo, a Figura <span class="red">A</span>, quando <span class="math inline">k=1</span>, percebemos initidamente que houve <em>overfitting</em>, i.e., h√° uma interpola√ß√£o dos dados. J√° na Figura <span class="red">D</span> temos um modelo com vari√¢ncia menor, por√©m, este √© muito simplista, o que sugere um alto vi√©s.</p>
<p><br></p>

<img data-src="imgs/knn_plot.png" class="r-stretch quarto-figure-center"></section>
<section id="k-vizinhos-mais-pr√≥ximo-pr√≥ximos-7" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais pr√≥ximo pr√≥ximos</h2>
<p><br></p>
<p>Estude o c√≥digo! Ele fornece a solu√ß√£o para o exemplo.</p>
<p><br></p>
<div class="cell">
<details>
<summary>Solu√ß√£o pelo m√©todo knn do exemplo anterior</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="fu">library</span>(glue)</span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb12-6"><a href="#cb12-6"></a></span>
<span id="cb12-7"><a href="#cb12-7"></a>tidymodels<span class="sc">::</span><span class="fu">tidymodels_prefer</span>()</span>
<span id="cb12-8"><a href="#cb12-8"></a></span>
<span id="cb12-9"><a href="#cb12-9"></a><span class="co"># Lendo dados</span></span>
<span id="cb12-10"><a href="#cb12-10"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb12-11"><a href="#cb12-11"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb12-12"><a href="#cb12-12"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb12-13"><a href="#cb12-13"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb12-14"><a href="#cb12-14"></a></span>
<span id="cb12-15"><a href="#cb12-15"></a>dados <span class="ot">&lt;-</span> </span>
<span id="cb12-16"><a href="#cb12-16"></a>  dados_expectativa_renda <span class="sc">|&gt;</span> </span>
<span id="cb12-17"><a href="#cb12-17"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>CountryName) <span class="sc">|&gt;</span> </span>
<span id="cb12-18"><a href="#cb12-18"></a>  dplyr<span class="sc">::</span><span class="fu">rename</span>(<span class="at">y =</span> LifeExpectancy, <span class="at">x =</span> GDPercapita)</span>
<span id="cb12-19"><a href="#cb12-19"></a></span>
<span id="cb12-20"><a href="#cb12-20"></a>knn_exp_pip <span class="ot">&lt;-</span> <span class="cf">function</span>(dados, <span class="at">k =</span> 1L){</span>
<span id="cb12-21"><a href="#cb12-21"></a>  <span class="co"># Criando receita</span></span>
<span id="cb12-22"><a href="#cb12-22"></a>  receita <span class="ot">&lt;-</span> <span class="fu">recipe</span>(y <span class="sc">~</span> x, <span class="at">data =</span> dados)</span>
<span id="cb12-23"><a href="#cb12-23"></a>  </span>
<span id="cb12-24"><a href="#cb12-24"></a>  <span class="co"># Definindo o modelo</span></span>
<span id="cb12-25"><a href="#cb12-25"></a>  modelo_knn <span class="ot">&lt;-</span> <span class="fu">nearest_neighbor</span>(<span class="at">neighbors =</span> k) <span class="sc">|&gt;</span> </span>
<span id="cb12-26"><a href="#cb12-26"></a>    <span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb12-27"><a href="#cb12-27"></a>    <span class="fu">set_engine</span>(<span class="st">"kknn"</span>)</span>
<span id="cb12-28"><a href="#cb12-28"></a>  </span>
<span id="cb12-29"><a href="#cb12-29"></a>  <span class="co"># Workflow</span></span>
<span id="cb12-30"><a href="#cb12-30"></a>  ajuste_final <span class="ot">&lt;-</span> </span>
<span id="cb12-31"><a href="#cb12-31"></a>    <span class="fu">workflow</span>() <span class="sc">|&gt;</span> </span>
<span id="cb12-32"><a href="#cb12-32"></a>    <span class="fu">add_model</span>(modelo_knn) <span class="sc">|&gt;</span> </span>
<span id="cb12-33"><a href="#cb12-33"></a>    <span class="fu">add_recipe</span>(receita) <span class="sc">|&gt;</span> </span>
<span id="cb12-34"><a href="#cb12-34"></a>    <span class="fu">fit</span>(<span class="at">data =</span> dados)</span>
<span id="cb12-35"><a href="#cb12-35"></a>  </span>
<span id="cb12-36"><a href="#cb12-36"></a>  <span class="co"># Retornando previsoes</span></span>
<span id="cb12-37"><a href="#cb12-37"></a>  y_chapeu <span class="ot">&lt;-</span> <span class="fu">predict</span>(ajuste_final, <span class="at">new_data =</span> dados)</span>
<span id="cb12-38"><a href="#cb12-38"></a>  </span>
<span id="cb12-39"><a href="#cb12-39"></a>  dados <span class="ot">&lt;-</span> </span>
<span id="cb12-40"><a href="#cb12-40"></a>    dados <span class="sc">|&gt;</span> </span>
<span id="cb12-41"><a href="#cb12-41"></a>    <span class="fu">mutate</span>(<span class="at">y_chapeu =</span> y_chapeu<span class="sc">$</span>.pred)</span>
<span id="cb12-42"><a href="#cb12-42"></a>  </span>
<span id="cb12-43"><a href="#cb12-43"></a>  dados <span class="sc">|&gt;</span> </span>
<span id="cb12-44"><a href="#cb12-44"></a>    <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb12-45"><a href="#cb12-45"></a>    <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb12-46"><a href="#cb12-46"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y_chapeu), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb12-47"><a href="#cb12-47"></a>    <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"k-nearest neighbours"</span>, <span class="at">subtitle =</span> <span class="fu">glue</span>(<span class="st">"k = {k}"</span>)) <span class="sc">+</span></span>
<span id="cb12-48"><a href="#cb12-48"></a>    <span class="fu">theme</span>(</span>
<span id="cb12-49"><a href="#cb12-49"></a>      <span class="at">title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb12-50"><a href="#cb12-50"></a>    )</span>
<span id="cb12-51"><a href="#cb12-51"></a>}</span>
<span id="cb12-52"><a href="#cb12-52"></a></span>
<span id="cb12-53"><a href="#cb12-53"></a>p1 <span class="ot">&lt;-</span> <span class="fu">knn_exp_pip</span>(dados, <span class="at">k =</span> 1L)</span>
<span id="cb12-54"><a href="#cb12-54"></a>p2 <span class="ot">&lt;-</span> <span class="fu">knn_exp_pip</span>(dados, <span class="at">k =</span> 7L)</span>
<span id="cb12-55"><a href="#cb12-55"></a>p3 <span class="ot">&lt;-</span> <span class="fu">knn_exp_pip</span>(dados, <span class="at">k =</span> 10L)</span>
<span id="cb12-56"><a href="#cb12-56"></a>p4 <span class="ot">&lt;-</span> <span class="fu">knn_exp_pip</span>(dados, <span class="at">k =</span> 200L)</span>
<span id="cb12-57"><a href="#cb12-57"></a></span>
<span id="cb12-58"><a href="#cb12-58"></a>p <span class="ot">&lt;-</span> p1 <span class="sc">+</span> p2 <span class="sc">+</span> p3 <span class="sc">+</span> p4 <span class="sc">+</span> <span class="fu">plot_annotation</span>(<span class="at">tag_levels =</span> <span class="st">"A"</span>)</span>
<span id="cb12-59"><a href="#cb12-59"></a></span>
<span id="cb12-60"><a href="#cb12-60"></a><span class="fu">ggsave</span>(p, <span class="at">file =</span> <span class="st">"imgs/knn_plot.png"</span>, <span class="at">width =</span> <span class="dv">50</span>, <span class="at">height =</span> <span class="dv">30</span>, <span class="at">units =</span> <span class="st">"cm"</span>)</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><br></p>

<img data-src="gifs/chapulin-colorado-no.gif" style="width:25.0%" class="r-stretch"></section>
<section id="k-vizinhos-mais-pr√≥ximo-pr√≥ximos-8" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais pr√≥ximo pr√≥ximos</h2>
<p><br></p>
<p><strong>Algumas limita√ß√µes do</strong> <span class="math inline">k</span>NN s√£o:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>√â totalmente dependente do conjunto de dados para fazer novas predi√ß√µes;</p></li>
<li class="fragment"><p>Para novas observa√ß√µes que s√£o fora dos limites dos dados de treinamento, as predi√ß√µes do <span class="math inline">k</span>NN tendem a serem imprecisas;</p></li>
<li class="fragment"><p>Pode ser custoso para uma grande base de dados;</p></li>
<li class="fragment"><p>O c√°lculo de dist√¢ncias pode sofrer com a chamada ‚Äúmaldi√ß√£o da dimensionalidade‚Äù, em que a depender da m√©trica de dist√¢ncia utilizada, tudo fica muito distante.</p></li>
</ol>
<p><br></p>
<p><strong>Algumas vantagens s√£o</strong>:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>√â um m√©todo simples de ser implementado;</p></li>
<li class="fragment"><p>√â muito utilizado para imputa√ß√£o de observa√ß√µes faltantes;</p></li>
<li class="fragment"><p>√â comumente utilizado, por conta de sua simplicidade, em explora√ß√µes iniciais.</p></li>
</ol>
</section>
<section id="exerc√≠cios-5" class="slide level2">
<h2>üìö Exerc√≠cios</h2>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Considere o problema em que o objetivo √© prever a <strong>pontua√ß√£o</strong> (<em>score</em>) / (item 2) do aluno com base em algumas vari√°veis. S√£o elas:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Horas estudadas</strong>: o n√∫mero total de horas gastas estudando por cada aluno;</li>
<li class="fragment"><strong>Pontua√ß√£o</strong>: As notas obtidas pelos alunos em testes anteriores;</li>
<li class="fragment"><strong>Atividades extracurriculares</strong>: Se o aluno participa de atividades extracurriculares (Sim ou N√£o);</li>
<li class="fragment"><strong>Horas de sono</strong>: o n√∫mero m√©dio de horas de sono que o aluno teve por dia;</li>
<li class="fragment"><strong>Amostras de perguntas praticadas</strong>: O n√∫mero de amostras de perguntas que o aluno praticou.</li>
</ol>
<p><br></p>
<p>Voc√™ poder√° baixar e ter uma descri√ß√£o maior da base de dados clicando <a href="https://www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression">aqui</a>. Avalie o poder preditivo do modelo lasso com o do <span class="math inline">k</span>NN. Sua an√°lise deve conter uma an√°lise explorat√≥ria dos dados, dever√° utilizar um esquema de <span class="math inline">k</span>-<em>folds cross-validation</em>, com <span class="math inline">k = 20</span> e considerar um esquema de <em>data splitting</em> na propor√ß√£o <span class="math inline">90\%</span> para treino e <span class="math inline">10\%</span> para teste. Sua an√°lise dever√° estar em um notebook de <a href="https://quarto.org/">quarto</a>, em que voc√™ dever√° comentar cada passo da an√°lise.</p>
</section>
<section id="nadaraya-watson" class="slide level2">
<h2>Nadaraya-Watson</h2>
<p><br></p>
<p>Uma varia√ß√£o do m√©todo <span class="math inline">k</span>NN que √© bastante difundida na comunidade estat√≠stica √© o m√©todo de Nadaraya-Watson, propostos nos artigos:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>Nadaraya, E. A. (1964). <strong>On estimating regression</strong>. Theory of Probability &amp; Its Applications, 9(1), 141‚Äì142;</p></li>
<li class="fragment"><p>Watson, G. S. (1964). <strong>Smooth regression analysis</strong>. Sankhya: The Indian Journal of Statistics, Series A, 359‚Äì372.</p></li>
</ol>
<p><br></p>
<p>Esse m√©todo tamb√©m √© chamado de estimador <span class="math inline">k</span>-vizinhos ponderados (<span class="math inline">k</span>NN ponderado), uma vez que a estima√ß√£o da fun√ß√£o de regress√£o em um dado ponto <span class="math inline">{\bf x}</span> utiliza de m√©dias ponderadas das observa√ß√µes do conjunto de treinamento:</p>
<p><span class="math display">g({\bf x}) = \sum_{i = 1}^n w_i({\bf x})y_i,</span> em que <span class="math inline">w_i({\bf x})</span> √© o peso atribu√≠do √† <span class="math inline">i</span>-√©sima observa√ß√£o, medindo a similaridade de <span class="math inline">{\bf x}_i</span> √† <span class="math inline">{\bf x}</span>.</p>
</section>
<section id="nadaraya-watson-1" class="slide level2">
<h2>Nadaraya-Watson</h2>
<p><br></p>
<p>Temos que <span class="math inline">w_i({\bf x})</span> √© definido por:</p>
<p><span class="math display">w_i({\bf x}) = \frac{K({\bf x}, {\bf x}_i)}{\sum_{j = 1}^n K({\bf x}, {\bf x}_j)},</span> em que <span class="math inline">K({\bf x}, {\bf x}_j)</span> √© um kernel de suaviza√ß√£o usado para medir a similaridade entre as observa√ß√µes. Escolhas que s√£o populares para <span class="math inline">K({\bf x}, {\bf x}_j)</span>, s√£o:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><span class="red">Kernel uniforme</span>: <span class="math inline">K({\bf x}, {\bf x}_i) = \mathbb{I}(d({\bf x}, {\bf x}_i) \leq h)</span>;</li>
<li class="fragment"><span class="red">Kernel gaussiano</span>: <span class="math inline">K({\bf x}, {\bf x}_i) = (\sqrt{2\pi h^2})^{-1}\exp\left\{ -\frac{d^2({\bf x}, {\bf x}_i)}{2h^2}\right\}</span>;</li>
<li class="fragment"><span class="red">Kernel triangular</span>: <span class="math inline">K({\bf x}, {\bf x}_i) = (1 - d({\bf x}, {\bf x}_i)/h)\mathbb{I}(d({\bf x}, {\bf x}_i) \leq h)</span>;</li>
<li class="fragment"><span class="red">Kernel de Epanechnikov</span>: <span class="math inline">K({\bf x}, {\bf x}_i) = (1 - d^2({\bf x}, {\bf x}_i)/h^2)\mathbb{I}(d({\bf x}, {\bf x}_i) \leq h)</span>.</li>
</ol>
</section>
<section id="nadaraya-watson-2" class="slide level2">
<h2>Nadaraya-Watson</h2>
<p><br></p>
<p>Enquanto no kernel uniforme, os pesos s√£o iguais para as observa√ß√µes a uma dist√¢ncia menor que <span class="math inline">h</span> de <span class="math inline">{\bf x}</span> e atribui peso zero para observa√ß√µes maiores que <span class="math inline">h</span>, os kernels triangular e de Epanechnikov atribui pesos maiores para observa√ß√µes mais pr√≥ximas de <span class="math inline">{\bf x}</span>. A quantidade <span class="math inline">h</span> √© um <span class="red"><em>tuning parameter</em></span>, e na pr√°tica, deve ser estimada por um procedimento de valida√ß√£o cruzada.</p>
<p><br></p>
<p><strong>Algumas propriedades de uma fun√ß√£o kernel s√£o</strong>:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Simetria</strong>: <span class="math inline">K(x,y) = K(y,x)</span>, permitindo que a fun√ß√£o de similaridade seja invariante em rela√ß√£o a ordem dos argumentos;</li>
<li class="fragment"><strong>Positiva definida</strong>: Para qualquer vetor <span class="math inline">c</span>, em que seja poss√≠vel fazer <span class="math inline">c^{T}K(x,y)</span>, temos que <span class="math inline">c^{T}K(x,y)c &gt; 0</span>.</li>
</ol>
<p><br></p>
<p>Voc√™ poder√° encontrar outras fun√ß√µes kernel <a href="https://en.wikipedia.org/wiki/Kernel_(statistics)">aqui</a>.</p>
</section>
<section id="nadaraya-watson-3" class="slide level2">
<h2>Nadaraya-Watson</h2>
<p><br></p>
<p><span class="red">Exemplo</span>: Novamente, considerando o conjunto de dados de expectativa de vida versus PIB per Capita dispon√≠veis <a href="https://github.com/prdm0/dados/blob/main/dados_expectativa_renda.RData">aqui</a>. Utilizando o <a href="https://www.tidymodels.org/">tidymodels</a>, vamos construir uma fu√ß√£o que retorne um gr√°fico com as estimativas. A fun√ß√£o receber√° como argumentos o conjunto de dados e o valor de <span class="math inline">h</span>. Observe a documenta√ß√£o da fun√ß√£o <code>nearest_neighbor</code> do pacote <a href="https://parsnip.tidymodels.org/">parsnip</a>. Perceba que o argumento <code>weight_func</code> permite que possamos escolher entre algumas fun√ß√µes kernel. Por√©m, como m√©trica de dist√¢ncias, apenas poderemos utilizar a de Minkowski. Seja <span class="math inline">x = (x_1, \cdots, x_n)</span> e <span class="math inline">y = (y_1, \cdots, y_n)</span>, ambos vetores do <span class="math inline">\mathbb{R}^n</span>. Ent√£o, a dist√¢ncia de Minkowski √© dada por:</p>
<p><span class="math display">d(x,y) = \left(\sum_{i = 1}^n |x_i - y_i|^p\right)^{\frac{1}{p}},</span> com <span class="math inline">p \geq 1</span>. Note que se <span class="math inline">p = 1</span> temos a dist√¢ncia euclidiana (dist√¢ncia <span class="math inline">L_1</span>) e se <span class="math inline">p = 2</span> teremos a distancia de Manhattan (dist√¢ncia <span class="math inline">L_2</span>).</p>
</section>
<section id="nadaraya-watson-4" class="slide level2">
<h2>Nadaraya-Watson</h2>
<p><br></p>
<p>Portanto, com o argumento <code>dist_power</code> da fun√ß√£o <code>nearest_neighbor</code>, do pacote <a href="https://parsnip.tidymodels.org/">parsnip</a>, voc√™ poder√° especificar o valor de <span class="math inline">p</span>, que inclusive poder√° ser um hiperpar√¢metro, podendo ser obtido por meio de uma valida√ß√£o cruzada.</p>
<p><br></p>

<img data-src="gifs/mr-bean-pivot.gif" class="r-stretch"><p><br></p>
<p>No exemplo n√£o iremos fazer a valida√ß√£o cruzada, pois apenas queremos implementar uma fun√ß√£o em que seja poss√≠vel experimentar o m√©todo para diferentes valores de <span class="math inline">h</span> e diferentes fun√ß√µes de kernel.</p>
</section>
<section id="nadaraya-watson-5" class="slide level2">
<h2>Nadaraya-Watson</h2>
<p><br> A imagem abaixo utiliza o kernel gaussiano:</p>
<p><br></p>

<img data-src="imgs/nadaraya_watson.png" class="r-stretch quarto-figure-center"></section>
<section id="nadaraya-watson-6" class="slide level2">
<h2>Nadaraya-Watson</h2>
<p><br></p>
<div class="cell">
<details>
<summary>Solu√ß√£o do exempƒ∫o de Nadaraya-Watson</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="fu">library</span>(glue)</span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb13-6"><a href="#cb13-6"></a></span>
<span id="cb13-7"><a href="#cb13-7"></a>tidymodels<span class="sc">::</span><span class="fu">tidymodels_prefer</span>()</span>
<span id="cb13-8"><a href="#cb13-8"></a></span>
<span id="cb13-9"><a href="#cb13-9"></a></span>
<span id="cb13-10"><a href="#cb13-10"></a><span class="co"># Lendo dados</span></span>
<span id="cb13-11"><a href="#cb13-11"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb13-12"><a href="#cb13-12"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb13-13"><a href="#cb13-13"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb13-14"><a href="#cb13-14"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb13-15"><a href="#cb13-15"></a></span>
<span id="cb13-16"><a href="#cb13-16"></a>dados <span class="ot">&lt;-</span> </span>
<span id="cb13-17"><a href="#cb13-17"></a>  dados_expectativa_renda <span class="sc">|&gt;</span> </span>
<span id="cb13-18"><a href="#cb13-18"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>CountryName) <span class="sc">|&gt;</span> </span>
<span id="cb13-19"><a href="#cb13-19"></a>  dplyr<span class="sc">::</span><span class="fu">rename</span>(<span class="at">y =</span> LifeExpectancy, <span class="at">x =</span> GDPercapita)</span>
<span id="cb13-20"><a href="#cb13-20"></a></span>
<span id="cb13-21"><a href="#cb13-21"></a>nadaraya_watson_exp_pip <span class="ot">&lt;-</span> <span class="cf">function</span>(dados, <span class="at">h =</span> <span class="dv">1</span>, ...){</span>
<span id="cb13-22"><a href="#cb13-22"></a>  <span class="co"># Criando receita</span></span>
<span id="cb13-23"><a href="#cb13-23"></a>  receita <span class="ot">&lt;-</span> </span>
<span id="cb13-24"><a href="#cb13-24"></a>    <span class="fu">recipe</span>(y <span class="sc">~</span> x, <span class="at">data =</span> dados) <span class="sc">|&gt;</span> </span>
<span id="cb13-25"><a href="#cb13-25"></a>    <span class="fu">step_normalize</span>()</span>
<span id="cb13-26"><a href="#cb13-26"></a>  </span>
<span id="cb13-27"><a href="#cb13-27"></a>  <span class="co"># Definindo o modelo</span></span>
<span id="cb13-28"><a href="#cb13-28"></a>  modelo_knn <span class="ot">&lt;-</span> <span class="fu">nearest_neighbor</span>(<span class="at">dist_power =</span> h, ...) <span class="sc">|&gt;</span> </span>
<span id="cb13-29"><a href="#cb13-29"></a>    <span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb13-30"><a href="#cb13-30"></a>    <span class="fu">set_engine</span>(<span class="st">"kknn"</span>)</span>
<span id="cb13-31"><a href="#cb13-31"></a>  </span>
<span id="cb13-32"><a href="#cb13-32"></a>  <span class="co"># Workflow</span></span>
<span id="cb13-33"><a href="#cb13-33"></a>  ajuste_final <span class="ot">&lt;-</span> </span>
<span id="cb13-34"><a href="#cb13-34"></a>    <span class="fu">workflow</span>() <span class="sc">|&gt;</span> </span>
<span id="cb13-35"><a href="#cb13-35"></a>    <span class="fu">add_model</span>(modelo_knn) <span class="sc">|&gt;</span> </span>
<span id="cb13-36"><a href="#cb13-36"></a>    <span class="fu">add_recipe</span>(receita) <span class="sc">|&gt;</span> </span>
<span id="cb13-37"><a href="#cb13-37"></a>    <span class="fu">fit</span>(<span class="at">data =</span> dados)</span>
<span id="cb13-38"><a href="#cb13-38"></a>  </span>
<span id="cb13-39"><a href="#cb13-39"></a>  <span class="co"># Retornando previsoes</span></span>
<span id="cb13-40"><a href="#cb13-40"></a>  y_chapeu <span class="ot">&lt;-</span> <span class="fu">predict</span>(ajuste_final, <span class="at">new_data =</span> dados)</span>
<span id="cb13-41"><a href="#cb13-41"></a>  </span>
<span id="cb13-42"><a href="#cb13-42"></a>  dados <span class="ot">&lt;-</span> </span>
<span id="cb13-43"><a href="#cb13-43"></a>    dados <span class="sc">|&gt;</span> </span>
<span id="cb13-44"><a href="#cb13-44"></a>    <span class="fu">mutate</span>(<span class="at">y_chapeu =</span> y_chapeu<span class="sc">$</span>.pred)</span>
<span id="cb13-45"><a href="#cb13-45"></a>  </span>
<span id="cb13-46"><a href="#cb13-46"></a>  dados <span class="sc">|&gt;</span> </span>
<span id="cb13-47"><a href="#cb13-47"></a>    <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb13-48"><a href="#cb13-48"></a>    <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb13-49"><a href="#cb13-49"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y_chapeu), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb13-50"><a href="#cb13-50"></a>    <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Nadaraya-Watson"</span>, <span class="at">subtitle =</span> <span class="fu">glue</span>(<span class="st">"h = {h}"</span>)) <span class="sc">+</span></span>
<span id="cb13-51"><a href="#cb13-51"></a>    <span class="fu">theme</span>(</span>
<span id="cb13-52"><a href="#cb13-52"></a>      <span class="at">title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb13-53"><a href="#cb13-53"></a>    )</span>
<span id="cb13-54"><a href="#cb13-54"></a>}</span>
<span id="cb13-55"><a href="#cb13-55"></a></span>
<span id="cb13-56"><a href="#cb13-56"></a>p1 <span class="ot">&lt;-</span> <span class="fu">nadaraya_watson_exp_pip</span>(dados, <span class="at">h =</span> <span class="dv">1</span>, <span class="at">weight_func =</span> <span class="st">"gaussian"</span>)</span>
<span id="cb13-57"><a href="#cb13-57"></a>p2 <span class="ot">&lt;-</span> <span class="fu">nadaraya_watson_exp_pip</span>(dados, <span class="at">h =</span> <span class="dv">1000</span>, <span class="at">weight_func =</span> <span class="st">"gaussian"</span>)</span>
<span id="cb13-58"><a href="#cb13-58"></a></span>
<span id="cb13-59"><a href="#cb13-59"></a>p <span class="ot">&lt;-</span> p1 <span class="sc">+</span> p2 <span class="sc">+</span> <span class="fu">plot_annotation</span>(<span class="at">tag_levels =</span> <span class="st">"A"</span>)</span>
<span id="cb13-60"><a href="#cb13-60"></a></span>
<span id="cb13-61"><a href="#cb13-61"></a><span class="fu">ggsave</span>(p, <span class="at">file =</span> <span class="st">"imgs/nadaraya_watson.png"</span>, <span class="at">width =</span> <span class="dv">30</span>, <span class="at">height =</span> <span class="dv">15</span>, <span class="at">units =</span> <span class="st">"cm"</span>)</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><br></p>

<img data-src="gifs/side-eyeing-chloe-chloe.gif" class="r-stretch"></section>
<section id="tidymodels-knn-e-nadaraya-watson" class="slide level2">
<h2>Tidymodels <span class="math inline">k</span>NN e Nadaraya-Watson</h2>
<p><br></p>
<p>Para que possamos experimentar o <a href="https://www.tidymodels.org/">tidymodels</a> seguindo todo o fluxo de ‚Äúpadr√£o‚Äù de aprendizagem de m√°quina, com divis√£o do conjunto de dados, valida√ß√£o cruzada e busca pelos melhores hiperpar√¢metros (‚Äútunagem‚Äù). Iremos reproduir dois exemplos com os dados de expectativa de vida e PIB per Capita.</p>
<p><br></p>
<p><span class="red">Exemplo</span>: Nesse exemplo estamos realizando o <span class="math inline">k</span>NN, em que estamos obtendo um candidato para o valor de <span class="math inline">k</span>, por meio de um procedimento de <em>cross-validation</em> (10-<em>folds cross-validation</em>). Aqui, a busca do hiperpar√¢metro <span class="math inline">k</span> far√° uso de um <em>grid search</em>. Perceba, no c√≥digo que segue o exemplo, que no processo de ‚Äútunagem‚Äù, alterei o grid de valores usando a fun√ß√£o <code>grid_max_entropy</code> do pacote <a href="https://dials.tidymodels.org/reference/grid_max_entropy.html">dails</a>. Uma observa√ß√£o √© que voc√™ poderia criar uma tibble com valores do seu interesse. O argumento <code>grid</code> da fun√ß√£o <code>tune_grid</code> do pacote <a href="https://tune.tidymodels.org/reference/tune_grid.html">tune</a> deve ser um data frame/tibble ou um n√∫mero inteiro. Esse objeto conter√° todas as poss√≠veis combina√ß√µes de hiperpar√¢metros que ser√£o testadas na valida√ß√£o cruzada, no nosso caso, temos apenas um hiperpar√¢metro. Foi utilizado um <em>grid</em> de tamanho 60. Foi utilizado uma divis√£o de <span class="math inline">80\%</span> para treinamento e <span class="math inline">20\%</span> para teste.</p>
</section>
<section id="tidymodels-knn-e-nadaraya-watson-1" class="slide level2">
<h2>Tidymodels <span class="math inline">k</span>NN e Nadaraya-Watson</h2>
<p><br></p>
<div class="cell">
<details>
<summary>Workflow completo do treimanento de um modelo KNN.</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb14-3"><a href="#cb14-3"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb14-4"><a href="#cb14-4"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb14-5"><a href="#cb14-5"></a></span>
<span id="cb14-6"><a href="#cb14-6"></a><span class="co"># Resolvendo eventuais conflitos entre tidymodels e outros pacotes eventualmente</span></span>
<span id="cb14-7"><a href="#cb14-7"></a><span class="co"># carregados:</span></span>
<span id="cb14-8"><a href="#cb14-8"></a>tidymodels<span class="sc">::</span><span class="fu">tidymodels_prefer</span>()</span>
<span id="cb14-9"><a href="#cb14-9"></a></span>
<span id="cb14-10"><a href="#cb14-10"></a><span class="co"># Lendo a base de dados:</span></span>
<span id="cb14-11"><a href="#cb14-11"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb14-12"><a href="#cb14-12"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb14-13"><a href="#cb14-13"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb14-14"><a href="#cb14-14"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb14-15"><a href="#cb14-15"></a></span>
<span id="cb14-16"><a href="#cb14-16"></a>dados_expectativa_renda <span class="ot">&lt;-</span></span>
<span id="cb14-17"><a href="#cb14-17"></a>  dados_expectativa_renda <span class="sc">|&gt;</span></span>
<span id="cb14-18"><a href="#cb14-18"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>CountryName)</span>
<span id="cb14-19"><a href="#cb14-19"></a></span>
<span id="cb14-20"><a href="#cb14-20"></a><span class="co"># Setando uma semente</span></span>
<span id="cb14-21"><a href="#cb14-21"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb14-22"><a href="#cb14-22"></a></span>
<span id="cb14-23"><a href="#cb14-23"></a><span class="co"># Divis√£o da base de dados ------------------------------------------------</span></span>
<span id="cb14-24"><a href="#cb14-24"></a><span class="co"># Divis√£o inicial (treino e teste)</span></span>
<span id="cb14-25"><a href="#cb14-25"></a>splits <span class="ot">&lt;-</span> </span>
<span id="cb14-26"><a href="#cb14-26"></a>  rsample<span class="sc">::</span><span class="fu">initial_split</span>(</span>
<span id="cb14-27"><a href="#cb14-27"></a>    dados_expectativa_renda,</span>
<span id="cb14-28"><a href="#cb14-28"></a>    <span class="at">strata =</span> <span class="st">"LifeExpectancy"</span>, </span>
<span id="cb14-29"><a href="#cb14-29"></a>    <span class="at">prop =</span> <span class="fl">0.8</span> </span>
<span id="cb14-30"><a href="#cb14-30"></a>  )</span>
<span id="cb14-31"><a href="#cb14-31"></a></span>
<span id="cb14-32"><a href="#cb14-32"></a><span class="co"># O conjunto de dados de treinamento ser√° utilizado para ajustar/treinar o</span></span>
<span id="cb14-33"><a href="#cb14-33"></a><span class="co"># modelo:</span></span>
<span id="cb14-34"><a href="#cb14-34"></a>treinamento <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(splits)</span>
<span id="cb14-35"><a href="#cb14-35"></a></span>
<span id="cb14-36"><a href="#cb14-36"></a><span class="co"># O conjunto de teste ser√° utilizado apenas no fim, para avaliar o desempenho</span></span>
<span id="cb14-37"><a href="#cb14-37"></a><span class="co"># preditivo final do modelo:</span></span>
<span id="cb14-38"><a href="#cb14-38"></a>teste <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">testing</span>(splits) </span>
<span id="cb14-39"><a href="#cb14-39"></a></span>
<span id="cb14-40"><a href="#cb14-40"></a><span class="co"># Criando uma receita para os dados ---------------------------------------</span></span>
<span id="cb14-41"><a href="#cb14-41"></a></span>
<span id="cb14-42"><a href="#cb14-42"></a><span class="co"># Poderia ter passado o conjunto treinamento ou posso passar o conjunto "splits".</span></span>
<span id="cb14-43"><a href="#cb14-43"></a><span class="co"># O comando recipes j√° entende que dever√° utilizar o conjunto de treinamento.</span></span>
<span id="cb14-44"><a href="#cb14-44"></a>receita <span class="ot">&lt;-</span> </span>
<span id="cb14-45"><a href="#cb14-45"></a>  recipes<span class="sc">::</span><span class="fu">recipe</span>(<span class="at">formula =</span> LifeExpectancy <span class="sc">~</span> ., <span class="at">data =</span> treinamento) <span class="sc">|&gt;</span> </span>
<span id="cb14-46"><a href="#cb14-46"></a>  <span class="fu">step_YeoJohnson</span>(<span class="fu">all_predictors</span>()) <span class="sc">|&gt;</span> <span class="co"># Ajuda na normaliza√ß√£o dos dados. Pode ser bom!</span></span>
<span id="cb14-47"><a href="#cb14-47"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>()) <span class="co"># Normalizando vari√°veis num√©ricas.</span></span>
<span id="cb14-48"><a href="#cb14-48"></a></span>
<span id="cb14-49"><a href="#cb14-49"></a><span class="co"># Construindo modelo kNN --------------------------------------------------</span></span>
<span id="cb14-50"><a href="#cb14-50"></a>modelo_knn <span class="ot">&lt;-</span> </span>
<span id="cb14-51"><a href="#cb14-51"></a>  parsnip<span class="sc">::</span><span class="fu">nearest_neighbor</span>(<span class="at">neighbors =</span> <span class="fu">tune</span>()) <span class="sc">|&gt;</span> </span>
<span id="cb14-52"><a href="#cb14-52"></a>  parsnip<span class="sc">::</span><span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb14-53"><a href="#cb14-53"></a>  parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"kknn"</span>)</span>
<span id="cb14-54"><a href="#cb14-54"></a></span>
<span id="cb14-55"><a href="#cb14-55"></a><span class="co"># Construindo um workflow (pipeline) --------------------------------------</span></span>
<span id="cb14-56"><a href="#cb14-56"></a>wf_knn <span class="ot">&lt;-</span></span>
<span id="cb14-57"><a href="#cb14-57"></a>  workflows<span class="sc">::</span><span class="fu">workflow</span>() <span class="sc">|&gt;</span></span>
<span id="cb14-58"><a href="#cb14-58"></a>  workflows<span class="sc">::</span><span class="fu">add_recipe</span>(receita) <span class="sc">|&gt;</span></span>
<span id="cb14-59"><a href="#cb14-59"></a>  workflows<span class="sc">::</span><span class="fu">add_model</span>(modelo_knn)</span>
<span id="cb14-60"><a href="#cb14-60"></a></span>
<span id="cb14-61"><a href="#cb14-61"></a><span class="co"># Cross-validation --------------------------------------------------------</span></span>
<span id="cb14-62"><a href="#cb14-62"></a>cv <span class="ot">&lt;-</span> </span>
<span id="cb14-63"><a href="#cb14-63"></a>  treinamento <span class="sc">|&gt;</span> </span>
<span id="cb14-64"><a href="#cb14-64"></a>  rsample<span class="sc">::</span><span class="fu">vfold_cv</span>(<span class="at">v =</span> 10L, <span class="at">strata =</span> <span class="st">"LifeExpectancy"</span>)</span>
<span id="cb14-65"><a href="#cb14-65"></a></span>
<span id="cb14-66"><a href="#cb14-66"></a><span class="co"># Busca do hiperpar√¢metro k -----------------------------------------------</span></span>
<span id="cb14-67"><a href="#cb14-67"></a>metrica <span class="ot">&lt;-</span> <span class="fu">metric_set</span>(rmse)</span>
<span id="cb14-68"><a href="#cb14-68"></a></span>
<span id="cb14-69"><a href="#cb14-69"></a><span class="co"># Extraindo e atualizando range do par√¢metro ------------------------------</span></span>
<span id="cb14-70"><a href="#cb14-70"></a>update_parametros <span class="ot">&lt;-</span></span>
<span id="cb14-71"><a href="#cb14-71"></a>  wf_knn <span class="sc">|&gt;</span> </span>
<span id="cb14-72"><a href="#cb14-72"></a>  <span class="fu">extract_parameter_set_dials</span>() <span class="sc">|&gt;</span></span>
<span id="cb14-73"><a href="#cb14-73"></a>  <span class="fu">update</span>(<span class="st">"neighbors"</span> <span class="ot">=</span> <span class="fu">neighbors</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">50</span>)))</span>
<span id="cb14-74"><a href="#cb14-74"></a></span>
<span id="cb14-75"><a href="#cb14-75"></a><span class="co"># Tunagem -----------------------------------------------------------------</span></span>
<span id="cb14-76"><a href="#cb14-76"></a>meu_grid <span class="ot">&lt;-</span> dials<span class="sc">::</span><span class="fu">grid_max_entropy</span>(update_parametros, <span class="at">size =</span> <span class="dv">60</span>)</span>
<span id="cb14-77"><a href="#cb14-77"></a></span>
<span id="cb14-78"><a href="#cb14-78"></a>tunagem <span class="ot">&lt;-</span></span>
<span id="cb14-79"><a href="#cb14-79"></a>  tune<span class="sc">::</span><span class="fu">tune_grid</span>(</span>
<span id="cb14-80"><a href="#cb14-80"></a>    wf_knn,</span>
<span id="cb14-81"><a href="#cb14-81"></a>    <span class="at">resamples =</span> cv,</span>
<span id="cb14-82"><a href="#cb14-82"></a>    <span class="at">grid =</span> meu_grid,</span>
<span id="cb14-83"><a href="#cb14-83"></a>    <span class="at">metrics =</span> metrica,</span>
<span id="cb14-84"><a href="#cb14-84"></a>    <span class="at">control =</span> <span class="fu">control_grid</span>(<span class="at">save_pred =</span> <span class="cn">TRUE</span>, <span class="at">verbose =</span> <span class="cn">TRUE</span>)</span>
<span id="cb14-85"><a href="#cb14-85"></a>  )</span>
<span id="cb14-86"><a href="#cb14-86"></a></span>
<span id="cb14-87"><a href="#cb14-87"></a>p_hiper <span class="ot">&lt;-</span> <span class="fu">autoplot</span>(tunagem) <span class="sc">+</span></span>
<span id="cb14-88"><a href="#cb14-88"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"KNN - Sele√ß√£o do n√∫mero k (vizinhos)"</span>, <span class="at">subtitle =</span> <span class="st">"Sintoniza√ß√£o do hiperpar√¢metro (valor de k)"</span>) <span class="sc">+</span></span>
<span id="cb14-89"><a href="#cb14-89"></a>  <span class="fu">theme</span>(</span>
<span id="cb14-90"><a href="#cb14-90"></a>    <span class="at">title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb14-91"><a href="#cb14-91"></a>  )</span>
<span id="cb14-92"><a href="#cb14-92"></a></span>
<span id="cb14-93"><a href="#cb14-93"></a><span class="co"># Atualizando workflow ----------------------------------------------------</span></span>
<span id="cb14-94"><a href="#cb14-94"></a>wf_knn <span class="ot">&lt;-</span> </span>
<span id="cb14-95"><a href="#cb14-95"></a>  wf_knn <span class="sc">|&gt;</span> </span>
<span id="cb14-96"><a href="#cb14-96"></a>  <span class="fu">finalize_workflow</span>(<span class="fu">select_best</span>(tunagem))</span>
<span id="cb14-97"><a href="#cb14-97"></a></span>
<span id="cb14-98"><a href="#cb14-98"></a><span class="co"># Ajustar o modelo ao conjunto de treinamento e avaliar no teste --------</span></span>
<span id="cb14-99"><a href="#cb14-99"></a>ajuste_final <span class="ot">&lt;-</span> <span class="fu">last_fit</span>(wf_knn, splits)</span>
<span id="cb14-100"><a href="#cb14-100"></a></span>
<span id="cb14-101"><a href="#cb14-101"></a><span class="co"># Ajuste final com toda a base de dados -----------------------------------</span></span>
<span id="cb14-102"><a href="#cb14-102"></a>modelo_final <span class="ot">&lt;-</span> <span class="fu">fit</span>(wf_knn, <span class="at">data =</span> dados_expectativa_renda)</span>
<span id="cb14-103"><a href="#cb14-103"></a></span>
<span id="cb14-104"><a href="#cb14-104"></a><span class="co"># Visualizando as predi√ß√µes na base de treino</span></span>
<span id="cb14-105"><a href="#cb14-105"></a>p_ajuste <span class="ot">&lt;-</span> ajuste_final<span class="sc">$</span>.predictions[[1L]] <span class="sc">|&gt;</span> </span>
<span id="cb14-106"><a href="#cb14-106"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> LifeExpectancy, <span class="at">y =</span> .pred)) <span class="sc">+</span> </span>
<span id="cb14-107"><a href="#cb14-107"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb14-108"><a href="#cb14-108"></a>  <span class="fu">labs</span>(</span>
<span id="cb14-109"><a href="#cb14-109"></a>    <span class="at">title =</span> <span class="st">"Predi√ß√µes versus Real"</span>, </span>
<span id="cb14-110"><a href="#cb14-110"></a>    <span class="at">subtitle =</span> <span class="st">"Usando apenas os dados de teste"</span></span>
<span id="cb14-111"><a href="#cb14-111"></a>  ) <span class="sc">+</span></span>
<span id="cb14-112"><a href="#cb14-112"></a>  <span class="fu">xlab</span>(<span class="st">"LifeExpectancy"</span>) <span class="sc">+</span> </span>
<span id="cb14-113"><a href="#cb14-113"></a>  <span class="fu">xlab</span>(<span class="st">"LifeExpectancy predito"</span>) <span class="sc">+</span> </span>
<span id="cb14-114"><a href="#cb14-114"></a>  <span class="fu">theme</span>(</span>
<span id="cb14-115"><a href="#cb14-115"></a>    <span class="at">title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb14-116"><a href="#cb14-116"></a>  )</span>
<span id="cb14-117"><a href="#cb14-117"></a></span>
<span id="cb14-118"><a href="#cb14-118"></a><span class="co"># Unindo os dois plots</span></span>
<span id="cb14-119"><a href="#cb14-119"></a>p <span class="ot">&lt;-</span> p_hiper <span class="sc">+</span> p_ajuste <span class="sc">+</span> <span class="fu">plot_annotation</span>(<span class="at">tag_levels =</span> <span class="st">"A"</span>)</span>
<span id="cb14-120"><a href="#cb14-120"></a></span>
<span id="cb14-121"><a href="#cb14-121"></a><span class="co"># Salvando gr√°ficos</span></span>
<span id="cb14-122"><a href="#cb14-122"></a><span class="fu">ggsave</span>(p, <span class="at">file =</span> <span class="st">"imgs/plot_hiper_ajuste_tidymodels_knn_whatson.png"</span>, <span class="at">width =</span> <span class="dv">30</span>,</span>
<span id="cb14-123"><a href="#cb14-123"></a>       <span class="at">height =</span> <span class="dv">15</span>, <span class="at">units =</span> <span class="st">"cm"</span>)</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>O pacote <a href="https://dials.tidymodels.org/reference/grid_max_entropy.html">dials</a> fornece tr√™s fun√ß√µes o <em>grid</em> de par√¢metros. S√£o apenas fun√ß√µes que criam <em>grids</em> para os hiperpar√¢metros, segundo algumas metodologias, mas que na pr√°tica n√£o h√° garantias de qual ir√° funcionar melhor. A ideia √© experimentar e, por meio de uma valida√ß√£o cruzada, decidir por qual utilizar. Normalmente, a <code>grid_max_entropy</code>, utilizada no c√≥digo acima, funciona bem.</p>
<div class="columns">
<div class="column" style="width:50%;">
<ol type="1">
<li class="fragment"><a href="https://dials.tidymodels.org/reference/grid_max_entropy.html"><code>grid_max_entropy</code></a></li>
<li class="fragment"><a href="https://dials.tidymodels.org/reference/grid_max_entropy.html"><code>grid_latin_hypercube</code></a></li>
<li class="fragment"><a href="https://dials.tidymodels.org/reference/grid_regular.html"><code>grid_regular</code></a></li>
<li class="fragment"><a href="https://dials.tidymodels.org/reference/grid_regular.html"><code>grid_random</code></a></li>
</ol>
</div><div class="column" style="width:50%;">
<p><img data-src="imgs/dails.png"></p>
</div>
</div>
<p><img data-src="gifs/thumbs-up-nod.gif"></p>
</section>
<section id="tidymodels-knn-e-nadaraya-watson-2" class="slide level2">
<h2>Tidymodels <span class="math inline">k</span>NN e Nadaraya-Watson</h2>
<p><br></p>
<p>Para demonstar que podemos realizar transforma√ß√µes (receitas) na base de dados, entre do processo de treinamento, foi realizado duas transforma√ß√µes na vari√°vel <code>GDPercapita</code>. A primeira foi a Yeo‚ÄìJohnson <em>transformation</em> e a segunda foi uma simples normaliza√ß√£o dos dados (subitrair da m√©dia e dividir pelo desvio padr√£o). A <a href="https://en.wikipedia.org/wiki/Power_transform#Yeo%E2%80%93Johnson_transformation">transforma√ß√£o de Yeo‚ÄìJohnson</a> √© uma transforma√ß√£o semelhante a de <a href="https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation">Box-Cox</a>. A transforma√ß√£o de <a href="https://en.wikipedia.org/wiki/Power_transform#Yeo%E2%80%93Johnson_transformation">Yeo‚ÄìJohnson</a> trata de situa√ß√µes que a transforma√ß√£o de <a href="https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation">Box-Cox</a> n√£o trata. Por exemplo, ela trata de valores negativos e zero.</p>
<p><br></p>
<p>A ideia do <em>steps</em> utilizados na faze de pr√©-processamento dos dados, em que utilizamos o pacote recipes do R, √© que para novas observa√ß√µes, depois do modelo ajustado, essas transforma√ß√µes ser√£o automaticamente aplicadas aos novos dados.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="tidymodels-knn-e-nadaraya-watson-3" class="slide level2">
<h2>Tidymodels <span class="math inline">k</span>NN e Nadaraya-Watson</h2>
<p><br></p>

<img data-src="imgs/plot_hiper_ajuste_tidymodels_knn.png" style="width:50.0%" class="r-stretch quarto-figure-center"></section>
<section id="tidymodels-knn-e-nadaraya-watson-4" class="slide level2">
<h2>Tidymodels <span class="math inline">k</span>NN e Nadaraya-Watson</h2>
<p><br></p>
<p><span class="red">Exemplo</span>: Vamos reproduzir todo o fluxo de treinamento que fizemos com o m√©todo <span class="math inline">k</span>NN no exemplo anterior, agora, utilizando o modelo Nadaraya-Watson. Note que uma vez que entendemos o <a href="https://www.tidymodels.org/">tidymodels</a>, fica f√°cil adaptar um c√≥digo j√° existente para o treinamento de um outro modelo. Na verdade, o m√©todo de Nadaraya-Watson implementado no tidymodels √© um pouco diferente. Ainda utilizamos a informa√ß√£o de <span class="math inline">k</span>, em que o valor de <span class="math inline">h</span> √© deverminado pela m√©dia dos vizinhos mais pr√≥ximos de <span class="math inline">{\bf x}_i</span> √† <span class="math inline">{\bf x}</span>. Al√©m de <span class="math inline">k</span>, temos o valor de <span class="math inline">p</span> para a dist√¢ncia de Minkowski como hiperpar√¢metro. Portanto, aqui, teremos dois hiperpar√¢metros (par√¢metros de sintoniza√ß√£o).</p>
<p><br></p>
<div class="cell">
<details>
<summary>Workflow completo do treimanento de um modelo Nadaraya-Watson.</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb15-3"><a href="#cb15-3"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb15-5"><a href="#cb15-5"></a><span class="fu">library</span>(doMC)</span>
<span id="cb15-6"><a href="#cb15-6"></a></span>
<span id="cb15-7"><a href="#cb15-7"></a><span class="co"># Paralelizando o c√≥digo em sistemas Unix</span></span>
<span id="cb15-8"><a href="#cb15-8"></a><span class="fu">registerDoMC</span>(<span class="at">cores =</span> parallel<span class="sc">::</span><span class="fu">detectCores</span>())</span>
<span id="cb15-9"><a href="#cb15-9"></a></span>
<span id="cb15-10"><a href="#cb15-10"></a><span class="co"># Resolvendo eventuais conflitos entre tidymodels e outros pacotes eventualmente</span></span>
<span id="cb15-11"><a href="#cb15-11"></a><span class="co"># carregados:</span></span>
<span id="cb15-12"><a href="#cb15-12"></a>tidymodels<span class="sc">::</span><span class="fu">tidymodels_prefer</span>()</span>
<span id="cb15-13"><a href="#cb15-13"></a></span>
<span id="cb15-14"><a href="#cb15-14"></a><span class="co"># Lendo a base de dados:</span></span>
<span id="cb15-15"><a href="#cb15-15"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb15-16"><a href="#cb15-16"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb15-17"><a href="#cb15-17"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb15-18"><a href="#cb15-18"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb15-19"><a href="#cb15-19"></a></span>
<span id="cb15-20"><a href="#cb15-20"></a>dados_expectativa_renda <span class="ot">&lt;-</span></span>
<span id="cb15-21"><a href="#cb15-21"></a>  dados_expectativa_renda <span class="sc">|&gt;</span></span>
<span id="cb15-22"><a href="#cb15-22"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>CountryName)</span>
<span id="cb15-23"><a href="#cb15-23"></a></span>
<span id="cb15-24"><a href="#cb15-24"></a><span class="co"># Setando uma semente</span></span>
<span id="cb15-25"><a href="#cb15-25"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb15-26"><a href="#cb15-26"></a></span>
<span id="cb15-27"><a href="#cb15-27"></a><span class="co"># Divis√£o da base de dados ------------------------------------------------</span></span>
<span id="cb15-28"><a href="#cb15-28"></a><span class="co"># Divis√£o inicial (treino e teste)</span></span>
<span id="cb15-29"><a href="#cb15-29"></a>splits <span class="ot">&lt;-</span> </span>
<span id="cb15-30"><a href="#cb15-30"></a>  rsample<span class="sc">::</span><span class="fu">initial_split</span>(</span>
<span id="cb15-31"><a href="#cb15-31"></a>    dados_expectativa_renda,</span>
<span id="cb15-32"><a href="#cb15-32"></a>    <span class="at">strata =</span> <span class="st">"LifeExpectancy"</span>, </span>
<span id="cb15-33"><a href="#cb15-33"></a>    <span class="at">prop =</span> <span class="fl">0.8</span> </span>
<span id="cb15-34"><a href="#cb15-34"></a>  )</span>
<span id="cb15-35"><a href="#cb15-35"></a></span>
<span id="cb15-36"><a href="#cb15-36"></a><span class="co"># O conjunto de dados de treinamento ser√° utilizado para ajustar/treinar o</span></span>
<span id="cb15-37"><a href="#cb15-37"></a><span class="co"># modelo:</span></span>
<span id="cb15-38"><a href="#cb15-38"></a>treinamento <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(splits)</span>
<span id="cb15-39"><a href="#cb15-39"></a></span>
<span id="cb15-40"><a href="#cb15-40"></a><span class="co"># O conjunto de teste ser√° utilizado apenas no fim, para avaliar o desempenho</span></span>
<span id="cb15-41"><a href="#cb15-41"></a><span class="co"># preditivo final do modelo:</span></span>
<span id="cb15-42"><a href="#cb15-42"></a>teste <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">testing</span>(splits) </span>
<span id="cb15-43"><a href="#cb15-43"></a></span>
<span id="cb15-44"><a href="#cb15-44"></a><span class="co"># Criando uma receita para os dados ---------------------------------------</span></span>
<span id="cb15-45"><a href="#cb15-45"></a></span>
<span id="cb15-46"><a href="#cb15-46"></a><span class="co"># Poderia ter passado o conjunto treinamento ou posso passar o conjunto "splits".</span></span>
<span id="cb15-47"><a href="#cb15-47"></a><span class="co"># O comando recipes j√° entende que dever√° utilizar o conjunto de treinamento.</span></span>
<span id="cb15-48"><a href="#cb15-48"></a>receita <span class="ot">&lt;-</span> </span>
<span id="cb15-49"><a href="#cb15-49"></a>  recipes<span class="sc">::</span><span class="fu">recipe</span>(<span class="at">formula =</span> LifeExpectancy <span class="sc">~</span> ., <span class="at">data =</span> treinamento) <span class="sc">|&gt;</span> </span>
<span id="cb15-50"><a href="#cb15-50"></a>  <span class="fu">step_YeoJohnson</span>(<span class="fu">all_predictors</span>()) <span class="sc">|&gt;</span> <span class="co"># Ajuda na normaliza√ß√£o dos dados. Pode ser bom!</span></span>
<span id="cb15-51"><a href="#cb15-51"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>()) <span class="co"># Normalizando vari√°veis num√©ricas.</span></span>
<span id="cb15-52"><a href="#cb15-52"></a></span>
<span id="cb15-53"><a href="#cb15-53"></a><span class="co"># Construindo modelo Nadaraya ---------------------------------------------</span></span>
<span id="cb15-54"><a href="#cb15-54"></a>modelo_nadaraya <span class="ot">&lt;-</span> </span>
<span id="cb15-55"><a href="#cb15-55"></a>  parsnip<span class="sc">::</span><span class="fu">nearest_neighbor</span>(<span class="at">dist_power =</span> <span class="fu">tune</span>(), <span class="at">weight_func =</span> <span class="st">"gaussian"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb15-56"><a href="#cb15-56"></a>  parsnip<span class="sc">::</span><span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb15-57"><a href="#cb15-57"></a>  parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"kknn"</span>)</span>
<span id="cb15-58"><a href="#cb15-58"></a></span>
<span id="cb15-59"><a href="#cb15-59"></a><span class="co"># Construindo um workflow (pipeline) --------------------------------------</span></span>
<span id="cb15-60"><a href="#cb15-60"></a>wf_nadaraya <span class="ot">&lt;-</span></span>
<span id="cb15-61"><a href="#cb15-61"></a>  workflows<span class="sc">::</span><span class="fu">workflow</span>() <span class="sc">|&gt;</span></span>
<span id="cb15-62"><a href="#cb15-62"></a>  workflows<span class="sc">::</span><span class="fu">add_recipe</span>(receita) <span class="sc">|&gt;</span></span>
<span id="cb15-63"><a href="#cb15-63"></a>  workflows<span class="sc">::</span><span class="fu">add_model</span>(modelo_nadaraya)</span>
<span id="cb15-64"><a href="#cb15-64"></a></span>
<span id="cb15-65"><a href="#cb15-65"></a><span class="co"># Cross-validation --------------------------------------------------------</span></span>
<span id="cb15-66"><a href="#cb15-66"></a>cv <span class="ot">&lt;-</span> </span>
<span id="cb15-67"><a href="#cb15-67"></a>  treinamento <span class="sc">|&gt;</span> </span>
<span id="cb15-68"><a href="#cb15-68"></a>  rsample<span class="sc">::</span><span class="fu">vfold_cv</span>(<span class="at">v =</span> 10L, <span class="at">strata =</span> <span class="st">"LifeExpectancy"</span>)</span>
<span id="cb15-69"><a href="#cb15-69"></a></span>
<span id="cb15-70"><a href="#cb15-70"></a><span class="co"># Busca do hiperpar√¢metro k -----------------------------------------------</span></span>
<span id="cb15-71"><a href="#cb15-71"></a>metrica <span class="ot">&lt;-</span> <span class="fu">metric_set</span>(rmse)</span>
<span id="cb15-72"><a href="#cb15-72"></a></span>
<span id="cb15-73"><a href="#cb15-73"></a><span class="co"># Extraindo e atualizando range do par√¢metro ------------------------------</span></span>
<span id="cb15-74"><a href="#cb15-74"></a>update_parametros <span class="ot">&lt;-</span></span>
<span id="cb15-75"><a href="#cb15-75"></a>  wf_nadaraya <span class="sc">|&gt;</span> </span>
<span id="cb15-76"><a href="#cb15-76"></a>  <span class="fu">extract_parameter_set_dials</span>() <span class="sc">|&gt;</span></span>
<span id="cb15-77"><a href="#cb15-77"></a>  <span class="fu">update</span>(<span class="st">"dist_power"</span> <span class="ot">=</span> <span class="fu">dist_power</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">50</span>)))</span>
<span id="cb15-78"><a href="#cb15-78"></a></span>
<span id="cb15-79"><a href="#cb15-79"></a><span class="co"># Tunagem -----------------------------------------------------------------</span></span>
<span id="cb15-80"><a href="#cb15-80"></a>meu_grid <span class="ot">&lt;-</span> dials<span class="sc">::</span><span class="fu">grid_max_entropy</span>(update_parametros, <span class="at">size =</span> 100L)</span>
<span id="cb15-81"><a href="#cb15-81"></a></span>
<span id="cb15-82"><a href="#cb15-82"></a>tunagem <span class="ot">&lt;-</span></span>
<span id="cb15-83"><a href="#cb15-83"></a>  tune<span class="sc">::</span><span class="fu">tune_grid</span>(</span>
<span id="cb15-84"><a href="#cb15-84"></a>    wf_knn,</span>
<span id="cb15-85"><a href="#cb15-85"></a>    <span class="at">resamples =</span> cv,</span>
<span id="cb15-86"><a href="#cb15-86"></a>    <span class="at">grid =</span> meu_grid,</span>
<span id="cb15-87"><a href="#cb15-87"></a>    <span class="at">metrics =</span> metrica,</span>
<span id="cb15-88"><a href="#cb15-88"></a>    <span class="at">control =</span> <span class="fu">control_grid</span>(<span class="at">save_pred =</span> <span class="cn">TRUE</span>, <span class="at">verbose =</span> <span class="cn">TRUE</span>)</span>
<span id="cb15-89"><a href="#cb15-89"></a>  )</span>
<span id="cb15-90"><a href="#cb15-90"></a></span>
<span id="cb15-91"><a href="#cb15-91"></a>p_hiper <span class="ot">&lt;-</span> <span class="fu">autoplot</span>(tunagem) <span class="sc">+</span></span>
<span id="cb15-92"><a href="#cb15-92"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Nadaraya-Watson - Sele√ß√£o do par√¢metro p"</span>, <span class="at">subtitle =</span> <span class="st">"Sintoniza√ß√£o do hiperpar√¢metro (dist√¢ncia p)"</span>) <span class="sc">+</span></span>
<span id="cb15-93"><a href="#cb15-93"></a>  <span class="fu">theme</span>(</span>
<span id="cb15-94"><a href="#cb15-94"></a>    <span class="at">title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb15-95"><a href="#cb15-95"></a>  )</span>
<span id="cb15-96"><a href="#cb15-96"></a></span>
<span id="cb15-97"><a href="#cb15-97"></a><span class="co"># Atualizando workflow ----------------------------------------------------</span></span>
<span id="cb15-98"><a href="#cb15-98"></a>wf_nadaraya <span class="ot">&lt;-</span> </span>
<span id="cb15-99"><a href="#cb15-99"></a>  wf_nadaraya <span class="sc">|&gt;</span> </span>
<span id="cb15-100"><a href="#cb15-100"></a>  <span class="fu">finalize_workflow</span>(<span class="fu">select_best</span>(tunagem))</span>
<span id="cb15-101"><a href="#cb15-101"></a></span>
<span id="cb15-102"><a href="#cb15-102"></a><span class="co"># Ajustar o modelo ao conjunto de treinamento e avaliar no teste --------</span></span>
<span id="cb15-103"><a href="#cb15-103"></a>ajuste_final <span class="ot">&lt;-</span> <span class="fu">last_fit</span>(wf_nadaraya, splits)</span>
<span id="cb15-104"><a href="#cb15-104"></a></span>
<span id="cb15-105"><a href="#cb15-105"></a><span class="co"># Ajuste final com toda a base de dados -----------------------------------</span></span>
<span id="cb15-106"><a href="#cb15-106"></a>modelo_final <span class="ot">&lt;-</span> <span class="fu">fit</span>(wf_nadaraya, <span class="at">data =</span> dados_expectativa_renda)</span>
<span id="cb15-107"><a href="#cb15-107"></a></span>
<span id="cb15-108"><a href="#cb15-108"></a><span class="co"># Visualizando as predi√ß√µes na base de treino</span></span>
<span id="cb15-109"><a href="#cb15-109"></a>p_ajuste <span class="ot">&lt;-</span> ajuste_final<span class="sc">$</span>.predictions[[1L]] <span class="sc">|&gt;</span> </span>
<span id="cb15-110"><a href="#cb15-110"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> LifeExpectancy, <span class="at">y =</span> .pred)) <span class="sc">+</span> </span>
<span id="cb15-111"><a href="#cb15-111"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb15-112"><a href="#cb15-112"></a>  <span class="fu">labs</span>(</span>
<span id="cb15-113"><a href="#cb15-113"></a>    <span class="at">title =</span> <span class="st">"Predi√ß√µes versus Real"</span>, </span>
<span id="cb15-114"><a href="#cb15-114"></a>    <span class="at">subtitle =</span> <span class="st">"Usando apenas os dados de teste"</span></span>
<span id="cb15-115"><a href="#cb15-115"></a>  ) <span class="sc">+</span></span>
<span id="cb15-116"><a href="#cb15-116"></a>  <span class="fu">xlab</span>(<span class="st">"LifeExpectancy"</span>) <span class="sc">+</span> </span>
<span id="cb15-117"><a href="#cb15-117"></a>  <span class="fu">xlab</span>(<span class="st">"LifeExpectancy predito"</span>) <span class="sc">+</span> </span>
<span id="cb15-118"><a href="#cb15-118"></a>  <span class="fu">theme</span>(</span>
<span id="cb15-119"><a href="#cb15-119"></a>    <span class="at">title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb15-120"><a href="#cb15-120"></a>  )</span>
<span id="cb15-121"><a href="#cb15-121"></a></span>
<span id="cb15-122"><a href="#cb15-122"></a><span class="co"># Unindo os dois plots</span></span>
<span id="cb15-123"><a href="#cb15-123"></a>p <span class="ot">&lt;-</span> p_hiper <span class="sc">+</span> p_ajuste <span class="sc">+</span> <span class="fu">plot_annotation</span>(<span class="at">tag_levels =</span> <span class="st">"A"</span>)</span>
<span id="cb15-124"><a href="#cb15-124"></a></span>
<span id="cb15-125"><a href="#cb15-125"></a><span class="co"># Salvando gr√°ficos</span></span>
<span id="cb15-126"><a href="#cb15-126"></a><span class="fu">ggsave</span>(p, <span class="at">file =</span> <span class="st">"imgs/plot_hiper_ajuste_tidymodels_nadaraya.png"</span>, <span class="at">width =</span> <span class="dv">30</span>,</span>
<span id="cb15-127"><a href="#cb15-127"></a>       <span class="at">height =</span> <span class="dv">15</span>, <span class="at">units =</span> <span class="st">"cm"</span>)</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="tidymodels-knn-e-nadaraya-watson-5" class="slide level2">
<h2>Tidymodels <span class="math inline">k</span>NN e Nadaraya-Watson</h2>
<p><br></p>
<p><img data-src="imgs/plot_hiper_ajuste_tidymodels_nadaraya.png" data-fig-aling="center"> ## Tidymodels <span class="math inline">k</span>NN e Nadaraya-Watson</p>
<p><br></p>
<p>Podemos visualizar a melhor combina√ß√£o de hiperpar√¢metros, segundo <span class="math inline">\widehat{R}(g)</span> usando fazendo:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># As cinco melhores combina√ß√µes</span></span>
<span id="cb16-2"><a href="#cb16-2"></a>tunagem <span class="sc">|&gt;</span> </span>
<span id="cb16-3"><a href="#cb16-3"></a>  <span class="fu">show_best</span>()</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img data-src="gifs/bean_simple.gif"></p>
</section>
<section id="comparando-dois-ou-mais-modelos" class="slide level2">
<h2>Comparando dois ou mais modelos</h2>
<p><br></p>
<p><strong>Voc√™ poderia perguntar</strong>: ‚ÄúCerto, mais tenho como comparar dois ou mais modelos de uma √∫nica vez?‚Äù</p>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="gifs/giphy.gif"></p>
</div><div class="column" style="width:50%;">
<p><br></p>
<p>A resposta √© <strong>Sim</strong>!</p>
<p><br></p>
<p><img data-src="gifs/yes.gif"></p>
</div>
</div>
</section>
<section id="comparando-dois-ou-mais-modelos-1" class="slide level2">
<h2>Comparando dois ou mais modelos</h2>
<p><br></p>
<p>Na verdade, √© bem mais interessante comparar conjuntamente, visto que para poder compararmos devemos garantir que as mesmas amostras de treinamento e teste est√£o sendo utilizadas, i.e., para termos uma compara√ß√£o mais justa. Claro que d√° para fazer de forma separada, fixando a semente dos geradores de n√∫meros pseudo-aleat√≥rios, para que a biblioteca rsample possa reproduzir a mesma divis√£o para ambos os modelos. Por√©m, a estrat√©gia do exemplo abaixo √© mais consistente.</p>
<p><br></p>
<p><span class="red">Exemplo</span>: Estude o c√≥digo que segue! Ele compara os modelos de <span class="math inline">k</span>NN com o m√©todo de Nadaraya-Watson.</p>
<p><br></p>
<div class="cell">
<details>
<summary>Workflow completo do treimanento e compara√ß√£o de dois modelos (KNN e Nadaraya-Whatson).</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb17-3"><a href="#cb17-3"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb17-4"><a href="#cb17-4"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb17-5"><a href="#cb17-5"></a></span>
<span id="cb17-6"><a href="#cb17-6"></a><span class="co"># Resolvendo eventuais conflitos entre tidymodels e outros pacotes eventualmente</span></span>
<span id="cb17-7"><a href="#cb17-7"></a><span class="co"># carregados:</span></span>
<span id="cb17-8"><a href="#cb17-8"></a>tidymodels<span class="sc">::</span><span class="fu">tidymodels_prefer</span>()</span>
<span id="cb17-9"><a href="#cb17-9"></a></span>
<span id="cb17-10"><a href="#cb17-10"></a><span class="co"># Lendo a base de dados:</span></span>
<span id="cb17-11"><a href="#cb17-11"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb17-12"><a href="#cb17-12"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb17-13"><a href="#cb17-13"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb17-14"><a href="#cb17-14"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb17-15"><a href="#cb17-15"></a></span>
<span id="cb17-16"><a href="#cb17-16"></a>dados_expectativa_renda <span class="ot">&lt;-</span></span>
<span id="cb17-17"><a href="#cb17-17"></a>  dados_expectativa_renda <span class="sc">|&gt;</span></span>
<span id="cb17-18"><a href="#cb17-18"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>CountryName)</span>
<span id="cb17-19"><a href="#cb17-19"></a></span>
<span id="cb17-20"><a href="#cb17-20"></a><span class="co"># Setando uma semente</span></span>
<span id="cb17-21"><a href="#cb17-21"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb17-22"><a href="#cb17-22"></a></span>
<span id="cb17-23"><a href="#cb17-23"></a><span class="co"># Divis√£o da base de dados ------------------------------------------------</span></span>
<span id="cb17-24"><a href="#cb17-24"></a><span class="co"># Divis√£o inicial (treino e teste)</span></span>
<span id="cb17-25"><a href="#cb17-25"></a>splits <span class="ot">&lt;-</span> </span>
<span id="cb17-26"><a href="#cb17-26"></a>  rsample<span class="sc">::</span><span class="fu">initial_split</span>(</span>
<span id="cb17-27"><a href="#cb17-27"></a>    dados_expectativa_renda,</span>
<span id="cb17-28"><a href="#cb17-28"></a>    <span class="at">strata =</span> <span class="st">"LifeExpectancy"</span>, </span>
<span id="cb17-29"><a href="#cb17-29"></a>    <span class="at">prop =</span> <span class="fl">0.8</span> </span>
<span id="cb17-30"><a href="#cb17-30"></a>  )</span>
<span id="cb17-31"><a href="#cb17-31"></a></span>
<span id="cb17-32"><a href="#cb17-32"></a><span class="co"># O conjunto de dados de treinamento ser√° utilizado para ajustar/treinar o</span></span>
<span id="cb17-33"><a href="#cb17-33"></a><span class="co"># modelo:</span></span>
<span id="cb17-34"><a href="#cb17-34"></a>treinamento <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(splits)</span>
<span id="cb17-35"><a href="#cb17-35"></a></span>
<span id="cb17-36"><a href="#cb17-36"></a><span class="co"># O conjunto de teste ser√° utilizado apenas no fim, para avaliar o desempenho</span></span>
<span id="cb17-37"><a href="#cb17-37"></a><span class="co"># preditivo final do modelo:</span></span>
<span id="cb17-38"><a href="#cb17-38"></a>teste <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">testing</span>(splits) </span>
<span id="cb17-39"><a href="#cb17-39"></a></span>
<span id="cb17-40"><a href="#cb17-40"></a><span class="co"># Criando uma receita para os dados ---------------------------------------</span></span>
<span id="cb17-41"><a href="#cb17-41"></a></span>
<span id="cb17-42"><a href="#cb17-42"></a><span class="co"># Poderia ter passado o conjunto treinamento ou posso passar o conjunto "splits".</span></span>
<span id="cb17-43"><a href="#cb17-43"></a><span class="co"># O comando recipes j√° entende que dever√° utilizar o conjunto de treinamento.</span></span>
<span id="cb17-44"><a href="#cb17-44"></a>receita_knn <span class="ot">&lt;-</span> </span>
<span id="cb17-45"><a href="#cb17-45"></a>  recipes<span class="sc">::</span><span class="fu">recipe</span>(<span class="at">formula =</span> LifeExpectancy <span class="sc">~</span> ., <span class="at">data =</span> treinamento) <span class="sc">|&gt;</span> </span>
<span id="cb17-46"><a href="#cb17-46"></a>  <span class="fu">step_YeoJohnson</span>(<span class="fu">all_predictors</span>()) <span class="sc">|&gt;</span> <span class="co"># Ajuda na normaliza√ß√£o dos dados. Pode ser bom!</span></span>
<span id="cb17-47"><a href="#cb17-47"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>()) <span class="co"># Normalizando vari√°veis num√©ricas.</span></span>
<span id="cb17-48"><a href="#cb17-48"></a></span>
<span id="cb17-49"><a href="#cb17-49"></a>receita_nadaraya <span class="ot">&lt;-</span> receita_knn</span>
<span id="cb17-50"><a href="#cb17-50"></a></span>
<span id="cb17-51"><a href="#cb17-51"></a><span class="co"># Construindo modelo kNN --------------------------------------------------</span></span>
<span id="cb17-52"><a href="#cb17-52"></a>modelo_knn <span class="ot">&lt;-</span> </span>
<span id="cb17-53"><a href="#cb17-53"></a>  parsnip<span class="sc">::</span><span class="fu">nearest_neighbor</span>(<span class="at">neighbors =</span> <span class="fu">tune</span>()) <span class="sc">|&gt;</span> </span>
<span id="cb17-54"><a href="#cb17-54"></a>  parsnip<span class="sc">::</span><span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-55"><a href="#cb17-55"></a>  parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"kknn"</span>)</span>
<span id="cb17-56"><a href="#cb17-56"></a></span>
<span id="cb17-57"><a href="#cb17-57"></a><span class="co"># Construindo modelo Nadaraya ---------------------------------------------</span></span>
<span id="cb17-58"><a href="#cb17-58"></a>modelo_nadaraya <span class="ot">&lt;-</span> </span>
<span id="cb17-59"><a href="#cb17-59"></a>  parsnip<span class="sc">::</span><span class="fu">nearest_neighbor</span>(<span class="at">dist_power =</span> <span class="fu">tune</span>(), <span class="at">neighbors =</span> <span class="fu">tune</span>(),</span>
<span id="cb17-60"><a href="#cb17-60"></a>                            <span class="at">weight_func =</span> <span class="st">"gaussian"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-61"><a href="#cb17-61"></a>  parsnip<span class="sc">::</span><span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-62"><a href="#cb17-62"></a>  parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"kknn"</span>)</span>
<span id="cb17-63"><a href="#cb17-63"></a></span>
<span id="cb17-64"><a href="#cb17-64"></a><span class="co"># Valida√ß√£o cruzada -------------------------------------------------------</span></span>
<span id="cb17-65"><a href="#cb17-65"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb17-66"><a href="#cb17-66"></a>cv <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">vfold_cv</span>(treinamento, <span class="at">v =</span> 5L)</span>
<span id="cb17-67"><a href="#cb17-67"></a></span>
<span id="cb17-68"><a href="#cb17-68"></a><span class="co"># Criando workflow conjunto -----------------------------------------------</span></span>
<span id="cb17-69"><a href="#cb17-69"></a>all_wf <span class="ot">&lt;-</span> </span>
<span id="cb17-70"><a href="#cb17-70"></a>  <span class="fu">workflow_set</span>(</span>
<span id="cb17-71"><a href="#cb17-71"></a>    <span class="at">preproc =</span> <span class="fu">list</span>(receita_knn, receita_nadaraya),</span>
<span id="cb17-72"><a href="#cb17-72"></a>    <span class="at">models =</span> <span class="fu">list</span>(<span class="at">modelo_knn =</span> modelo_knn, <span class="at">modelo_nadaraya =</span> modelo_nadaraya)</span>
<span id="cb17-73"><a href="#cb17-73"></a>  )</span>
<span id="cb17-74"><a href="#cb17-74"></a></span>
<span id="cb17-75"><a href="#cb17-75"></a><span class="co"># Tunando ambos os modelos ------------------------------------------------</span></span>
<span id="cb17-76"><a href="#cb17-76"></a>tunagem <span class="ot">&lt;-</span> </span>
<span id="cb17-77"><a href="#cb17-77"></a>  all_wf <span class="sc">|&gt;</span> </span>
<span id="cb17-78"><a href="#cb17-78"></a>  <span class="fu">workflow_map</span>(</span>
<span id="cb17-79"><a href="#cb17-79"></a>    <span class="at">seed =</span> <span class="dv">0</span>, </span>
<span id="cb17-80"><a href="#cb17-80"></a>    <span class="at">verbose =</span> <span class="cn">TRUE</span>,</span>
<span id="cb17-81"><a href="#cb17-81"></a>    <span class="at">resamples =</span> cv,</span>
<span id="cb17-82"><a href="#cb17-82"></a>    <span class="at">grid =</span> <span class="dv">50</span>,</span>
<span id="cb17-83"><a href="#cb17-83"></a>    <span class="at">metrics =</span> <span class="fu">metric_set</span>(rmse)</span>
<span id="cb17-84"><a href="#cb17-84"></a>  )</span>
<span id="cb17-85"><a href="#cb17-85"></a></span>
<span id="cb17-86"><a href="#cb17-86"></a><span class="co"># Selecionando o melhor de cada um dos modelos ----------------------------</span></span>
<span id="cb17-87"><a href="#cb17-87"></a>melhor_knn <span class="ot">&lt;-</span> </span>
<span id="cb17-88"><a href="#cb17-88"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb17-89"><a href="#cb17-89"></a>  <span class="fu">extract_workflow_set_result</span>(<span class="st">"recipe_1_modelo_knn"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-90"><a href="#cb17-90"></a>  <span class="fu">select_best</span>(<span class="st">"rmse"</span>)</span>
<span id="cb17-91"><a href="#cb17-91"></a></span>
<span id="cb17-92"><a href="#cb17-92"></a>melhor_nadaraya <span class="ot">&lt;-</span> </span>
<span id="cb17-93"><a href="#cb17-93"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb17-94"><a href="#cb17-94"></a>  <span class="fu">extract_workflow_set_result</span>(<span class="st">"recipe_1_modelo_nadaraya"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-95"><a href="#cb17-95"></a>  <span class="fu">select_best</span>(<span class="st">"rmse"</span>)</span>
<span id="cb17-96"><a href="#cb17-96"></a></span>
<span id="cb17-97"><a href="#cb17-97"></a><span class="co"># Avaliando o desempenho no conjunto de teste</span></span>
<span id="cb17-98"><a href="#cb17-98"></a>teste_knn <span class="ot">&lt;-</span> </span>
<span id="cb17-99"><a href="#cb17-99"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb17-100"><a href="#cb17-100"></a>  <span class="fu">extract_workflow</span>(<span class="st">"recipe_1_modelo_knn"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-101"><a href="#cb17-101"></a>  <span class="fu">finalize_workflow</span>(melhor_knn) <span class="sc">|&gt;</span> </span>
<span id="cb17-102"><a href="#cb17-102"></a>  <span class="fu">last_fit</span>(<span class="at">split =</span> splits)</span>
<span id="cb17-103"><a href="#cb17-103"></a></span>
<span id="cb17-104"><a href="#cb17-104"></a>teste_nadaraya <span class="ot">&lt;-</span> </span>
<span id="cb17-105"><a href="#cb17-105"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb17-106"><a href="#cb17-106"></a>  <span class="fu">extract_workflow</span>(<span class="st">"recipe_1_modelo_nadaraya"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-107"><a href="#cb17-107"></a>  <span class="fu">finalize_workflow</span>(melhor_nadaraya) <span class="sc">|&gt;</span> </span>
<span id="cb17-108"><a href="#cb17-108"></a>  <span class="fu">last_fit</span>(<span class="at">split =</span> splits)</span>
<span id="cb17-109"><a href="#cb17-109"></a></span>
<span id="cb17-110"><a href="#cb17-110"></a><span class="co"># Visualizando as m√©tricas de cada um</span></span>
<span id="cb17-111"><a href="#cb17-111"></a><span class="fu">collect_metrics</span>(teste_knn)</span>
<span id="cb17-112"><a href="#cb17-112"></a><span class="fu">collect_metrics</span>(teste_nadaraya)</span>
<span id="cb17-113"><a href="#cb17-113"></a></span>
<span id="cb17-114"><a href="#cb17-114"></a><span class="co"># Ajustando o modelo com todos os dados. Aqui escolhemos o Nadaraya-Watson</span></span>
<span id="cb17-115"><a href="#cb17-115"></a>modelo_final <span class="ot">&lt;-</span> </span>
<span id="cb17-116"><a href="#cb17-116"></a>  teste_nadaraya <span class="sc">|&gt;</span> </span>
<span id="cb17-117"><a href="#cb17-117"></a>  <span class="fu">extract_workflow</span>(<span class="st">"recipe_1_modelo_nadaraya"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-118"><a href="#cb17-118"></a>  <span class="fu">fit</span>(<span class="at">data =</span> dados_expectativa_renda)</span>
<span id="cb17-119"><a href="#cb17-119"></a></span>
<span id="cb17-120"><a href="#cb17-120"></a><span class="co"># Fazendo previs√µes com novos dados. Aqui usarei os mesmos dados</span></span>
<span id="cb17-121"><a href="#cb17-121"></a><span class="fu">predict</span>(modelo_final, <span class="at">new_data =</span> dados_expectativa_renda)</span>
<span id="cb17-122"><a href="#cb17-122"></a></span>
<span id="cb17-123"><a href="#cb17-123"></a><span class="co"># Salvando o modelo em um arquivo. Aqui estou supondo que salvei em</span></span>
<span id="cb17-124"><a href="#cb17-124"></a><span class="co"># "~/Downloads/modelo_final.rds":</span></span>
<span id="cb17-125"><a href="#cb17-125"></a><span class="co"># saveRDS(modelo_final, file = "~/Downloads/modelo_final.rds")</span></span>
<span id="cb17-126"><a href="#cb17-126"></a></span>
<span id="cb17-127"><a href="#cb17-127"></a><span class="co"># Lendo um modelo salvo para depois fazer predi√ß√µes. Aqui estou supondo que </span></span>
<span id="cb17-128"><a href="#cb17-128"></a><span class="co"># o modelo encontra-se salvo em "~/Downloads/modelo_final.rds":</span></span>
<span id="cb17-129"><a href="#cb17-129"></a><span class="co"># readRDS("~/Downloads/modelo_final.rds")</span></span>
<span id="cb17-130"><a href="#cb17-130"></a><span class="sc">!</span>[](gifs<span class="sc">/</span>bom.gif)</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="suport-vector-regression-machine" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>M√©todos de estima√ß√£o da fun√ß√£o de regress√£o <span class="math inline">r({\bf x})</span> com base em <em>Reproducing Kernel Hilbert Spaces</em> - RKHs s√£o fam√≠lias de metodologias bastante gerais. A ideia desses m√©todos envolvem definir uma fun√ß√£o objetivo para a quantifica√ß√£o da qualidade das predi√ß√µes e, porteriormente, busca-se uma fun√ß√£o que melhor se ajuste ao espa√ßo de fun√ß√µes <span class="math inline">\mathcal{H}</span>. Busca-se uma solu√ß√£o para</p>
<p><span id="eq-objetivo-geral-hilbert"><span class="math display">\argmin_{g \in \mathcal{H}} \sum_{k = 1}^n L(g({\bf x}_k, y_k)) + \mathcal{P}(g), \tag{4}</span></span> em que <span class="math inline">L</span> √© uma fun√ß√£o de perda arbitr√°ria, <span class="math inline">\mathcal{P}</span> uma medida de complexidade de <span class="math inline">g</span> e <span class="math inline">\mathcal{H}</span> um subespa√ßo de fun√ß√µes.</p>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Ocorre que para um espa√ßo de fun√ß√µes arbitr√°rio, a solu√ß√£o para o problema seria bastante dif√≠cil. A ideia √© poder uma grande fam√≠lia de espa√ßos <span class="math inline">\mathcal{H}</span> (RKHs) de modo que a solu√ß√£o do problema seja relativamente simples de ser implementada.</p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="gifs/ok.gif"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="suport-vector-regression-machine-1" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>M√©todos de regress√£o que se baseiam em panaliza√ß√£o em RKHS s√£o uma generaliza√ß√£o da <a href="#/suport-vector-regression-machine" class="quarto-xref">Equa√ß√£o&nbsp;4</a>, em que a fun√ß√£o objetivo nesse caso √© dada por:</p>
<p><span id="eq-funcao-objetivo-geral"><span class="math display">\argmin_{g \in \mathcal{H}_k} \sum_{k = 1}^n L(g({\bf x}_k, y_k)) + \lambda||g||_{\mathcal{H}_k}^2, \tag{5}</span></span> em que <span class="math inline">\mathcal{H}_{k}</span> √© um RKHS e <span class="math inline">L</span> √© uma fun√ß√£o adequada para o problema em quest√£o. Calma que vai ser relativamente simples definir <span class="math inline">\mathcal{H}_{k}</span>, uma vez que isso √© feito utilizando fun√ß√µes kernel, em particular, o <a href="https://en.wikipedia.org/wiki/Mercer%27s_theorem">kernel de Mercer</a>.</p>

<img data-src="gifs/bean_simple.gif" class="r-stretch"></section>
<section id="suport-vector-regression-machine-2" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>O termo <span class="math inline">||g||_{\mathcal{H}_k}^2</span> na <a href="#/suport-vector-regression-machine-1" class="quarto-xref">Equa√ß√£o&nbsp;5</a> reflete a suavidade das fun√ß√µes em <span class="math inline">\mathcal{H}_k</span> e cada espa√ßo poder√° conter uma no√ß√£o de suavidade diferente, a depender da fun√ß√£o de kernel escolhida. Sim, a fun√ß√£o ed kernel e fun√ß√£o de perda <span class="math inline">L</span> s√£o escolhidas pelo usu√°rio da metodologia.</p>
<p><br></p>
<p>√â claro que para diferen√ßas kernel (kernel de Mercer), poderemos ter diferentes resultados que pode ser avaliado em um procedimento de valida√ß√£o-cruzada (<em>cross-validation</em>). O par√¢metro <span class="math inline">\lambda</span> √© um hiperpar√¢metro que podemos obter dentro de uma valida√ß√£o-cruzada, testando, por exemplo, um <em>grid</em> de par√¢metros, para selecionarmos um <span class="math inline">\lambda</span> que forne√ßa o melhor risco estimado <span class="math inline">\widehat{R}(g)</span>.</p>
<p><br></p>
<p>A parcela <span class="math inline">||g||_{\mathcal{H}_k}^2</span> mensura a suavidade das fun√ß√µes em <span class="math inline">\mathcal{H}_k</span>. Assim como <span class="math inline">g</span>, <span class="math inline">||g||_{\mathcal{H}_k}^2</span> ser√° definidas em termos da fun√ß√£o de kernel, e essa √© a grande sacada!</p>
<p><br></p>

<img data-src="gifs/uau.gif" class="r-stretch"></section>
<section id="suport-vector-regression-machine-3" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p><strong>Defini√ß√£o</strong> (<span class="red">Kernel de Mercer</span>): Seja <span class="math inline">K({\bf x}_a, {\bf x}_b)</span> uma fun√ß√£o com dom√≠nio em <span class="math inline">\mathcal{X} \times \mathcal{X}</span> (dom√≠nio das <em>features</em>/covari√°veis/espa√ßo de caracter√≠sticas) que poder√° ser mais geral que <span class="math inline">\mathbb{R}^d</span>. Diremos que uma fun√ß√£o <span class="math inline">K</span>, tal que <span class="math inline">K:{\mathcal{X}\times\mathcal{X}}\longrightarrow \mathbb{R}</span> √© um <strong>Kernel de Mercer</strong> se ele satisfaz √†s condi√ß√µes que seguem:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p><span class="red">Simetria</span>: <span class="math inline">K({\bf x}_a, {\bf x}_b) = K({\bf x}_b, {\bf x}_a)</span>, para todo <span class="math inline">{\bf x}_a,{\bf x}_b \in \mathcal{X}</span>;</p></li>
<li class="fragment"><p><span class="red">Positivo semi-definido</span>: a matriz <span class="math inline">\big[K({\bf x}_a,{\bf x}_b)\big]_{i,j = 1}^n</span> √© positiva semi-definida para todo <span class="math inline">n \in \mathbb{N}</span> e para todo <span class="math inline">{\bf x}_1,\cdots, {\bf x}_n \in \mathcal{X}</span>.</p></li>
</ol>
<p><br></p>
<p>Ser positiva semi-definida, significa que para qualquer sequ√™ncia <span class="math inline">c_r \in \mathbb{R}\,, \forall r = 1, \cdots, n</span>, temos que</p>
<p><span class="math display">\sum_{i = 1}^n\sum_{k = 1}^n c_i c_k K({\bf x}_i,{\bf x}_k)\geq 0,\, \forall n \geq 2.</span></p>
</section>
<section id="suport-vector-regression-machine-4" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>Alguns kernels de Mercer comuns s√£o:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Kernel Linear</strong>: <span class="math inline">K({\bf x}_i, {\bf x}_l) = {\bf x}_i^T{\bf x}_l</span>;</li>
<li class="fragment"><strong>Kernel Polinomial de grau</strong> <span class="math inline">p</span>: <span class="math inline">K({\bf x}_i, {\bf x}_l) = (1 + \langle{\bf x}_i, {\bf x}_l\rangle)^p, \gamma &gt; 0, \theta \geq 0, p \in \mathbb{N}</span>;</li>
<li class="fragment"><strong>Kernel Gaussiano</strong>: <span class="math inline">K({\bf x}_i, {\bf x}_l) = \exp\left\{-\frac{d^2({\bf x}_i, {\bf x}_l)}{2h^2}\right\},\, h &gt; 0</span>;</li>
<li class="fragment"><strong>Kernel Laplaciano</strong>: <span class="math inline">K({\bf x}_i, {\bf x}_l) = \mathrm{e}^{-\gamma d({\bf x}_i, {\bf x}_l)}\,, \gamma &gt; 0</span>;</li>
<li class="fragment"><strong>Kernel Sigm√≥ide</strong>: <span class="math inline">K({\bf x}_i, {\bf x}_l) = \tanh(\gamma {\bf x}_i^T{\bf x}_l + \theta), \, \gamma&gt;0, \theta&gt;0</span>.</li>
</ol>
<p><br></p>
<p>em que <span class="math inline">\gamma, h, \theta</span> e <span class="math inline">p</span>, s√£o par√¢metros do kernel.</p>
</section>
<section id="suport-vector-regression-machine-5" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>A ideia principal por tr√°s dos m√©todos que fazem uso de kernel √© fazer o uso de um mapeamento n√£o-linear arbitr√°rio <span class="math inline">\phi</span> do espa√ßo original dos padr√µes de entrada para um espa√ßo de mais alta dimens√£o, <span class="math inline">\mathcal{X} \times \mathcal{X}</span> chamado de espa√ßo de caracter√≠sticas.</p>
<p><br></p>
<p>Um conjunto de padr√µes que entrada, em um problema de classifica√ß√£o, por exemplo, que n√£o √© linearmente separ√°vel poder√° se tornar linearmente separ√°vel atrav√©s desse mapeamento n√£o linear.</p>
<p><br></p>
<p>Em um espa√ßo vetorial, o <strong>produto interno</strong>, <span class="math inline">\langle{\bf x}_i,{\bf x}_j\rangle = {\bf x}_i^{T}{\bf x}_j</span>, normalmente √© utilizado como medida de similaridade entre vetores. Por√©m, como n√£o conhecemos <span class="math inline">\phi(\cdot)</span>, n√£o √© poss√≠vel realizar <span class="math inline">\phi({\bf x}_i)^T\phi({\bf x}_j)</span> em <span class="math inline">\mathcal{X}\times\mathcal{X}.</span> Aparentemente √© bem complicado calcular <span class="math inline">\langle\phi({\bf x}_i),\phi({\bf x}_j)\rangle</span>, sem conhecer <span class="math inline">\phi(\cdot)</span>, n√£o √© verdade?!</p>

<img data-src="gifs/ok.gif" class="r-stretch"></section>
<section id="suport-vector-regression-machine-6" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>A ess√™ncia dos m√©todos m√©todos baseado em kernel, √© que n√£o precisamos conhecer <span class="math inline">\phi(\cdot)</span>. Os produtos internos no espa√ßo de caracter√≠sticas poder√° ser calculado utilizando um kernel de Mercer.</p>
<p><br></p>
<p><span class="red">Teorema de Mercer</span>: Todo kernel de Mercer <span class="math inline">K</span> pode ser decomposto como</p>
<p><span id="eq-mercer-decomposicao"><span class="math display">K({\bf x}_a,{\bf x}_b) = \sum_{i \geq 0} \gamma_i \phi_i({\bf x}_a)\phi_i({\bf x}_b), \tag{6}</span></span> em que <span class="math inline">\sum_{i \geq 0} \gamma_i^2 &lt; \infty</span> e <span class="math inline">\phi_0, \phi_1, \cdots</span> √© uma sequ√™ncia de fun√ß√µes. Essa propriedade dos em que o kernel de Mercer poder√° ser decomposto na forma acima e que n√£o precisamos conhecer <span class="math inline">\phi(\cdot)</span> para o c√°lculo de produtos internos no espa√ßo de caracter√≠sticas √© comumente denominada de <span class="red"><em>kernel trick</em></span>/<span class="red">truque kernel</span>. Ver detalhes em <a href="https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.1909.0016">aqui</a>.</p>
</section>
<section id="suport-vector-regression-machine-7" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p><span class="red">Defini√ß√£o</span>: Seja <span class="math inline">K</span> um kernel de Mercer, e sejam <span class="math inline">\phi_i</span> e <span class="math inline">\gamma_i,\, i \geq 0</span>, da forma do teorema acima. Ent√£o,</p>
<p><br></p>
<p><span class="math display">\mathcal{H}_K = \left\{g \in L^2(\mathcal{X}):\,\, existem\,\, (c_i)_{i\geq0}\,\, com\,\,\sum_{i\geq1}\frac{c_i^2}{\gamma_i} &lt; \infty\,\,,\, tais\,\, que\,\, g({\bf x}) = \sum_{i\geq1} c_i\phi_i({\bf x}) \right\}.</span></p>
<p><br></p>
<p>Diremos que <span class="math inline">\mathcal{H}_k</span> √© o <em>Reproducing Kernel Hilbert Space</em> - RKHS associado ao kernel <span class="math inline">K</span>, em que a norma de uma fun√ß√£o <span class="math inline">g({\bf x}) = \sum_{i\geq0}c_i\phi({\bf x})</span> √© definda por</p>
<p><span class="math display">||g||_{\mathcal{H}_k}^2 := \sum_{i \geq 0} c_i^2/\gamma_i.</span></p>
<p><br></p>
<p>Al√©m disso, tem-se que <span class="math inline">\mathcal{H}_K</span> √© <strong>√∫nica</strong> para um dado <span class="math inline">K</span>, muito embora a decomposi√ß√£o dada pela <a href="#/suport-vector-regression-machine-6" class="quarto-xref">Equa√ß√£o&nbsp;6</a> n√£o seja √∫nica.</p>
</section>
<section id="suport-vector-regression-machine-8" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br> O Teorema que segue, frequentemente atribu√≠do a ao artigo Kimeldorf, G. S. &amp; Wahba, G. (1970). <strong>A correspondence between Bayesian estimation on stochastic processes and smoothing by splines</strong>. The Annals of Mathematical Statistics, 495‚Äì502, simplifica o problema de otimizar a fun√ß√£o objetivo dada na <a href="#/suport-vector-regression-machine-1" class="quarto-xref">Equa√ß√£o&nbsp;5</a>.</p>
<p><br></p>
<p><span class="red">Teorema da Representa√ß√£o</span>: Seja <span class="math inline">K</span> um kernel de Mercer correspondente ap RKHS <span class="math inline">\mathcal{H}_K</span>. Considere o conjunto de treinamento <span class="math inline">({\bf x}_1, y_1), \cdots, ({\bf x}_n, y_n)</span> e uma fun√ß√£o de perda arbitr√°ria <span class="math inline">L</span>. Ent√£o, a solu√ß√£o de</p>
<p><br></p>
<p><span id="eq-teorema-representacao"><span class="math display">\argmin_{g \in \mathcal{H}_K} \sum_{k = 1}^n L(g({\bf x}_k), y_k) + \lambda||g||_{\mathcal{H}_K^2}, \tag{7}</span></span> existe, e √© √∫nica, em que</p>
<p><br></p>
<p><span class="math display">g({\bf x}) = \sum_{k=1}^n \alpha_k K({\bf x}_k, {\bf x}),</span> em que <span class="math inline">\alpha_1, \cdots, \alpha_n</span> √© uma sequ√™ncia de valores reais.</p>
</section>
<section id="suport-vector-regression-machine-9" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>√â poss√≠vel desmontrar que a otimiza√ß√£o da <a href="#/suport-vector-regression-machine-8" class="quarto-xref">Equa√ß√£o&nbsp;7</a> poder√° se dar pela otimiza√ß√£o de</p>
<p><br></p>
<p><span class="math display">\argmin_{\alpha_1, \cdots, \alpha_n}\sum_{k = 1}^n L\left(\overbrace{\sum_{i=1}^n\alpha_iK({\bf x}_i, {\bf x})}^{g({\bf x})}, y_k \right) + \lambda\underbrace{\sum_{1 \leq j,k \leq n}\alpha_i\alpha_k K({\bf x}_j, {\bf x}_k)}_{||g||_{\mathcal{H}_K}^2}.</span></p>
<p><br></p>
<p>Portanto, o problema de otimizar a fun√ß√£o objetivo dada na <a href="#/suport-vector-regression-machine-1" class="quarto-xref">Equa√ß√£o&nbsp;5</a> se reduz a encontrar os valores de <span class="math inline">\alpha_1, \cdots, \alpha_n</span> que minimiza a <a href="#/suport-vector-regression-machine-8" class="quarto-xref">Equa√ß√£o&nbsp;7</a>. Portanto, ir√° especificar o kernel <span class="math inline">K</span> e a fun√ß√£o de perda <span class="math inline">L</span>, em que <span class="math inline">\lambda</span> √© um par√¢metro de sintoniza√ß√£o (hiperpar√¢metro) que poder√° ser obtido por uma valida√ß√£o cruzada.</p>
</section>
<section id="suport-vector-regression-machine-10" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>Em se tratando de estimadores de regress√£o, <em>Support Vector Regression Machines</em>, utiliza-se uma fun√ß√£o de perda diferente da quadr√°tica, como definido em Drucker, H., Burges, C. J., Kaufman, L., Smola, A. J. &amp; Vapnik, V. (1997). <strong>Support vector regression machines</strong> em Advances in neural information processing systems.</p>
<p><br></p>
<p>Eles definem a seguinte fun√ß√£o de perda <span class="math inline">L(g({\bf x}_k, y_k)) = (|y_k - g({\bf x}_k)| - \varepsilon)_{+}</span>, que assume o valor 0 se <span class="math inline">|y_k - g({\bf x}_k)| &lt; \varepsilon</span> e assumir√° <span class="math inline">|y_k - g({\bf x}_k)| - \varepsilon</span>, caso contr√°rio.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="suport-vector-regression-machine-11" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p><span class="red">Exemplo</span>: Vamos consirar a base de dados de vinho vermelhoüç∑, dispon√≠veis <a href="https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009">aqui</a>, fa√ßa uma pequena an√°lise explorat√≥ria dos dados. No <a href="https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009">link</a> do Kaggle voc√™ consegue uma explica√ß√£o sobre o que significa cada uma das vari√°veis.</p>
<p><br></p>
 <iframe id="example1" src="reports_code/tufte_svm_regressao.html" style="border: none; width: 100%; height: 90%" frameborder="0"></iframe>
</section>
<section id="exerc√≠cios-6" class="slide level2">
<h2>üìö Exerc√≠cios</h2>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Considere o problema em que o objetivo √© prever a <strong>pontua√ß√£o</strong> (<em>score</em>) / (item 2) do aluno com base em algumas vari√°veis. S√£o elas:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Horas estudadas</strong>: o n√∫mero total de horas gastas estudando por cada aluno;</li>
<li class="fragment"><strong>Pontua√ß√£o</strong>: As notas obtidas pelos alunos em testes anteriores;</li>
<li class="fragment"><strong>Atividades</strong> Extracurriculares: Se o aluno participa de atividades extracurriculares (Sim ou N√£o);</li>
<li class="fragment"><strong>Horas de sono</strong>: o n√∫mero m√©dio de horas de sono que o aluno teve por dia;</li>
<li class="fragment"><strong>Amostras de perguntas praticadas</strong>: O n√∫mero de amostras de perguntas que o aluno praticou.</li>
</ol>
<p><br></p>
<p>Voc√™ poder√° baixar e ter uma descri√ß√£o maior da base de dados clicando <a href="https://www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression">aqui</a>. Avalie o poder preditivo do modelo lasso com o do SVRM. Sua an√°lise deve conter uma an√°lise explorat√≥ria dos dados, dever√° utilizar um esquema de <span class="math inline">k</span>-<em>folds cross-validation</em>, com <span class="math inline">k = 20</span> e considerar um esquema de <em>data splitting</em> na propor√ß√£o <span class="math inline">90\%</span> para treino e <span class="math inline">10\%</span> para teste. Sua an√°lise dever√° estar em um notebook de <a href="https://quarto.org/">quarto</a>, em que voc√™ dever√° comentar cada passo da an√°lise.</p>
</section>
<section id="exerc√≠cios-7" class="slide level2">
<h2>üìö Exerc√≠cios</h2>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Refa√ßa o exerc√≠cio anterior considerando a base de dados de dados pessoais de custos m√©dicos dispon√≠vel <a href="https://www.kaggle.com/datasets/mirichoi0218/insurance">aqui</a>. As informa√ß√µes contidas na base s√£o:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Idade</strong> (<code>age</code>): idade do benefici√°rio principal;</li>
<li class="fragment"><strong>Sexo</strong> (<code>sex</code>): sexo do benefici√°rio;</li>
<li class="fragment"><strong>√çndice de massa corporal - IMC</strong> (<code>bmi</code>): <span class="math inline">IMC = \frac{Peso}{altura^2}</span>, com peso em <span class="math inline">Kg</span> e altura em <span class="math inline">m</span> (metro);</li>
<li class="fragment"><strong>N√∫mero de filhos</strong> (<code>children</code>): n√∫mero de filhos cobertos pelo plano de sa√∫de;</li>
<li class="fragment"><strong>Fumante</strong> (<code>smoker</code>): vari√°vel <em>dummy</em> que informa se o benefici√°rio √© ou n√£o fumante;</li>
<li class="fragment"><strong>Regi√£o</strong> (<code>region</code>): regi√£o dos EUA em que o benefici√°rio reside (<em>northeast</em>, <em>southeast</em>, <em>southwest</em> ou <em>northwest</em>);</li>
<li class="fragment"><strong>Custo</strong> (<code>charges</code>): custos m√©dicos cobrados, em d√≥lares.</li>
</ol>
<p><br></p>
<p>O objetivo √© prever o custo (<code>charges</code>) com base nas demais informa√ß√µes. <strong>Dica</strong>: utilizando a biblioteca <a href="https://recipes.tidymodels.org/">recipes</a>, utilize a fun√ß√£o <code>step_dummy</code> especificando o argumento <code>one_hot = TRUE</code> para realizar um <em>one hot encoding</em> com a vari√°vel <code>region</code>. Dessa forma, <code>region</code> deixar√° ser ser uma vari√°vel nominal e se tornar√° uma vari√°vel num√©rica.</p>
</section>
<section id="√°rvores-de-regress√£o" class="slide level2">
<h2>üå≥ √Årvores de regress√£o</h2>
<p><br></p>
<p>Em aprendizagem de m√°quina, uma arvore de regress√£o consiste em uma metodologia n√£o-param√©trica que nos leva a resultados que s√£o facilmente interpret√°veis. A √°rvore √© construirda por meio de particionamentos recursivos no espa√ßo das covari√°veis. Cada parti√ß√£o recebe o nome de <span class="red">n√≥</span> e o resultado final (valor da regress√£o) √© denominado de <span class="red">folha</span> üçÉ.</p>
<p><br></p>
<p><span class="red">Exemplo</span>: Construindo uma √°rvore de regress√£o, usando a biblioteca <a href="https://cran.r-project.org/web/packages/rpart/index.html">rpart</a> para constru√ß√£o da √°rvore e a biblioteca <a href="https://cran.r-project.org/web/packages/rpart.plot/index.html">rpart.plot</a> para a plotagem da √°rvore estimada.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb18-3"><a href="#cb18-3"></a></span>
<span id="cb18-4"><a href="#cb18-4"></a>dados <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(<span class="at">file =</span> <span class="st">"dados/winequality-red.csv"</span>, <span class="at">show_col_types =</span> <span class="cn">FALSE</span>)</span>
<span id="cb18-5"><a href="#cb18-5"></a>arvore <span class="ot">&lt;-</span>  rpart<span class="sc">::</span><span class="fu">rpart</span>(<span class="at">formula =</span> quality <span class="sc">~</span> ., <span class="at">data =</span> dados)</span>
<span id="cb18-6"><a href="#cb18-6"></a><span class="fu">rpart.plot</span>(arvore, <span class="at">type =</span> <span class="dv">4</span>, <span class="at">extra =</span> <span class="dv">1</span>)</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="index_files/figure-revealjs/unnamed-chunk-19-1.png" width="960" class="r-stretch"><p>Perceba que h√° um particionamento bin√°rio em cada n√≥ da √°rvore üå≥ e que poderemos sequir para a aresta da esquerda quando a condi√ß√£o no n√≥ for verdadeira e para direita caso contr√°rio. Dada uma nova observa√ß√£o <span class="math inline">{\bf x}_i</span>, poderemos seguir as condi√ß√µes da √°rvore at√© chegar a uma folha üçÉ dessa √°rvore. Nesse caso, a folha üçÉ cont√©m uma estimativa da qualidade do vinho.</p>
<p><br></p>
<p>Por exemplo, na √°rvore acima podemos perceber que vinhos com teor alco√≥lico inferior √† 11 nos conduzem a vinhos üç∑ de qualidade inferior. Temos que vinhos com teor alco√≥lico maior que 11 e com sulfato maior que 0.65 nos levam √† √≥timos vinhos, segundo o conjunto de dados utilizado.</p>
<p><br></p>
<p>Perceba que dado <span class="math inline">{\bf x}_i</span>, poderemos percorrer a √°rvore a m√£o.</p>
</section>
<section id="√°rvores-de-regress√£o-1" class="slide level2">
<h2>üå≥ √Årvores de regress√£o</h2>
<p><br></p>
<p>Formalmente, temos que a metodologia de estima√ß√£o de uma √°rvore de regress√£o cria uma parti√ß√£o no espa√ßo das covari√°veis em regi√µes distintas e disjuntas, que denotaremos de <span class="math inline">R_1, R_2, \cdots, R_j</span>, com <span class="math inline">R_a \cap R_b</span>, <span class="math inline">\forall a,b \in 1, \cdots, j</span>. A predi√ß√£o √© dada por:</p>
<p><span class="math display">g({\bf x}) = \frac{1}{|\{i:{\bf x}_i \in R_k\}|}\sum_{i:{\bf x}_i \in R_k} y_i.</span></p>
<p><br></p>
<p><strong>A constru√ß√£o de uma √°rvore de regress√£o envolve dois passos principais</strong>:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>A cria√ß√£o de uma √°rvore complexa que nos leve a parti√ß√µes ‚Äúpuras‚Äù, i.e., a parti√ß√µes nas observa√ß√µes do conjunto de covari√°veis que nos leve a valores de <span class="math inline">Y</span>, no conjunto de treinamento em cada uma das folhas sejam homog√™neas;</p></li>
<li class="fragment"><p>Podar a √°rvore, com a finalidade de evitarmos super-ajuste (<em>overffiting</em>), e portanto, termos uma alta vari√¢ncia do modelo.</p></li>
</ol>
</section>
<section id="√°rvores-de-regress√£o-2" class="slide level2">
<h2>üå≥ √Årvores de regress√£o</h2>
<p><br></p>
<p>A ideia de uma √°rvore üå≥ pura √© de que ‚Äútodo mundo‚Äù que cai em uma dada folha √© muito homog√™neo em rela√ß√£o a vari√°vel resposta (ao r√≥tulo/<em>label</em>).</p>
<p><br></p>
<p>Em uma √°rvore de regress√£o, a maneira mais simples de definir pureza √© utilizar o EQM.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="√°rvores-de-regress√£o-3" class="slide level2">
<h2>üå≥ √Årvores de regress√£o</h2>
<p><br></p>
<p>No passo 1, para avaliarmos o qu√£o razo√°vel √© uma √°rvore <span class="math inline">T</span>, utilizamos o Erro Quadr√°tico M√©dio - EQM, em que</p>
<p><span class="math display">\mathcal{P}(T) = \sum_{R}\sum_{i:{\bf x}_i \in R} \frac{(y_i - \widehat{y}_R)^2}{n},</span> em que <span class="math inline">\widehat{y}_R</span> √© o valor predito de <span class="math inline">y</span> para a resposta de uma observa√ß√£o pertencente √† regi√£o <span class="math inline">R</span>. No gr√°fico de uma √°rvore de regress√£o, o <span class="math inline">R</span> √© dado por todos os indiv√≠duos na base de dados que est√£o em uma dada folha üçÉ.</p>
<p><br></p>
<p>Encontrar uma √°rvore <span class="math inline">T</span> que minimize <span class="math inline">\mathcal{P}(T)</span> √© uma tarefa computacionalmente cara. Por isso, que os algoritmos de estima√ß√£o de <span class="math inline">T</span> normalmente utilizam parti√ß√µes bin√°rias, como no exemplo anterior.</p>
<p><br></p>
<p>Existem diversos algoritmos utilizados para estima√ß√£o de <span class="math inline">T</span>, em que o <span class="red"><em>C</em></span><em>lassification <span class="red">A</span>nd <span class="red">R</span>egression <span class="red">T</span>ree</em> - <span class="red">CART</span> √© o mais conhecido. O algoritmo foi estabelecido no livro Breiman L., Friedman J. H., Olshen R. A., and Stone, C. J. (1984) <strong>Classification and Regression Trees</strong>. Wadsworth.</p>
</section>
<section id="√°rvores-de-regress√£o-4" class="slide level2">
<h2>üå≥ √Årvores de regress√£o</h2>
<p><br></p>
<p>O algoritmo particiona o espa√ßo de covari√°veis em duas regi√µes disjuntas. Para a escolha dessa parti√ß√£o, busca-se, dentre todas as covari√°veis <span class="math inline">x_i</span> e cortes <span class="math inline">t_1</span>, a combina√ß√£o que conduz a uma parti√ß√£o <span class="math inline">(R_1, R_2)</span> com menor predi√ß√µes de erro quadr√°tico, i.e., dado um n√≥, a parti√ß√£o √© constru√≠da de modo a minimizar:</p>
<p><span id="eq-sse-tree"><span class="math display">\overbrace{SSE}^{\text{sum of squares error}} = \sum_{i:{\bf x}_i \in R_1}^n (y_i - \widehat{y}_{R_1})^2 + \sum_{i:{\bf x}_i \in R_2}^n (y_i - \widehat{y}_{R_2})^2, \tag{8}</span></span> em que <span class="math inline">\widehat{y}_{R_k}</span> √© a predi√ß√£o de <span class="math inline">y</span> fornecida pela regi√£o <span class="math inline">R_k</span> e SSE √© denominado <span class="red"><em>S</em></span><em>um of <span class="red">S</span>quares of <span class="red">E</span>rrors</em> - SSE.</p>
<p><br></p>
<p>Assim, tem-se que o algoritmo ir√° fornecer:</p>
<p><br></p>
<p><span class="math display">R_1 = \{{\bf x} : {\bf x}_i &lt; t_1\}\, \mathrm{e}\, R_2 = \{{\bf x} : {\bf x}_i \geq t_1\},</span> em que <span class="math inline">x_i</span> √© a vari√°vel escolhida e <span class="math inline">t_1</span> √© o corte definido.</p>
</section>
<section id="√°rvores-de-regress√£o-5" class="slide level2">
<h2>üå≥ √Årvores de regress√£o</h2>
<p><br></p>
<p>Uma vez que estabelecemos um n√≥ raiz, este √© fixado. No passo seguinte, o algoritmo ir√° particionar as regi√µes <span class="math inline">R_1</span> e <span class="math inline">R_2</span> em regi√µes menores, seguindo o mesmo crit√©rio,tanto para <span class="math inline">R_1</span> quanto para <span class="math inline">R_2</span>. O algoritmo continua de forma recursiva at√© que tenhamos uma √°rvore com poucas observa√ß√µes em uma das folhas üçÉ.</p>
<p><br></p>
<p>Por exemplo, podemos decidir em parar de tornar a √°rvore profunda quando em cada folha tivermos menos de 5 observa√ß√µes. Por√©m, essa √°rvore criada ir√° produzir boas predi√ß√µes para o conjunto de treinamento, por√©m, n√£o ir√° performar bem em novas observa√ß√µes. Isso, por conta, do <em>trade off</em> entre vi√©s e vari√¢ncia. Em outras palavas, haver√° <em>overfitting</em>.</p>
<p><br> Na etapa do processo de poda, cada n√≥ √© retirado, um por vez, e observa-se como o erro de predi√ß√£o varia no conjunto de valida√ß√£o. Com base nisso, decide-se quais n√≥s permanecem na √°rvore. O processo de poda reduz o <em>overffiting</em> do modelo.</p>
</section>
<section id="√°rvores-de-regress√£o-6" class="slide level2">
<h2>üå≥ √Årvores de regress√£o</h2>
<p><br></p>
<p>Uma observa√ß√£o importante √© que sempre √© bom crescer a √°rvore ao m√°ximo poss√≠vel e depois proceder com a etapa de ‚Äúpoda‚Äù, em que removemos os ramos mais profundos e reavaliamos o poder preditivo da √°rvore üå≥ <span class="math inline">T</span>. Isso porqu√™ a melhoria do poder preditivo da √°rvore n√£o √© linear, i.e., as vezes podemos ter uma divis√£o que piore um pouco a capacidade preditiva da √°rvore, por√©m, a divis√£o seguinte poder√° dar um grande salto de melhoria. Dessa forma, √© mais conveniente deixar a √°rvore ‚Äúprofunda‚Äù para depois sairmos ‚Äúpodando‚Äù ‚úÇÔ∏è a √°rvore üå≥.</p>
<p><br></p>

<img data-src="gifs/cortando-a-arvore-fail.gif" class="r-stretch"></section>
<section id="√°rvores-de-regress√£o-7" class="slide level2">
<h2>üå≥ √Årvores de regress√£o</h2>
<p><br></p>
<p>Para limitar a profundidade da arvore <span class="math inline">T</span> a ser estimada, um par√¢metro de penaliza√ß√£o/complexidade poder√° ser introduzido na <a href="#/√°rvores-de-regress√£o-4" class="quarto-xref">Equa√ß√£o&nbsp;8</a>. Assim, o problema consistem em obter regi√µes e pontos de cortes que minimize:</p>
<p><span class="math display">SSE + \alpha|T|,</span> em que <span class="math inline">\alpha &gt; 0</span> √© o hiperpar√¢metro de complexidade do modelo e <span class="math inline">|T|</span> √© um valor inteiro que define a profundidade m√°xima da √°rvore. Por exemplo, <span class="math inline">|T|</span> na biblioteca <a href="https://cran.r-project.org/web/packages/rpart/index.html">rpart</a> √© definido pelo par√¢metro <code>tree_depth</code> em 30. Normalmente nos concentramos em em tunar o par√¢metro <code>cost_complexity</code> que √© o <span class="math inline">\alpha</span>.</p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="√°rvores-de-regress√£o-8" class="slide level2">
<h2>üå≥ √Årvores de regress√£o</h2>
<p><br></p>
<p><span class="red">Exemplo</span>: Poderemos podar a √°rvore usando a fun√ß√£o <code>prune</code> do pacote <a href="https://cran.r-project.org/web/packages/rpart/index.html">rpart</a>. Considerando o exemplo anterior, tornando a √°rvore menos comple√ßa, poder√≠amos decidir em podar alterando o seu par√¢metro de complexidade.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb19-2"><a href="#cb19-2"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb19-3"><a href="#cb19-3"></a></span>
<span id="cb19-4"><a href="#cb19-4"></a>dados <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(<span class="at">file =</span> <span class="st">"dados/winequality-red.csv"</span>)</span>
<span id="cb19-5"><a href="#cb19-5"></a></span>
<span id="cb19-6"><a href="#cb19-6"></a>arvore <span class="ot">&lt;-</span> dados <span class="sc">|&gt;</span> </span>
<span id="cb19-7"><a href="#cb19-7"></a>  rpart<span class="sc">::</span><span class="fu">rpart</span>(<span class="at">formula =</span> quality <span class="sc">~</span> ., <span class="at">data =</span> _) </span>
<span id="cb19-8"><a href="#cb19-8"></a></span>
<span id="cb19-9"><a href="#cb19-9"></a><span class="co"># O melhor par√¢metro de custo</span></span>
<span id="cb19-10"><a href="#cb19-10"></a>melhor_grau <span class="ot">&lt;-</span> arvore<span class="sc">$</span>cptable[<span class="fu">nrow</span>(arvore<span class="sc">$</span>cptable),][1L]</span>
<span id="cb19-11"><a href="#cb19-11"></a></span>
<span id="cb19-12"><a href="#cb19-12"></a><span class="co"># Aumentando o par√¢metro de custo de 0.01 para 0.04</span></span>
<span id="cb19-13"><a href="#cb19-13"></a>arvore_podada <span class="ot">&lt;-</span> rpart<span class="sc">::</span><span class="fu">prune</span>(<span class="at">tree =</span> arvore, <span class="at">cp =</span> <span class="fl">0.04</span>)</span>
<span id="cb19-14"><a href="#cb19-14"></a></span>
<span id="cb19-15"><a href="#cb19-15"></a><span class="co"># Plotando a √°rvore podada</span></span>
<span id="cb19-16"><a href="#cb19-16"></a>rpart.plot<span class="sc">::</span><span class="fu">rpart.plot</span>(arvore_podada)</span></code><button title="Copiar para a √°rea de transfer√™ncia" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="index_files/figure-revealjs/unnamed-chunk-20-1.png" width="960" class="r-stretch"></section>
<section id="√°rvores-de-regress√£o-9" class="slide level2">
<h2>üå≥ √Årvores de regress√£o</h2>
<p><br> O par√¢metro de complexidade/custo √© um hiperpar√¢metro, e dever√° ser estimado dentro de um procedimento de valida√ß√£o cruzada.</p>
<p><br></p>

<img data-src="gifs/bom.gif" class="r-stretch"></section>
<section id="√°rvores-de-regress√£o-10" class="slide level2">
<h2>üå≥ √Årvores de regress√£o</h2>
<p><br> <strong>Algumas observa√ß√µes sobre a √°rvore üå≥ de regress√£o</strong>:</p>
<p><br></p>
<ol type="1">
<li class="fragment">√â um m√©todo n√£o-param√©trico;</li>
<li class="fragment">Pode ser representado graficamente;</li>
<li class="fragment">√â √∫til na an√°lise explorat√≥ria dos dados do problema em quest√£o;</li>
<li class="fragment">Pode ser utilizada para selecionar vari√°veis. Aparentemente, as vari√°veis que pertence √† √°rvore tem uma maior import√¢ncia para o problema em quest√£o;</li>
<li class="fragment">Poder√° trabalhar com vari√°veis num√©ricas, mas tamb√©m poder√° trabalhar com vari√°veis categ√≥ricas;</li>
<li class="fragment">A √°rvore √© robusta na presen√ßa de vari√°veis irrelevantes.</li>
</ol>
<p><br></p>

<img data-src="gifs/chapulin-colorado-no.gif" class="r-stretch"></section>
<section id="√°rvores-de-regress√£o-11" class="slide level2">
<h2>üå≥ √Årvores de regress√£o</h2>
<p><br></p>
<p>O n√≠vel de complexidade √© obtido via <em>cross-validation</em> de modo a encontrar a menor sub√°rvore que generaliza melhor o problema para dados n√£o visto.</p>
<p><br></p>
<p>Assim como nos modelos de regress√£o com penalidade que vimos anteriormente, aqui, para valores menores de <span class="math inline">\alpha</span> tende a produzir modelos mais complexos. Consequentemente, √† medida que uma √°rvore cresce, a redu√ß√£o no <span class="math inline">SSE</span> deve ser maior do que a penalidade de complexidade de custo.</p>
<p><br></p>

<img data-src="gifs/chaves-isso.gif" class="r-stretch"></section>
<section id="√°rvores-de-regress√£o-12" class="slide level2">
<h2>üå≥ √Årvores de regress√£o</h2>
<p><br></p>
<p>Experimente alterar o par√¢metro de complexidade e perceba como ele influencia nas regi√ß√µes da √°rvore de regress√£o. Quando maior o valor, maior a penalidade, e portanto, mais simples ser√° a √°rvore de regress√£o que ir√° estimar os valores de <span class="math inline">Y_i</span> com base em <span class="math inline">X_i</span>.</p>
<p><br></p>
 <iframe id="example1" src="https://pedro-rafael.shinyapps.io/shiny_apps/#section-%C3%A1rvore-de-regress%C3%A3o" style="border: none; width: 100%; height: 90%" frameborder="0"></iframe>

<img data-src="gifs/giphy.gif" class="r-stretch"></section>
<section id="√°rvores-de-regress√£o-13" class="slide level2">
<h2>üå≥ √Årvores de regress√£o</h2>
<p><br></p>
<p><strong>Algumas outras caracter√≠sticas das üå≥ √°rvores de regress√£o s√£o</strong>:</p>
<p><br></p>
<ol type="1">
<li class="fragment">A fun√ß√£o de regress√£o estimada <span class="math inline">\widehat{r}({\bf x})</span> √© <strong>constante por partes</strong>, i.e., em uma folha üçÉ tendemos predizer que v√°rios indiv√≠duos distintos tem o mesmo valor de <span class="math inline">\widehat{r}({\bf x})</span>;</li>
<li class="fragment">Em uma √°rvore üå≥ de regress√£o as intera√ß√µes entre vari√°veis s√£o consideradas de forma autom√°tica, enquanto em uma regress√£o as intera√ß√µes s√£o introduzidas como os produtos entre covari√°veis;</li>
<li class="fragment">Elas s√£o uma esp√©cie de ‚Äúsamambaias‚Äù, em que crescem para baixo;</li>
<li class="fragment">√â f√°cil introduzir vari√°veis categ√≥ricas üéâ;</li>
<li class="fragment">Uma pessoa sem muito conhecimento poder√° estimar <span class="math inline">\widehat{r}({\bf x})</span>, dada uma √°rvore, para cada nova observa√ß√£o <span class="math inline">{\bf x}</span>.</li>
</ol>

<img data-src="gifs/hum_02.gif" class="r-stretch"></section>
<section id="exerc√≠cios-8" class="slide level2">
<h2>üìö Exerc√≠cios</h2>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Considere a vari√°vel aleat√≥ria <span class="math inline">Y_i \sim \mathcal{N}(\sin(X_i), \sigma^2)</span>, com <span class="math inline">X_i \in \mathcal{U}(0, 10)\,, \forall i.</span> Utilizando a biblioteca <strong>rpart</strong>, implemente a fun√ß√£o <code>arvore(n = 250L, complexidade, sigma = 0.1)</code> que devolve o gr√°fico <em>scatterplot</em> com os pontos <span class="math inline">X_i</span> e <span class="math inline">Y_i</span> e por cima deles o gr√°fico de linha com os valores preditos. A fun√ß√£o dever√° ter tr√™s argumentos, <code>n</code>, <code>complexidade</code> e <code>sigma</code>, que s√£o o tamanho da amostra, o grau de complexidade do modelo, e o desvio padr√£o, respectivamente. Aqui n√£o se preocupe com divis√£o entre treino e teste nem valida√ß√£o cruzada. A solu√ß√£o desse exerc√≠cio n√£o tem como objetivo encontrar o melhor hiperpar√¢metro, i.e., n√£o √© necess√°rio ‚Äútunar‚Äù o par√¢metro <code>complexidade</code>. A fun√ß√£o dever√° retornar algo como:</p>

<img data-src="index_files/figure-revealjs/unnamed-chunk-21-1.png" width="960" class="r-stretch"></section>
<section id="exerc√≠cios-9" class="slide level2">
<h2>üìö Exerc√≠cios</h2>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Ainda com base no exerc√≠cio anterior, construa uma fun√ß√£o que retorne v√°rias estimativas obtidas por √°rvores de regress√£o, com base em v√°rias amostra de <span class="math inline">X_i</span> e <span class="math inline">Y_i</span>. O gr√°fico dever√° mostrar as estimativas das diversas √°rvores de regress√£o, sem mostrar os pontos. Perceba a flutua√ß√£o das estimativas em diferentes valores de <code>complexidade</code>, dado <code>n</code> e <code>sigma</code> fixos.</p>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Sem utilizar a biblioteca <strong>tidymodels</strong>, apenas as bibliotecas <strong>rsample</strong> e <strong>rpart</strong>, treine um modelo com 10 mil observa√ß√µes geradas. No procedimento <span class="math inline">k</span>-folds cross-validation, para <span class="math inline">k = 20</span>, encontre um bom valor para o grau de complexidade considerando um <em>grid</em> de poss√≠veis valores. <em>Dica</em>: experimente testar um par√¢metro dentro do conjunto de treino no procedimento de <em>cross-validation</em>. Por exemplo, experimente criar um <em>grid</em> com valores entre <span class="math inline">0.001</span> e <span class="math inline">0.4</span>. Aprensente o gr√°fico com a estimativa do melhor modelo. N√£o esque√ßa de fixar um valor de semente, para que os resultados possam ser reproduzidos.</p>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Refa√ßa o exerc√≠cio anterior utilizando a biblioteca <strong>tidymodels</strong>. Fique livre para tunar o hiperpar√¢metro que achar necess√°rio, da <em>engine</em> que utilizar com a biblioteca <strong>parsnip</strong>. Compare o risco preditivo do exerc√≠cio anterior com o que voc√™ obteve utilizando o <strong>tidymodels</strong>.</p>
</section>
<section id="combinando-predi√ß√µes---bagging" class="slide level2">
<h2>Combinando predi√ß√µes - bagging</h2>
<p><br></p>
<p>As √°rvores üå≥ de regress√£o tem a caracter√≠stica de ser facilmente interpret√°vel, por√©m, costumam ser uma capacidade preditiva baixa.</p>
<p><br></p>
<p>Uma das ideias de melhorar a capacidade preditiva √© combinar √°rvores usando a metodologia de reamostragem <em>bootstrap</em>. As t√©cnicas de reamostragem via bootstrap n√£o-param√©trico s√£o bastante difundidas na estat√≠stica e consiste reamostrar da amostra original com reposi√ß√£o.</p>
<p><br></p>
<p>Na estat√≠stica, a ideia de <em>bootstrap</em> √© muito comum para corre√ß√£o de v√≠es de um estimador, c√°lculo do erro-padr√£o de um estimador, constru√ß√£o de intervalos de confian√ßas e teste de hip√≥teses.</p>
<p><br></p>
<p>Em aprendizagem de m√°quina, o conceito de <em>bagging</em> consiste em reaplicar uma metodologia, nesse caso a de √°rvore üå≥ de regress√£o em diferentes amostras obtidas da amostra original com reposi√ß√£o.</p>
<p><br></p>

<img data-src="gifs/laughing-ha.gif" class="r-stretch"></section>
<section id="combinando-predi√ß√µes---bagging-1" class="slide level2">
<h2>Combinando predi√ß√µes - bagging</h2>
<p><br> Caso voc√™ tenha interesse e entender o m√©todo de <em>bootstrap</em>, no contexto mais difundido na estat√≠stica como mencionado no slide anterior, assista a v√≠deo aula sobre bootstrap do Prof.&nbsp;Pedro Rafael do Departamento de Estat√≠stica da Universidade Federal da Para√≠ba - UFPB.</p>
<p><br></p>
<iframe data-external="1" src="https://www.youtube.com/embed/XEQlzfVc6lI" width="50%" height="50%" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="combinando-predi√ß√µes---bagging-2" class="slide level2">
<h2>Combinando predi√ß√µes - bagging</h2>
<p><br></p>
<p>Portanto, √© importante ficar claro que o <em>bagging</em> √© uma metodologia gen√©rica que poder√° ser aplicada em situa√ß√µes ao qual desejamos combinar modelos. Por exemplo, o m√©todo <a href="https://en.wikipedia.org/wiki/Random_forest">ramdom forest</a> que veremos mais a frente √© uma pequena modifica√ß√£o de um <em>bagging</em> de √°rvores de regress√£o que vimos anteriormente. Na verdade o <a href="https://en.wikipedia.org/wiki/Random_forest">ramdom forest</a>, assim como as √°rvores de decis√µes podem ser utilizadas tamb√©m para problemas de classifica√ß√£o, como veremos mais adiante no curso.</p>
<p><br></p>

<img data-src="gifs/ok-2.gif" class="r-stretch"></section>
<section id="combinando-predi√ß√µes---bagging-3" class="slide level2">
<h2>Combinando predi√ß√µes - bagging</h2>
<p><br></p>
<p>A ideia de combinar √°rvores de regress√£o √© interessante, uma vez que o risco preditivo da m√©dia das previs√µes das √°rvores √© menor que o risco individual de cada uma das √°rvores.</p>
<p><br></p>
<p>Lembre-se que quando fal√°vamos em momentos anteriores do curso sobre o balan√ßo entre vi√©s e vari√¢ncia, apresentamos uma decomposi√ß√£o do risco quadr√°tico <span class="math inline">R(g)</span> condicional a um novo <span class="math inline">{\bf x}</span> dada na <a href="#/balan√ßo-vi√©s-e-vari√¢ncia-1" class="quarto-xref">Equa√ß√£o&nbsp;2</a>. Para que n√£o seja necess√°rio necess√°rio voltar um grande n√∫mero de slides, recoloco a decomposi√ß√£o abaixo:</p>
<p><br></p>
<p><span class="math display">\mathbb{E}\left[(Y - \widehat{g}({\bf X}))^2| {\bf X} = {\bf x}\right] = \underbrace{\mathbb{V}[Y | {\bf X = x}]}_{\mathrm{i - Vari√¢ncia\,\, intr√≠nseca}} + \overbrace{(r({\bf x}) - \mathbb{E}[\widehat{g}({\bf x})])^2}^{\mathrm{ii - Vi√©s\, ao\, quadrado\, do\, modelo}} + \underbrace{\mathbb{V}[\widehat{g}({\bf x})]}_{\mathrm{iii - Vari√¢ncia\, do\, modelo}}.</span></p>
</section>
<section id="combinando-predi√ß√µes---bagging-4" class="slide level2">
<h2>Combinando predi√ß√µes - bagging</h2>
<p><br></p>
<p>Ainda no caso de √°rvores üå≥ de regress√£o, suponha um caso mais simples, em que temos dois modelos de √°rvores de regress√£o, sejam eles <span class="math inline">g_1</span> e <span class="math inline">g_2</span>, respectivamente, em que a previs√£o combinada √© dada por:</p>
<p><span class="math display">\widehat{g}({\bf x}) = \frac{\widehat{g_1}({\bf x})  + \widehat{g_2}({\bf x})}{2},</span> em que <span class="math inline">\widehat{g_1}({\bf x})</span> e <span class="math inline">\widehat{g_2}({\bf x})</span> s√£o as estimativas da fun√ß√£o de regress√£o <span class="math inline">r({\bf x})</span> fornecidades pelas √°rvores <span class="math inline">g_1</span> e <span class="math inline">g_2</span>, respectivamente.</p>
<p><br></p>
<p>Supondo que <span class="math inline">\widehat{g_1}({\bf x})</span> e <span class="math inline">\widehat{g_2}({\bf x})</span> s√£o n√£o-viesados e possuem a mesma vari√¢ncia, e al√©m disso s√£o n√£o correlacionados ent√£o:</p>
<span class="math display">\begin{align*}
\mathbb{E}[(Y - g({\bf x}))^2|{\bf x}] &amp;= \mathbb{V}[Y|{\bf x}] + \frac{1}{4}(\mathbb{V}[\widehat{g}_1({\bf x}) + \widehat{g}_2({\bf x})|{\bf x}]) \\
&amp;\quad + \left(\mathbb{E}[Y|{\bf x}] - \frac{\mathbb{E}[\widehat{g}_1({\bf x})|{\bf x}]+\mathbb{E}[\widehat{g}_2({\bf x})|{\bf x}]}{2} \right)^2
\end{align*}</span>
</section>
<section id="combinando-predi√ß√µes---bagging-5" class="slide level2">
<h2>Combinando predi√ß√µes - bagging</h2>
<p><br></p>
<p>Note que se <span class="math inline">\widehat{g_1}({\bf x})</span> e <span class="math inline">\widehat{g_2}({\bf x})</span> ent√£o <span class="math inline">\mathbb{E}(\widehat{g}_1({\bf x})|{\bf x}) = \mathbb{E}(\widehat{g}_2({\bf x})|{\bf x}) = r({\bf x})</span>. Isso faz com que a √∫ltima parcela da equa√ß√£o anterior seja zero. Se al√©m disso, se os estimadores possuem a mesma vari√¢ncia, ent√£o a decomposi√ß√£o do risco preditivo combinado √© simplificada e dada por:</p>
<p><span id="eq-risco-combinado-decom"><span class="math display">\overbrace{\mathbb{E}[(Y - \widehat{g}({\bf x}))^2|{\bf x}]}^{\text{Risco preditivo combinado}} = \mathbb{V}[Y|{\bf x}] + \frac{1}{2}\mathbb{V}[\widehat{g}_i({\bf x})| {\bf x}] \leq \underbrace{\mathbb{E}[(Y - \widehat{g}_i({\bf x}))^2 | {\bf x}]}_{\text{Risco preditivo individual}}, \tag{9}</span></span> para um dado <span class="math inline">i</span>, com <span class="math inline">i = 1, 2</span>. Dado as suposi√ß√µes de estimadores n√£o-viesados, vari√¢ncia iguais e que os estimadores s√£o n√£o-correlacionados, a <a href="#/combinando-predi√ß√µes---bagging-5" class="quarto-xref">Equa√ß√£o&nbsp;9</a> poderia ser generalizada para o caso de mais de dois estimadores, no nosso caso, para mais de duas √°rvores de regress√£o üå≥. Basta utilizar indu√ß√£o matem√°tica!</p>
<p><br></p>

<img data-src="gifs/bom.gif" class="r-stretch"></section>
<section id="combinando-predi√ß√µes---bagging-6" class="slide level2">
<h2>Combinando predi√ß√µes - bagging</h2>
<p><br></p>
<p>Devemos notar, contudo que, para que tenhamos √°rvores üå≥ aproximadamente n√£o-viesadas, n√£o devemos ‚Äúpodar‚Äù as √°rvoresÔ∏è üå≥. Muito embora elas possam aprensenta <em>overfitting</em> quando consideradas individualmente o <strong>risco preditivo combinado ir√° diminuir</strong>.</p>
<p><br></p>
<p>Portanto, seja <span class="math inline">B</span> o n√∫mero de pseudo-amostras <em>bootstrap</em>, i.e., amostras obtidas da amostra original com reposi√ß√£o. Ent√£o, o estimador combinado em um procedimento de <em>bagging</em> √© dado por:</p>
<p><span class="math display">\widehat{g}({\bf x}) = \frac{1}{B}\sum_{b = 1}^B \widehat{g}_b({\bf x}).</span></p>

<img data-src="gifs/bean_01.gif" class="r-stretch"></section>
<section id="combinando-predi√ß√µes---bagging-7" class="slide level2">
<h2>Combinando predi√ß√µes - bagging</h2>
<p><br></p>
<p>Uma observa√ß√£o importante √© que o <em>bagging</em> pode ser ruim em modelos quando utilizado em modelos intrinsecamente est√°veis, como √© o caso de modelos lineares de regress√£o, <span class="math inline">k</span>NN, regress√£o log√≠stica, entre outros. O procedimento de <em>bagging</em> costuma ser eficaz quando s√£o utilizados em modelos que possuem uma alta vari√¢ncia e que tendem a ter <em>overfitting</em>, como o caso da √°rvore de decis√£o profunda (√°rvores de regress√£o e de classifica√ß√£o).</p>
<p><br></p>

<img data-src="gifs/bean_01.gif" class="r-stretch"></section>
<section id="combinando-predi√ß√µes---bagging-8" class="slide level2">
<h2>Combinando predi√ß√µes - bagging</h2>
<p><br></p>
<p>Uma forma de perceber a estabilidade de um modelo de aprendizagem de m√°quina √© avaliar as predi√ß√µes do modelo em diferentes parti√ß√µes do conjunto de dados, por exemplo, em um procedimento de valida√ß√£o cruzada repetida. A fun√ß√£o <code>rsample::vfold_cv()</code>, permite a possibilidade de retepir uma valida√ß√£o cruzada por meio do argumento <code>repeats</code>, que por <em>default</em> √© igual √† <span class="math inline">1</span>.</p>
<p><br></p>
<p>Uma outra forma seria utilizar um procedimento de <em>bootstrap</em> n√£o-param√©trico (reamostrar da amostra com reposi√ß√£o) e treinar o modelo em cada pseudo-amostra <em>bootstrap</em> e avaliar a variabilidade das estimativas no conjunto de valida√ß√£o.</p>
<p><br></p>
<p>Uma outra forma seria avaliar a <strong>curva de aprendizado</strong> do modelo. Essa curva poder√° ser obtida treinando o modelo em diferentes tamanhos de conjunto de treinamento, em que observa-se como varia a peformance do modelo nos diferentes tamanhos do conjunto de treinamento.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="combinando-predi√ß√µes---bagging-9" class="slide level2">
<h2>Combinando predi√ß√µes - bagging</h2>
<p><br></p>
<p>Muito embora a combina√ß√£o preditiva usando <em>bagging</em> de um conjunto de √°rvores de regress√£o n√£o s√£o t√£o f√°ceis de interpretar quando comparada com uma √∫nica √°rvore de regress√£o, ele permite que possamos criar uma medida de import√¢ncia para cada vari√°vel. Essa medida baseia-se na redu√ß√£o da <em>Residual Sum of squares</em> - RSS <strong>de cada divis√£o</strong>.</p>
<p><br></p>

<img data-src="imgs/dendograma_simples.png" class="r-stretch quarto-figure-center"><p class="caption">Uma simples divis√£o bin√°ria em qualquer ponto de uma dada √°rvore üå≥ de regress√£o. Desejamos encontrar a import√¢ncia da vari√°vel ‚Äúpai‚Äù que poder√° aparecer em diversas divis√µes em uma mesma √°rvore.</p><p><br></p>
<p>Devemos computar a redu√ß√£o da soma dos quadrados dos res√≠duos em cada n√≥ em que a vari√°vel do n√≥ ‚Äúpai‚Äù aparece em todas as √°rvores obtidas pelo procedimento de <em>bagging</em>, em que calculamos a soma dos quadrados no n√≥ ‚Äúpai‚Äù e subtra√≠mos da soma dos quadrados do n√≥ esquerdo e do n√≥ direito.</p>
</section>
<section id="combinando-predi√ß√µes---bagging-10" class="slide level2">
<h2>Combinando predi√ß√µes - bagging</h2>
<p><br></p>
<p>Assim, a import√¢ncia da vari√°vel no n√≥ ‚Äúpai‚Äù, em uma dada divis√£o bin√°ria em uma dada √°rvore üå≥ do procedimento de <em>bagging</em> √© dada por:</p>
<p><br></p>
<p><span class="math display">\begin{align*}
\text{Import√¢ncia local} &amp;= RSS_{pai} - RSS_{esq} - RSS_{dir} = \\
&amp; \sum_{i \in pai} (y_i - \overline{y}_{pai})^2 - \sum_{i \in esq} (y_i - \overline{y}_{esq})^2 - \sum_{i \in dir} (y_i - \overline{y}_{dir})^2.
\end{align*}</span></p>
<p><br></p>
<p>Foi denominado de <strong>‚ÄúImport√¢ncia local‚Äù</strong>, uma vez que o n√≥ ‚Äúpai‚Äù (vari√°vel de interesse) poder√° aparecer nas diversas <span class="math inline">B</span> √°rvores de regress√£o do procedimento <em>bagging</em>, e mais, poder√° aparecer v√°rias vezes em uma mesma √°rvore. Portanto, em todas as <span class="math inline">B</span> e em todas as ocorr√™ncias da vari√°vel ‚Äúpai‚Äù em qualquer ponto de uma √°rvore a ‚ÄúImport√¢ncia local‚Äù dever√° ser calculada. Ao fim, dever√° somar todas as import√¢ncias locais para se ter a <strong>import√¢ncia global</strong> da vari√°vel no n√≥ ‚Äúpai‚Äù.</p>
</section>
<section id="exerc√≠cios-10" class="slide level2">
<h2>üìö Exerc√≠cios</h2>
<p><br></p>
<p>Considere a vari√°vel aleat√≥ria <span class="math inline">Y_i \sim \mathcal{N}(\sin(X_i), \sigma^2)</span>, com <span class="math inline">X_i \in \mathcal{U}(0, 10)\,, \forall i</span>.</p>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Avalie a estabilidade de uma √°rvore de regress√£o usando um valida√ß√£o cruzada repetida. Construa um gr√°fico com os riscos observados na valida√ß√£o para um <span class="math inline">\sigma^2</span> fixo.</p>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Avalie a estibilidade usando um procedimento de bootstrap. Construa um gr√°fico com os riscos observados na valida√ß√£o para um <span class="math inline">\sigma^2</span> fixo.</p>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Por fim, avalie a estabilidade utilizando avaliando a curva de aprendizado do modelo.</p>
<p><br></p>

<img data-src="gifs/bean_01.gif" class="r-stretch"></section>
<section id="random-forest" class="slide level2">
<h2>üå≤üêûüå≥üêùüå≤ü¶ãüå≥ Random Forest</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p>O <em>random forest</em> (floresta aleat√≥ria) √© um procedimento de <em>bagging</em> em que introduz um n√≠veo de aleatoriedade maior no processo de sela√ß√£o das vari√°veis, visando reduzir ainda mais a corre√ß√£o entre as √°rvores de regress√£o. Isso √© feito sorteando <span class="math inline">m &lt; d</span> covari√°veis em cada particionamento, i.e., essa randomiza√ß√£o √© feita em toda divis√£o de todas as <span class="math inline">B</span> √°rvores do procedimento de <em>bagging</em>, em que <span class="math inline">d</span> √© o total de covari√°veis consideradas.</p>
<p><br></p>
<p>O valor de <span class="math inline">m</span> poder√° ser obtido via algum procedimento de <em>cross-validation</em>, i.e., √© um hiperpar√¢metro que poder√° ser ‚Äútunado‚Äù.</p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="gifs/forest.gif"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="random-forest-1" class="slide level2">
<h2>üå≤üêûüå≥üêùüå≤ü¶ãüå≥ Random Forest</h2>
<p><br></p>
<p>A ideia de construir √°rvores que sejam menos correlacionadas uma com as outras nos aproxima melhor do crit√©rio de n√£o correla√ß√£o que utilizamos para mostrar que o risco preditivo combinado √© menor que o risco preditivo ao considerar uma √∫nica √°rvore, como mostra a <a href="#/combinando-predi√ß√µes---bagging-5" class="quarto-xref">Equa√ß√£o&nbsp;9</a>.</p>
<p><br></p>
<p>O algoritmo <em>random forest</em> consegue produzir um estimador com menor vari√¢ncia que o <em>bagging</em>. Al√©m disso, assim como no <em>bagging</em>, podemos calcular a import√¢ncia de cada vari√°vel usando o mesmo procedimento apresentado anteriormente.</p>
<p><br></p>
<p>Muito embora o <em>bagging</em> √© um procedimento √∫til para diminuir a vari√¢ncia de um modelo utilizando predi√ß√µes combinadas, o procedimento de <em>random forest</em> para o caso de √°rvores de regress√£o ou de classifica√ß√£o nos conduz a um estimador com menor vari√¢ncia.</p>
<p><br></p>
<p>Um valor de <span class="math inline">m</span> frequentemente considerado √© o valor inteiro que aproxima <span class="math inline">\sqrt{d}</span>.</p>
</section>
<section id="random-forest-2" class="slide level2">
<h2>üå≤üêûüå≥üêùüå≤ü¶ãüå≥ Random Forest</h2>
<p><br></p>
<p>A aplica√ß√£o abaixo permite que voc√™ possa comparar as estrat√©gias de <em>bagging</em> com o <em>random forest</em>, sendo este um <em>bagging</em> de √°rvores de regress√£o com o pequeno ajuste mencionado anteriormente, permitindo termos √°rvores menos correlacionadas. Perceba que o <em>random forest</em> consegue diminuir ainda mais a vari√¢ncia das previs√µes de <span class="math inline">Y</span>. A linha <span class="red">vermelha</span> √© a distribui√ß√£o verdadeira.</p>
<p><br></p>
 <iframe id="example1" src="https://pedro-rafael.shinyapps.io/shiny_apps/#section-bagging-vs-random-forest" style="border: none; width: 100%; height: 90%" frameborder="0"></iframe>
<p><br></p>
<p>Se desejar ver de forma ampliada, acesse a aplica√ß√£o clicando <a href="https://pedro-rafael.shinyapps.io/shiny_apps">aqui</a>.</p>
</section>
<section id="boosting" class="slide level2">
<h2>üöÄ Boosting</h2>
<p><br></p>
<p>Da mesma forma que m√©todos como <em>bagging</em> e <em>random forest</em>, os m√©todos de <em>boosting</em> tamb√©m tem como objetivo agregar diferentes estimadores da fun√ß√£o de regre√ß√£o <span class="math inline">r({\bf x})</span>. A ideia desses m√©todos que combinam diferentes estimadores da fun√ß√£o de regress√£o √© melhora a precis√£o e a performance preditiva dos modelos de m√°quina, convertendo v√°rios aprendizes fracos em um √∫nico modelo de aprendizado forte.</p>
<p><br></p>
<p>O <em>boosting</em> üöÄ funciona construindo os modelos de forma sequencial, dando mais peso √†s inst√¢ncias que foram classificadas incorretamente nos modelos anteriores. O funcionamento do <em>boosting</em> √© semelhante ao <em>bagging</em> e <em>random forest</em>, exceto pelo fato de que a √°rvore ir√° crescendo sequencialmente: cada √°rvore √© ‚Äúcultivada‚Äù üå±üöøüå≥ usando informa√ß√µes de √°rvores crescidas. <em>Boosting</em> n√£o envolve amostragem <em>bootstrap</em>; em vez de cada √°rvore √© ajustada em uma vers√£o modificada do conjunto de dados original.</p>
<p><br></p>

<img data-src="gifs/prestando_atencao.gif" class="r-stretch"></section>
<section id="boosting-1" class="slide level2">
<h2>üöÄ Boosting</h2>
<p><br></p>
<p>Existem diversas varia√ß√µes e implementa√ß√µes de <em>boosting</em> com diversas implementa√ß√µes distintas em diferentes <em>frameworks</em> de aprendizagem de m√°quina. Aqui, ser√° descrito o conceito geral, sendo este a forma mais usual do <em>boosting</em>.</p>
<p><br></p>
<p>No <em>boosting</em>, como mencionado anteriormente, o estimador <span class="math inline">g({\bf x})</span> √© constr√≠do incrementalmente, i.e., de forma sequencial, melhorando a cada passo. Inicialmente considera-se <span class="math inline">g({\bf x}) \equiv 0</span>. Fazer <span class="math inline">g({\bf x}) \equiv 0</span> estamos iniciando um estimador com alto vi√©s, por√©m, com vari√¢ncia muito baixa, a saber, vari√¢ncia zero.</p>
<p><br></p>
<p>A cada passo do algoritmo, procuramos obter uma √°rvore menos viesada, por√©m, como mencionado em anteriormente, quadando falamos sobre o <em>trade off</em> entre vi√©s e vari√¢ncia, obtemos uma √°rvore atualizada com uma vari√¢ncia um pouco maior. Por isso √© importante partir de uma √°rvore inicial com vari√¢ncia muito baixa.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="boosting-2" class="slide level2">
<h2>üöÄ Boosting</h2>
<p><br></p>
<p>A ideia √© construir um estimador para <span class="math inline">r_i</span>, em que na primeira itera√ß√£o, considera-se <span class="math inline">r_i = y_i</span>. A cada passo subsequente, atualizamos <span class="math inline">r_i</span> para <span class="math inline">r_i = y_i - g({\bf x}_i)</span>, em que <span class="math inline">r_i</span> denominados de res√≠duo. Portanto, a ideia do <em>boosting</em> √© prever o res√≠duo que inicia-se no r√≥tulo/<em>label</em> <span class="math inline">y_i</span>.</p>
<p><br></p>
<p>Uma observa√ß√£o importante √© que as √°rvores tenham <strong>profundida pequena</strong> de modo a evitar <em>overfitting</em>.</p>
<p><br></p>
<p>Al√©m disso, considera-se uma taxa de aprendizagem (<em>learning rate</em>) que denotaremos por <span class="math inline">\lambda \in [0, 1]</span> que tem como objetivo controlar o super-ajuste. Trata-se de um hiperpar√¢metro que dever√° ser obtido via algum procedimento de <em>cross-validation</em>. Portanto, deveremos ‚Äútunar‚Äù üéõ o valor de <span class="math inline">\lambda</span> de modo a encontrar um valor adequado que maximize nosso risco observado, i.e., que maximize a previs√£o de <span class="math inline">R(g)</span>.</p>
</section>
<section id="boosting-3" class="slide level2">
<h2>üöÄ Boosting</h2>
<p><br></p>
<p>Os passos para conseguir boas estimativas de usando o <em>boosting</em> s√£o:</p>
<p><br></p>
<ol type="1">
<li class="fragment">Definimos <span class="math inline">g({\bf x}) \equiv 0</span> e <span class="math inline">r_i = y_i, \, \forall i = 1, \cdots, n</span>;</li>
<li class="fragment">Para cada <span class="math inline">b = 1, \cdots, B</span>, fazemos:</li>
</ol>
<ul>
<li class="fragment">Ajustamos uma √°rvore com <span class="math inline">p</span> folhas para <span class="math inline">({\bf x}_1, r_1), \cdots, ({\bf x}_n, r_n)</span>, em que denotamos essa fun√ß√£o de predi√ß√£o por <span class="math inline">g^b({\bf x})</span>. Lembre-se que estamos estimando <span class="math inline">r_i</span> com base em <span class="math inline">{\bf x}_i</span>;</li>
<li class="fragment">Atualizamos <span class="math inline">g</span> e os res√≠duos: <span class="math inline">g({\bf x}) \leftarrow g({\bf x}) + \lambda g^b({\bf x})</span> e <span class="math inline">r_i \leftarrow Y_i - g({\bf x})</span>.</li>
</ul>
<ol start="3" type="1">
<li class="fragment">Retorna-se o modelo final <span class="math inline">g({\bf x})</span>.</li>
</ol>
<p>No <em>boosting</em>, os valores de <span class="math inline">\lambda</span>, <span class="math inline">p</span> e <span class="math inline">B</span> s√£o hiperpar√¢metros e devem ser obtidos por meio de algum procedimento de valida√ß√£o cruzada, i.e., voc√™ dever√° ‚Äútunar‚Äù üéõ. √â comum que <span class="math inline">\lambda</span> seja pequeno, por exemplo, <span class="math inline">0,01</span> ou <span class="math inline">0,001</span>, <span class="math inline">B \approx 1000</span> e <span class="math inline">p</span> de ordem pr√≥xima √† <span class="math inline">2</span> ou <span class="math inline">4</span>.</p>
</section>
<section id="boosting-4" class="slide level2">
<h2>üöÄ Boosting</h2>
<p><br></p>
<p>Considere a vari√°vel aleat√≥ria <span class="math inline">Y_i \sim \mathcal{N}(\sin(X_i), \sigma^2)</span>, com <span class="math inline">X_i \in \mathcal{U}(0, 10)\,, \forall i.</span> Experimente na aplica√ß√£o web que segue o m√©todo <em>boosting</em>. Observe o comportamento do estimador variando os par√¢metros do algoritmo <em>boosting</em>. Perceba que para <span class="math inline">\lambda = 0</span> n√£o h√° aprendizado algum! Se desejar visualizar a aplica√ß√£o abaixo de forma ampliada, clique <a href="https://pedro-rafael.shinyapps.io/shiny_apps/#section-boosting">aqui</a>.</p>
<p><br></p>
 <iframe id="example1" src="https://pedro-rafael.shinyapps.io/shiny_apps/#section-boosting" style="border: none; width: 100%; height: 90%" frameborder="0"></iframe>
</section>
<section id="boosting-5" class="slide level2">
<h2>üöÄ Boosting</h2>
<p><br></p>
<p>Em geral, as abordagens de aprendizagem estat√≠stica que aprendem lentamente tendem a executar bem. Observe que no <em>boosting</em>, ao contr√°rio do ensacamento, a constru√ß√£o de cada √°rvore depende fortemente das √°rvores que j√° foram ‚Äúcultivadas‚Äù üå±üöøüå≥. Portanto, diferentemente do <em>bagging</em> que √© um algoritmo em que as √°rvores em cada itera√ß√£o s√£o mutuamente independentes e, portanto, facilmente paraleliz√°vel, no <em>boosting</em> as √°rvores s√£o dependentes umas das outras.</p>
<p><br></p>
<p>Na literatura de <em>machine learning</em> h√° diversos algoritmos que implementam o <em>boosting</em>. Uma implementa√ß√£o bastante popular, por conta de seu desempenho, √© denominada <strong>XGBoost</strong>, proposto em CHEN, Tianqi; GUESTRIN, Carlos. <a href="https://arxiv.org/pdf/1603.02754.pdf"><strong>Xgboost: A scalable tree boosting system</strong></a>. In: <em>Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</em>. 2016. p.&nbsp;785-794.</p>
<p><br></p>
<p>No R, o algoritmo est√° implementado na biblioteca <a href="https://github.com/dmlc/xgboost">xgboost</a>. Caso deseje utilizar a biblioteca <a href="https://www.tidymodels.org/">tidymodels</a> em sua modelagem, √© poss√≠vel utilizar o <strong>XGBoost</strong> usando a fun√ß√£o <code>boost_tree</code> da biblioteca <a href="https://parsnip.tidymodels.org/reference/boost_tree.html">parsnip</a> que faz parte do <a href="https://www.tidymodels.org/">tidymodels</a>. Por padr√£o, essa fun√ß√£o j√° utiliza o algoritmo <strong>XGBoost</strong>.</p>
</section>
<section id="boosting-6" class="slide level2">
<h2>üöÄ Boosting</h2>
<p><br></p>
<p>Uma outra forma de obter um valor adequado de <span class="math inline">B</span> √© parar de adicionar itera√ß√µes quando o risco observado come√ßa a aumentar. Lembre-se que h√° um <em>trade off</em> entre vi√©s e vari√¢ncia, em que as itera√ß√µes come√ßam por um estimador <span class="math inline">\widehat{g}</span> com vari√¢ncia nula e na medida que as itera√ß√µes progridem, <span class="math inline">\widehat{g}</span> aumenta sua vari√¢ncia em troca da diminui√ß√£o do vi√©s. Para algum valor de <span class="math inline">B</span> o estimador poder√° perder um pouco de performance. A ideia √© escolher um valor de <span class="math inline">B</span> antes de atingir uma piora de <span class="math inline">\widehat{R}(g)</span> (risco observado). Essa estrat√©gia √© denominada de <em>early stopping</em> ‚åõ.</p>
<p><br></p>
<p><strong>Muito embora aqui apresentamos o algoritmo <em>boostring</em> em um contexto de √°rvores de regress√£o, esse algoritmo √© gen√©rico e poder√° ser utilizado como estat√©gia para combinar modelos que s√£o individualmente fracos</strong>.</p>
<p><br></p>

<img data-src="gifs/pensativo.gif" class="r-stretch"></section>
<section id="exerc√≠cios-11" class="slide level2">
<h2>üìö Exerc√≠cios</h2>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Considere a vari√°vel aleat√≥ria <span class="math inline">Y_i \sim \mathcal{N}(\sin(X_i), \sigma^2)</span>, com <span class="math inline">X_i \in \mathcal{U}(0, 10)\,, \forall i.</span> Implemente uma fun√ß√£o em R que construi o gr√°fico da aplica√ß√£o anterior. Essa fun√ß√£o dever√° ter os argumentos do algoritmo <em>boosting</em>, al√©m de outros argumentos para controle do tamanho da amostra e da vari√¢ncia <span class="math inline">\sigma^2</span>.</p>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Construa um gr√°fico com a avalia√ß√£o do risco observado do m√©todo de <em>boosting</em> para diferentes valores de <span class="math inline">B</span>. Considere <span class="math inline">B = 1, \cdots, 10000</span>. Comente o resultado.</p>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Utilizando um procedimento de valida√ß√£o cruzada, e o <a href="https://www.tidymodels.org/">tidymodels</a>, obtenha uma estimativa para os hiperpar√¢metros da fun√ß√£o <a href="https://parsnip.tidymodels.org/reference/boost_tree.html"><code>boost_tree()</code></a>, a saber, os argumentos <code>trees</code> (n√∫mero de √°rvores), <code>tree_depth</code> (profundidade da √°rvore) e <code>learn_rate</code> (taxa de aprendizado). Avalie o risco preditivo do modelo, i.e., estime <span class="math inline">\widehat{R}(g)</span>.</p>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Compare o <em>bagging</em> de √°rvores de regress√£o, com o m√©todo <em>random forest</em> e o <em>boosting</em> de √°rvores de regress√£o. Qual o risco preditivo de cada um deles para prever <span class="math inline">Y_i</span>? Construa um gr√°fico com as previs√µes, no conjunto de teste, de cada um dos modelos.</p>
</section>
<section id="exerc√≠cios-12" class="slide level2">
<h2>üìö Exerc√≠cios</h2>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Considere o problema em que o objetivo √© prever a <strong>pontua√ß√£o</strong> (<em>score</em>) / (item 2) do aluno com base em algumas vari√°veis. S√£o elas:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Horas estudadas</strong>: o n√∫mero total de horas gastas estudando por cada aluno;</li>
<li class="fragment"><strong>Pontua√ß√£o</strong>: As notas obtidas pelos alunos em testes anteriores;</li>
<li class="fragment"><strong>Atividades extracurriculares</strong>: Se o aluno participa de atividades extracurriculares (Sim ou N√£o);</li>
<li class="fragment"><strong>Horas de sono</strong>: o n√∫mero m√©dio de horas de sono que o aluno teve por dia;</li>
<li class="fragment"><strong>Amostras de perguntas praticadas</strong>: O n√∫mero de amostras de perguntas que o aluno praticou.</li>
</ol>
<p><br></p>
<p>Voc√™ poder√° baixar e ter uma descri√ß√£o maior da base de dados clicando <a href="https://www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression">aqui</a>. Avalie o poder preditivo do <em>random forest</em> comparando com o <em>boosting</em> e <span class="math inline">k</span>NN. Construa uma an√°lise usando um notebook de <a href="https://quarto.org/">quarto</a>, comentando os passos.</p>
<p><br></p>
<p><span class="red">Exerc√≠cio</span>: Refa√ßa o exerc√≠cio anterior usando os dados de vermelho üçáüç∑, dispon√≠veis <a href="https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009">aqui</a>.</p>


<img src="https://www.ufpb.br/de/contents/imagens/logode.png" class="slide-logo r-stretch"><div class="footer footer-default">
<p><a href="https://www.ufpb.br/de">Departamento de Estat√≠stica da UFPB</a></p>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-pointer/pointer.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-attribution/attribution.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'pointer': {"key":"q","color":"red","pointerSize":16,"alwaysVisible":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: true,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, RevealPointer, RevealAttribution, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copiada");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copiada");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>