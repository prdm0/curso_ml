<!DOCTYPE html>
<html lang="pt"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/quarto-contrib/roughnotation-0.5.1/rough-notation.iife.js"></script>
<script src="site_libs/quarto-contrib/roughnotation-init-1.0.0/rough.js"></script><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.176">

  <meta name="author" content="Prof.&nbsp;Dr.&nbsp;Pedro Rafael D. Marinho Departamento de Estatística - UFPB ">
  <meta name="dcterms.date" content="2023-07-05">
  <title>Machine Learning / Aprendizagem de Máquina</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-pointer/pointer.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-attribution/attribution.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="site_libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
  <script src="site_libs/jquery-1.12.4/jquery.min.js"></script>
  <link href="site_libs/leaflet-1.3.1/leaflet.css" rel="stylesheet">
  <script src="site_libs/leaflet-1.3.1/leaflet.js"></script>
  <link href="site_libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet">
  <script src="site_libs/proj4-2.6.2/proj4.min.js"></script>
  <script src="site_libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
  <link href="site_libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet">
  <script src="site_libs/leaflet-binding-2.1.2/leaflet.js"></script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning / Aprendizagem de Máquina</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Prof.&nbsp;Dr.&nbsp;Pedro Rafael D. Marinho<br>Departamento de Estatística - UFPB<br> 
</div>
</div>
</div>

  <p class="date">2023-07-05</p>
</section>
<section class="slide level2">

<!-- ```{r} -->
<!-- #| echo: false -->
<!-- #| warning: false -->
<!-- #| eval: true -->
<!-- if(fs::dir_exists("index_files/")) -->
<!--   fs::dir_delete("index_files/") -->
<!-- ``` -->
<div class="r-fit-text">
<p>Aprendizagem de Máquina</p>
<p><span class="flow">Bacharelado em Estatística</span></p>
<p>UFPB</p>
</div>
</section>
<section>
<section id="section" class="title-slide slide level1 title center">
<h1></h1>
<div class="r-fit-text">
<p><span class="flow">Apresentação</span></p>
</div>
</section>
<section id="section-1" class="slide level2" data-background-image="https://raw.githubusercontent.com/prdm0/imagens/main/eu.jpg" data-background-size="contain" data-background-position="left">
<h2></h2>
<div class="columns">
<div class="column" style="width:40%;">

</div><div class="column" style="width:60%;">
<section id="sobre-mim" class="slide level2">
<h2>Sobre mim</h2>
<p><br> <br></p>
<ul>
<li class="fragment"><p>Me chamo <a href="https://prdm.netlify.app/about_pt_br.html" data-preview-link="true">Prof.&nbsp;Dr.&nbsp;Pedro Rafael D. Marinho</a>. Meu currículo Lattes poderá ser acessado clicando <a href="http://lattes.cnpq.br/7185368598935272" data-preview-link="true">aqui</a>.</p></li>
<li class="fragment"><p>Sou docente do Departamento de Estatística da UFPB. 👨‍🏫</p></li>
<li class="fragment"><p>Toda minha formação acadêmica é na área de estatística (bacharelado ao doutorado).</p></li>
<li class="fragment"><p>Tenho entusiasmo por programação, ciência de dados e aprendizagem de máquina 💻📈.</p></li>
<li class="fragment"><p><svg aria-hidden="true" role="img" viewbox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg> Me acompanhe no GitHub: <a href="https://github.com/prdm0" class="uri">https://github.com/prdm0</a>.</p></li>
<li class="fragment"><p><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg> Me acompanhe no Linkedin: <a href="https://www.linkedin.com/in/prdm0/" class="uri">https://www.linkedin.com/in/prdm0/</a>.</p></li>
</ul>
</section>
</div>
</div>
</section></section>
<section>
<section id="o-departamento" class="title-slide slide level1 title center">
<h1>O Departamento</h1>

</section>
<section id="meu-segundo-lar" class="slide level2" data-background-color="black" data-background-image="https://raw.githubusercontent.com/prdm0/imagens/main/foto_aerea_ufpb.jpeg" data-background-size="1600px" data-background-repeat="repeat" data-background-opacity="0.35">
<h2>Meu segundo lar</h2>
<div class="cell" width="100" height="100">
<div class="cell-output-display">
<div class="leaflet html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-d344596d4a9b8a5afd5d" style="width:960px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-d344596d4a9b8a5afd5d">{"x":{"options":{"crs":{"crsClass":"L.CRS.EPSG3857","code":null,"proj4def":null,"projectedBounds":null,"options":{}}},"calls":[{"method":"addMarkers","args":[-7.1404,-34.846199,null,null,null,{"interactive":true,"draggable":false,"keyboard":true,"title":"","alt":"","zIndexOffset":0,"opacity":1,"riseOnHover":false,"riseOffset":250},null,null,null,null,null,{"interactive":false,"permanent":false,"direction":"auto","opacity":1,"offset":[0,0],"textsize":"10px","textOnly":false,"className":"","sticky":true},null]},{"method":"addTiles","args":["https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png",null,null,{"minZoom":0,"maxZoom":18,"tileSize":256,"subdomains":"abc","errorTileUrl":"","tms":false,"noWrap":false,"zoomOffset":0,"zoomReverse":false,"opacity":1,"zIndex":1,"detectRetina":false,"attribution":"&copy; <a href=\"https://openstreetmap.org\">OpenStreetMap<\/a> contributors, <a href=\"https://creativecommons.org/licenses/by-sa/2.0/\">CC-BY-SA<\/a>"}]}],"limits":{"lat":[-7.1404,-7.1404],"lng":[-34.846199,-34.846199]},"setView":[[-7.1404,-34.846199],37,{"maxWidth":1500,"minWidth":1600,"autoPan":true,"keepInView":false,"closeButton":true,"className":""}]},"evals":[],"jsHooks":[]}</script>
<p>Departamento de Estatística da UFPB.</p>
</div>
</div>
</section>
<section id="que-linguagem-de-programação-utilizar" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M64 96c0-35.3 28.7-64 64-64H512c35.3 0 64 28.7 64 64V352H512V96H128V352H64V96zM0 403.2C0 392.6 8.6 384 19.2 384H620.8c10.6 0 19.2 8.6 19.2 19.2c0 42.4-34.4 76.8-76.8 76.8H76.8C34.4 480 0 445.6 0 403.2zM281 209l-31 31 31 31c9.4 9.4 9.4 24.6 0 33.9s-24.6 9.4-33.9 0l-48-48c-9.4-9.4-9.4-24.6 0-33.9l48-48c9.4-9.4 24.6-9.4 33.9 0s9.4 24.6 0 33.9zM393 175l48 48c9.4 9.4 9.4 24.6 0 33.9l-48 48c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l31-31-31-31c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0z"></path></svg> Que linguagem de programação utilizar?</h2>
<p><br></p>
<p>Nesse curso, será abordado a linguagem de programação <a href="https://www.r-project.org/">R</a>, mas lembre-se que você poderá utilizar qualquer linguagem de programação para fazer ciência de dados. Porém, R e Python são as minhas sugestões, haja vista que, atualmente, elas são as linguagens com maior quantidade de ferramentas e usuários trabalhando na área de <a href="https://en.wikipedia.org/wiki/Data_science">ciência de dados</a>.</p>
<p><br></p>
<p><span class="black">Outros motivos que me leva a lecionar a disciplina utilizando a linguagem R são:</span></p>
<ol type="1">
<li class="fragment">Possui ferramentas muito bem pensadas para manipulação e tratamento de dados;</li>
<li class="fragment">Normalmente, os frameworks de machine learning de R são menos verbosos que os de Python;</li>
<li class="fragment">Matrizes e data frames são estruturas de dados que já encontra-se definidas dentro da linguagem, não precisando assim de importar bibliotecas.</li>
</ol>
<p>Isso é meu gosto pessoal. É um gosto que, talvez, faz mais sentido, em se tratando de alguém que vem da estatística. No mercado de trabalho e em seus estudos, após cursar as disciplinas de R e Python, fornecidas pelo <a href="https://www.ufpb.br/de">Bacharelado em Estatística da UFPB</a>, você terá a capacidade de estudar os frameworks de machine learning, aos seus próprios passos e escolher o que melhor te agrada. A linguagem <a href="https://julialang.org/">Julia</a> também poderá ser uma ótima opção.</p>
</section></section>
<section>
<section id="section-2" class="title-slide slide level1 title center">
<h1></h1>
<div class="r-fit-text">
<p><span class="flow">Aprendizagem de Máquina: O que é?</span></p>
</div>
</section>
<section id="aprendizagem-de-máquina" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M320 0c17.7 0 32 14.3 32 32V96H472c39.8 0 72 32.2 72 72V440c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72H288V32c0-17.7 14.3-32 32-32zM208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H208zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H304zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H400zM264 256a40 40 0 1 0 -80 0 40 40 0 1 0 80 0zm152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80zM48 224H64V416H48c-26.5 0-48-21.5-48-48V272c0-26.5 21.5-48 48-48zm544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48H576V224h16z"></path></svg> Aprendizagem de máquina</h2>
<p><br></p>
<p><img data-src="gifs/am.gif" class="fragment" width="800" height="600"></p>
</section>
<section id="aprendizagem-de-máquina-1" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M320 0c17.7 0 32 14.3 32 32V96H472c39.8 0 72 32.2 72 72V440c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72H288V32c0-17.7 14.3-32 32-32zM208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H208zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H304zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H400zM264 256a40 40 0 1 0 -80 0 40 40 0 1 0 80 0zm152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80zM48 224H64V416H48c-26.5 0-48-21.5-48-48V272c0-26.5 21.5-48 48-48zm544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48H576V224h16z"></path></svg> Aprendizagem de máquina</h2>
<p><br> <br></p>
<p><strong>Alguns pontos</strong>:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>A <strong>A</strong>prendizagem de <strong>M</strong>áquina (AM), também chamada de <strong>M</strong>achine <strong>L</strong>earning (ML), no inglês, nasceu na década de 60 como um campo da inteligênica artificial.</p></li>
<li class="fragment"><p>Em sua origem, as aplicações de AM tinha como objetivo aprender padrões com base nos dados.</p></li>
<li class="fragment"><p>Originalmente, as aplicações de AM eram de cunho estritamente computacional. Todavia, desde o início dos anos 90, a área de aprendizagem de máquina expandiu seus horizontes e começou a se estabelecer como um campo por sim mesma.</p></li>
<li class="fragment"><p>Em particular, a área de aprendizagem de máquina começou a estabelecer muitas intersecções com a estatística. Muitos de seus algoritmos são construídos com base em metodologias que surgiram na estatística.</p></li>
<li class="fragment"><p>Atualmente, a comunidade de AM é bastante interdisciplinar e utiliza-se de ideias desenvolvidas em diversas áreas, sendo a estatística uma delas.</p></li>
</ol>
</section>
<section id="tipos-de-aprendizado" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M320 0c17.7 0 32 14.3 32 32V96H472c39.8 0 72 32.2 72 72V440c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72H288V32c0-17.7 14.3-32 32-32zM208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H208zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H304zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H400zM264 256a40 40 0 1 0 -80 0 40 40 0 1 0 80 0zm152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80zM48 224H64V416H48c-26.5 0-48-21.5-48-48V272c0-26.5 21.5-48 48-48zm544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48H576V224h16z"></path></svg> Tipos de Aprendizado</h2>
<p><br></p>
<p><span class="black">Aprendizado supervisionado</span></p>
<p><br></p>
<p>Nesse curso, inicialmente estudaremos problemas de <span class="red">aprendizado supervisionado</span>, que consiste em aprender a fazer predições a partir de conjunto de dados em que rótulos (valores da variável resposta <span class="math inline">Y</span>) são observados. Trataremos tanto de problemas de regressão (estimar um valor númérico) quanto problemas de classificação (classificar um cliente como aprovado ou reprovado, em um problema de concessão de crédito). Por exemplo, os <span class="red">modelos de regressão</span> são exemplos de aprendizado supervisionado.</p>
<p><br></p>
<p><span class="black">Aprendizado não supervisionado</span></p>
<p><br></p>
<p>Na segunda parte do curso, aprenderemos alguns métodos de aprendizado <span class="red">não supervisionado</span>, ou seja, algoritmos que não utilizam-se de rótulos, em que busca-se aprender mais sobre a estrutura dos dados. Por exemplo, os <span class="red">métodos de agrupamento</span> (cluster), são exempĺos de métodos de aprendizado não supervisionado.</p>
</section>
<section id="tipos-de-aprendizado-1" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M320 0c17.7 0 32 14.3 32 32V96H472c39.8 0 72 32.2 72 72V440c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72H288V32c0-17.7 14.3-32 32-32zM208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H208zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H304zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H400zM264 256a40 40 0 1 0 -80 0 40 40 0 1 0 80 0zm152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80zM48 224H64V416H48c-26.5 0-48-21.5-48-48V272c0-26.5 21.5-48 48-48zm544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48H576V224h16z"></path></svg> Tipos de Aprendizado</h2>
<p><br></p>
<p>Muito embora no nosso curso focaremos nas abordagens de aprendizagem <strong>supervisionada</strong> e <strong>não-supervisionada</strong>, os tipos de aprendizagem, em geral, podem ser mais amplos, em que temos:</p>
<p><br></p>
<ol type="1">
<li class="fragment">Aprendizagem supervisionada;</li>
<li class="fragment">Aprendizagem não-supervisionada;</li>
<li class="fragment">Aprendizagem semi-supervisionada;</li>
<li class="fragment">Aprendizagem por reforço.</li>
</ol>
</section>
<section id="o-que-é-aprender" class="slide level2">
<h2>O que é aprender?</h2>
<p><br></p>
<p>Antes de detalharmos os tipos de aprendizagem de máquina, uma dúvida que poderá surgir é: <span class="red">“O que é aprender?”</span>. <span class="red">“Como a máquina aprende?”</span>.</p>
<p><br></p>
<p><img data-src="gifs/am.gif" class="fragment" width="900" height="600"></p>
</section>
<section id="o-que-é-aprender-1" class="slide level2">
<h2>O que é aprender?</h2>
<p><br></p>
<p>De forma simples, aprender é ganhar conhecimento através de estudo, experiências, por meio de ensinamentos.</p>
<p><br></p>
<p>Tá, mais como é que a <span class="red">máquina</span> aprende?</p>
<p><br></p>
<ol type="1">
<li class="fragment"><span class="red">Aprendizagem</span> é o processo em que se adquire conhecimento, isto é, é o processo em que utilizamos de algoritmos e fornecemos dados a esses algoritmos para que possamos extrair conhecimento. Nesse processo de aprendisagem, os algoritmos fazem uso de dados para a extressão de conhecimento, através de procedimentos <strong>supervisionado</strong>, <strong>não-supervisionado</strong>, <strong>semi-supervisionado</strong> ou <strong>por reforço</strong>, a depender do algoritmo que você deseja utilizar.</li>
</ol>
<p><br></p>
<ol start="2" type="1">
<li class="fragment"><span class="red">Aprendizado</span> é o modelo ajustado, isto é, é o conhecimento adquirido após o treinamamento obtido no processo de aprendizagem. Você poderá entender como sendo o modelo ajustado e que utilizamores para a tomada de decisões.</li>
</ol>
</section>
<section id="o-que-é-aprender-2" class="slide level2">
<h2>O que é aprender?</h2>
<p><br></p>
<p>Portanto, você poderá entender, basiciamente, existe quatro tipos de aprendizagem, sendo os dois primeiros o que mais focaremos nesse curso e que de loge são os mais utilizados:</p>
<p><br></p>
<ol type="1">
<li class="fragment">Aprendizagem supervisionada;</li>
<li class="fragment">Aprendizagem não-supervisionada;</li>
<li class="fragment">Aprendizagem semi-supervisionada;</li>
<li class="fragment">Aprendizagem por reforço.</li>
</ol>
</section>
<section id="aprendizagem-supervisionada" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br></p>
<p>Nesse tipo de aprendizagem, o algoritmo irá receber um conjunto de dados em que conhecemos rótulos para a variável de interesse. É como se você soubesse onde um bom modelo deve chegar, para assim ser reconhecido como um bom modelo. Por exemplo,</p>
<p><br></p>
<ol type="1">
<li class="fragment"><span class="red">Classificação</span>: precisamos determinar a classe de uma instância de dados, o seu atributo, i.e., <span class="math inline">\widehat{y} = \mathrm{argmax}_y\,P(Y = y\,|\, X = \bf{x})</span>, em que y é um atributo que desejamos prever (cahorro, gato, sapo), e <span class="math inline">\bf{x}</span> é um vetor de características (peso, altura, comprimento, se tem rabo, etc).</li>
</ol>
<p><br></p>
<ol start="2" type="1">
<li class="fragment"><span class="red">Regressão</span>: precisamos estimar uma quantidade numérica, i.e., o valor da variável alvo por meio de uma instância de dados, ou seja, precisamos estimar <span class="math inline">Y = \mathbb{E}(Y\,|\,X = \bf{x})</span>, i.e., devemos encontrar meios de obter <span class="math inline">\widehat{Y}</span>.</li>
</ol>

<aside><div>
<p><strong>Algumas observações de nomenclaturas</strong>:</p>
<ol type="1">
<li class="fragment">É comum chamar cada exemplo de dados, i.e., o vetor <span class="math inline">\bf{x}</span> que será passado ao modelo de <span class="red">atributos</span> ou <span class="red">features</span>;</li>
<li class="fragment">Também é comum chamarmos de <span class="red">rótulo</span> ou <span class="red">label</span> a classe ou valor alvo, ou seja, estas são as formas de nomearmos <span class="math inline">Y</span>, sendo <span class="math inline">Y</span> uma quantidade numérica (modelos de regressão) ou não (modelos de classificação).</li>
</ol>
</div></aside></section>
<section id="aprendizagem-supervisionada-1" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br></p>
<p>Em se tratando de métodos de classificação, podemos ter os métodos:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p><span class="red">Generativos</span>: são os métodos que dada as variáveis <span class="math inline">X</span> e <span class="math inline">Y</span>, o objetivo é encontrar a distribuição de probabilidade conjunta <span class="math inline">P(X, Y)</span>, para então poder determinar <span class="math inline">P(Y\, | \, X = \bf{x})</span>. Alguns métodos são:</p>
<ul>
<li class="fragment">Naive Bayes;</li>
<li class="fragment">Descriminante linear.</li>
</ul></li>
</ol>
<p><br></p>
<ol start="2" type="1">
<li class="fragment"><span class="red">Descriminativos</span>: são os métodos que estimam diretamente a probabilidade condicional <span class="math inline">P(Y \, | \, X = \bf{x})</span> ou que mesmo nem assumem modelos probabilísticos. Podemos citar:
<ul>
<li class="fragment">Regressão logistica;</li>
<li class="fragment">Perceptron;</li>
<li class="fragment">Support Vector Machine - SVM.</li>
</ul></li>
</ol>
</section>
<section id="aprendizagem-supervisionada-2" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="gifs/classificacao.webp"></p>
</div><div class="column" style="width:40%;">
<p><br> Poderíamos estar interessados em classificar o tamanho de morangos:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>S (<strong>S</strong>low): pequeno;</p></li>
<li class="fragment"><p>M (<strong>M</strong>edium): médio;</p></li>
<li class="fragment"><p>L (<strong>L</strong>arge): grande.</p></li>
</ol>
</div>
</div>
</section>
<section id="aprendizagem-supervisionada-3" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br> <br></p>

<img data-src="gifs/Classification-Examples.gif" class="fragment r-stretch quarto-figure-center" width="900"><p class="caption">Mais dois problemas de classificação (linear x não-linear).</p></section>
<section id="aprendizagem-supervisionada-4" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br> <br></p>

<img data-src="gifs/regression.gif" class="fragment r-stretch quarto-figure-center" width="1200"><p>Um exemplo de de um problema de regressão. Aqui, a ideia é utilizar a equação da reta estimada, a reta que minimiza a soma dos quadrados entre a reta e os ponto seria a melhor, de modo a ter uma estimativa numérica através de novos atributos passado ao modelo, i.e., por meio da equação da reta e de um novo valor de <span class="math inline">x</span>.</p>
</section>
<section id="aprendizagem-supervisionada-5" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br></p>
<p>Um outro exemplo seria a classificação de imagem/vídeo, utilizando um algoritmo de rede neural, por exemplo, usando uma <strong>C</strong>onvolutional <strong>N</strong>eural <strong>N</strong>etwork - <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a>. Foram utilizados diversas imagens de pessoas “com” e “sem” máscara. Em que “com” representa detecção da máscara na face da pessoa e “sem” a não detecção.</p>
<p><br></p>
<iframe data-external="1" src="https://www.youtube.com/embed/Zt_Fr7YbU1c" width="50%" height="50%" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="aprendizagem-não-supervisionada" class="slide level2">
<h2>Aprendizagem não-supervisionada</h2>
<p><br></p>
<p>Nesse tipo de aprendizagem, os algoritmos trabalham sobre dados não rotulados, por exemplo, em uma trarefa de agrupamento.</p>
<p><br></p>
<p>Os algoritmos verificam se as instâncias observadas poderão ser arranjadas de alguma maneira, por exemplo, usando alguma métrica de distância, formando grupos (<em>clusters</em>).</p>
<p><br></p>
<p>A ideia é maximizar a distância entre os clusters e minimizar a distância entre os elementos no interrior do grupo. Em outras palavras, o que se quer é tornar os grupos mais diferentes possíveis e tornar os elementos dos grupos o mais parecido possível.</p>
<p><br></p>
<p>Aqui, por não haver rótulos, um problema comum é determinar a quantidade de grupos ideal que muitas vezes são obtidos de forma subjetiva ou por heurísticas. A quantidade de grupos é um dilema!</p>
</section>
<section id="aprendizagem-não-supervisionada-1" class="slide level2">
<h2>Aprendizagem não-supervisionada</h2>
<p><br></p>

<img data-src="gifs/kmeans.gif" class="fragment r-stretch quarto-figure-center" width="900"><p>Após a detecção dos grupos, é preciso analisar o resultado de modo a tentar extrair informações coerentes de modo a saber o que cada grupo representa no problema em questão.</p>
</section>
<section id="aprendizagem-semi-supervisionada" class="slide level2">
<h2>Aprendizagem semi-supervisionada</h2>
<p><br></p>
<p>A aprendizagem semi-supervisionada é uma abordagem na área de aprendizagem de máquina onde um algoritmo utiliza tanto dados rotulados quanto não rotulados para treinamento. Por exemplo, algoritmos que propagam rótulos, como o <em>Label Propagation</em>, em que rótulos conhecidos são propagados para dados não rotulados com base em sua sua proximidade no espaço de características.</p>
<p><br></p>
<p>Uma outra abordagem seria misturar modelos (<em>Model Blending</em>), em que diferentes modelos são treinados em diferentes partes do conjunto de dados, por exemplo, um modelo para a parte roturada e um para a parte não rotulada.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="fragment r-stretch" style="width:50.0%"></section>
<section id="aprendizagem-por-reforço" class="slide level2">
<h2>Aprendizagem por reforço</h2>
<p><br></p>
<p>Nesse tipo de aprendizagem, não há uma fonte externa de exemplos. O agente (modelo) aprende aprende com sua própria experiência, por tentativas e erros, em que você deverá definir uma medida de sucesso, e eventualmente recompensar os acertos. No vídeo abaixo, veja um joguinho que criei em R, onde o carrinho aprendeu a desviar de obstáculos aleatórios que aparecem em sua frente. Utilizou-se uma rede neural cuja a saída poderia ser (“parado”, “para cima” ou “para baixo”). Veja o código clicando <a href="https://github.com/prdm0/desviando_obstaculos"><strong>aqui</strong></a>.</p>
<p><br></p>
<iframe data-external="1" src="https://www.youtube.com/embed/9NXUtwGkkDw" width="50%" height="50%" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="dados-exploração-e-tratamentom" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamentom</h2>
<p><br></p>
<p>Um dos passos mais importante no fluxo de trabalho (<em>workflow</em>) de um modelo de aprendizagem de máquina, consiste na preparação dos dados, onde realizamos transformações, inputações de valores ausentes, identificação de outliers, remoção de variáveis altamente correlacionadas, entre outros.</p>
<p><br></p>
<p>Fazer uma análise exploratória dos dados é um passo importante para que se possa entender e detecatar possíveis inconsistências na base de dados. Não adianta fazer uso de modelos muito sofisticados quando se tem uma base de dados cheia de problemas.</p>
<p><br></p>
<p>Normalmente trabalhamos com juntos de dados (tabelas) relacionais, em que cada linha é uma observação e cada coluna representa um atributo do objeto/observação. A linha de uma base de dados relacional, sem sua a variável de interesse, lembre-se que denominamos <span class="math inline">Y</span> de <span class="red">rótulo</span> ou <span class="red">label</span>, fornece o vetor de características <span class="math inline">\bf{x}</span> que descreve uma dada observação.</p>
</section>
<section id="dados-exploração-e-tratamento" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p>No artigo <a href="https://www.jstatsoft.org/article/view/v059i10">Tidy Data</a>, 2014, publicado no Journal of Statistical Sofware, o Hadley Wickham discute que o princípio de dados organizados estão intimamente relacionados com banco de dados relacional e mais próximo do reciocínio que empregamos na álgebra. Nesse artigo, ele define o que é <span class="red">Tidy Dados</span>, sendo essa uma maneira de mapear um conjunto de dados.</p>
<p><br></p>
<p>Segundo o artigo, um conjunto de dados é <span class="red">bagunçado</span> ou <span class="red">arrumado</span>/<span class="red">tidy</span>, dependendo de como as linhas, colunas e tabelas são combinadas com as observações, variáveis e tipos. Em dados arrumados (dados tidy), temos que:</p>
<p><br></p>
<ol type="1">
<li class="fragment">Cada variável forma uma coluna;</li>
<li class="fragment">Cada observação forma uma linha;</li>
<li class="fragment">Cada valor deve ter sua própria célula.</li>
</ol>
<p><br></p>
<p>Embora existam situações em que já podemos começar a analisar uma base de dados real, essa é a exceção e não a regra. Normalmente, nos deparamos com bases de dados que violam uma ou mais dessas regras. Sempre, que possível, procure utilizar dados no formato <span class="red">Tidy</span>.</p>
</section>
<section id="dados-exploração-e-tratamento-1" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>

<img data-src="imgs/tidy-1.png" class="r-stretch quarto-figure-center"><p class="caption">Representação de uma base de dados no formato tidy.</p></section>
<section id="dados-exploração-e-tratamento-2" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>
<blockquote>
<p>“As famílias felizes são todas iguais; toda família infeliz é infeliz à sua maneira.” – <a href="https://en.wikipedia.org/wiki/Leo_Tolstoy">Leo Tolstoy</a></p>
</blockquote>
<blockquote>
<p>“Conjuntos de dados organizados são todos iguais, mas todo conjunto de dados confuso é confuso à sua maneira.” – <a href="https://hadley.nz/">Hadley Wickham</a></p>
</blockquote>
<p><br></p>

<img data-src="imgs/tidy-2.png" style="width:50.0%" class="r-stretch quarto-figure-center"><p class="caption">Trabalhar com a Tabela do lado esquerdo é melhor que a Tabela do lado direito. Prefira, sempre que possível, o formato tidy. Não permita-se ficar estressado tão facilmente.</p></section>
<section id="dados-exploração-e-tratamento-3" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p>A linguagem de programação R possue diversas ferramentas que permite manipular e explorar bases de dados. Enumero algumas:</p>
<ol type="1">
<li class="fragment"><a href="https://dplyr.tidyverse.org/">dplyr</a>: biblioteca que implementa é uma gramática de manipulação de dados, fornecendo um conjunto consistente de verbos que ajudam a resolver os desafios mais comuns de manipulação de dados;</li>
<li class="fragment"><a href="https://tidyr.tidyverse.org/">tidyr</a>: ferramentas para ajudar a criar dados organizados, onde cada coluna é uma variável, cada linha é uma observação e cada célula contém um único valor;</li>
<li class="fragment"><a href="https://ggplot2-book.org/">ggplot2</a>: um sistema para criar gráficos ‘declarativamente’, baseado no livro <a href="https://www.amazon.com.br/Grammar-Graphics-Leland-Wilkinson/dp/0387245448">The Grammar of Graphics</a>, de <a href="https://en.wikipedia.org/wiki/Leland_Wilkinson">Leland Wilkinson</a>;</li>
<li class="fragment"><a href="https://docs.ropensci.org/visdat/">visdat</a>: uma biblioteca útil para um visualização exploratória preliminar de dados;</li>
<li class="fragment"><a href="https://github.com/rolkra/explore">explore</a>: biblioteca que apresenta algumas rotinas de análise para realizar uma análise exploratória nos dados.</li>
</ol>
<p>Todas essas bibliotecas estão muito bem documentadas. É importante que vocês explorem as documentas dessas bibliotecas, pois eventualmente irei utizar alguma delas.</p>

<img data-src="gifs/ok.gif" class="r-stretch"></section>
<section id="dados-exploração-e-tratamento-4" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>
<p>No <a href="https://r4ds.had.co.nz/tidy-data.html">Capítulo 12</a>, do livro <a href="https://r4ds.had.co.nz/index.html">R for Data Science</a>, o autor aborda mais sobre o formato Tidy e como trabalhar com a biblioteca <a href="https://tidyr.tidyverse.org/">tidyr</a>. <a href="https://r4ds.had.co.nz/transform.html?q=dplyr#dplyr-basics">Aqui</a> o autor aborda de forma básica o pacote <a href="https://dplyr.tidyverse.org/">dplyr</a>.</p>
<p><br></p>
<p>Durante o curso, na medida da necessidade de utilização dessas ferramentas, durante a exposição de exemplos, abordaremos alguns conceitos. Ok?</p>
<p><br></p>

<img data-src="gifs/ok.gif" class="r-stretch"></section>
<section id="dados-exploração-e-tratamento-5" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>
<p>Muitas vezes, no processo de tratamento dos dados, também estamos preocupados em remover atributos que não são significativo para a modelagem, em que nesse momento a experiência dos especialistas são fundamentais.</p>
<p><br></p>
<p>É comum enriquercermos a base de dados com informações de outras bases de dados, em um sistema de gerenciamento de banco de dados relacional, em que as bases de dados estão relacionadas por uma chave. Nesse caso, buscamos por novos atributos para um mesmo objeto (para uma mesma linha da base), em que atributos cruzados devem ter um único valor, para cada objeto, respeitando a regra três de conjuntos de dados tidy.</p>
<p><br></p>
<p>As vezes transformamos variáveis. Por exemplo, é comum tomar o logaritmo de uma variável numérica que é assimétrica, se <span class="math inline">x &gt;= 1</span>, em que <span class="math inline">x</span> é um atributo numérico qualquer.</p>
<p><br></p>
<p>Em diveras situações, também é comum a base de dados apresentar informações faltantes. Nos data frames de R, a falta de informação na base, normlamente serão representadas por <code>NA</code>.</p>
</section>
<section id="dados-exploração-e-tratamento-6" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>
<p>Poderá ser que um dado atributo apresente informação faltante, e normalmente não optaremos em remover a observação e precisaremos imputar a informação, por exemplo:</p>
<p><br></p>
<ol type="1">
<li class="fragment">Tomando alguma medida de tendência central como média/moda/mediana dos valores que são conhecidos para aquele atributo;</li>
<li class="fragment">Criar um novo valor que é indicação de valor faltante;</li>
<li class="fragment">Usar algoritmos como <span class="math inline">k</span>-nearest neighbors - KNN (<span class="math inline">k</span> vizinhos mais próximos) para imputar valores coerentes;</li>
<li class="fragment">Interpolar os dados.</li>
</ol>
<p><br></p>
<p>Esses são alguns exemplos de como podemos imputar observações faltantes. Muitas vezes não podemos nos dar o luxo de percer observações de nossa base de dados.</p>
</section>
<section id="dados-exploração-e-tratamento-7" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>
<p>É comum ser necessário transformar os dados:</p>
<ol type="1">
<li class="fragment"><p>Pode ser necessário transformar os tipos ou os valores dos atributos para tentar obter um melhor ajuste do modelo;</p></li>
<li class="fragment"><p>Pode-se discretizar valores contínuos ou transformá-los em intervalos;</p></li>
<li class="fragment"><p>É comum transformar atributos categóricos com <span class="math inline">p</span> categorias, em <span class="math inline">p</span> novos atributos binários.</p>
<ul>
<li class="fragment"><a href="https://en.wikipedia.org/wiki/One-hot">One-hot encoding</a></li>
<li class="fragment"><a href="https://en.wikipedia.org/wiki/Dummy_variable_(statistics)">Variáveis dummy</a></li>
</ul></li>
<li class="fragment"><p>Outra transformação muito comum é a normalização dos dados. Normalizar os dados é muito útil quando os atributos numéricos possuem escalas muito diferentes.</p></li>
</ol>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="math display">X_{novo} = \frac{X - X_{min}}{X_{max} - X_{min}},</span> em que <span class="math inline">X_{novo} \in [0, 1].</span></p>
</div><div class="column" style="width:50%;">
<p><span class="math display">X_{novo} = Z = \frac{X - \mu}{\sigma^2},</span> em que <span class="math inline">\mathbb{E}(X) = \mu</span> é a média dos dados e <span class="math inline">\mathrm{Var}(X) = \sigma^2</span>. Na prática, em um contexto de v.a., iids, usamos <span class="math inline">\overline{x}</span> como estimador de <span class="math inline">\mu</span> e <span class="math inline">S^2</span> (variância amostral) como estimador de <span class="math inline">\sigma^2</span>.</p>
</div>
</div>
</section>
<section id="dados-exploração-e-tratamento-8" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>
<p>Lembre-se, como citado anteriormente, tomar o logaritmo natural, ou mesmo na base 10 de variáveis numéricas muito assimétricas, poderá ajudar um pouco, desde que seja possivel tomar o <span class="math inline">\log(\cdot)</span>.</p>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">rgamma</span>(<span class="dv">1000</span>, <span class="dv">2</span>, <span class="dv">2</span>) <span class="sc">|&gt;</span> </span>
<span id="cb1-3"><a href="#cb1-3"></a>  <span class="fu">hist</span>()</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img data-src="index_files/figure-revealjs/unnamed-chunk-2-1.png" width="960"></p>
</div>
</div>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="fu">rgamma</span>(<span class="dv">1000</span>, <span class="dv">2</span>, <span class="dv">2</span>) <span class="sc">|&gt;</span> </span>
<span id="cb2-3"><a href="#cb2-3"></a>  <span class="fu">log</span>() <span class="sc">|&gt;</span> <span class="fu">hist</span>()</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img data-src="index_files/figure-revealjs/unnamed-chunk-3-1.png" width="960"></p>
</div>
</div>
</div>
</div>
</section>
<section id="dados-exploração-e-tratamento-9" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>
<p>Anteriormente eu citei algumas bibliotecas úteis de R para explorar os dados, na fase de tratamento das observações. Porém, não estranhe não ter cidado bibliotecas do framework <a href="https://www.tidymodels.org/packages/">tidymodels</a>, em especial o <a href="https://recipes.tidymodels.org/">recipes</a> que é muito utilizado no workflow de aprendizagem de máquina na fase de pré-processamento dos dados. Muitas dessas transformações são aplicadas como receitas de pré-processamento com o pacote <a href="https://recipes.tidymodels.org/">recipes</a>.</p>
<p><br></p>
<p>O tidymodels será muito útil para nós, mas, aos poucos, seu uso e explicações mais detalhadas serão apresentadas, apesar que em algumas situações mais simples, poderei não utilizá-lo, expor detalhes que eventualmente não será possível ou estariam camuflados na utilização do tidymodels.</p>
<p><br></p>

<img data-src="gifs/ok-2.gif" style="width:20.0%" class="r-stretch"></section>
<section id="as-duas-culturas" class="slide level2">
<h2>As duas culturas</h2>
<p><br></p>
<p>Em <a href="https://projecteuclid.org/journalArticle/Download?urlId=10.1214%2Fss%2F1009213726">Breiman, L. (2001a). Statistical modeling: The two cultures. Statistical Science, 16(3), 199–231</a>, o Leo Breiman argumenta que existe duas culturas no uso de modelos estatísticos, em especialmente na área de modelos de regressão. Segundo eles, as culturas são:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><span class="red">Data modeling culture</span>: nela, em geral, se assume que o modelo de regressão utilizado <span class="math inline">r(x)</span>, por exemplo, <span class="math inline">r(x) = \beta_0 + \sum_{i = 1}^d \beta_ix_i</span> é correto. O principal objetivo dessa abordagem é a interpretação dos parâmetros que indexam o modelo <span class="math inline">r(x)</span>. Nesse tipo de cultura, a ideia também é construir intervalos aleatórios e testar hipóteses para os <span class="math inline">\beta_i's</span>. Sob essa ótica, muitas suposições sob o modelo são realizadas, em que formas para checar essas suposições são desenvolvidas, uma vez que elas são fundamentais para esse tipo de modelagem.</li>
</ol>
<p><br></p>
<ol start="2" type="1">
<li class="fragment"><span class="red">Algorithmic modeling culture</span>: essa é a cultura que domina a comunidade de aprendizagem de máquina. Nessa abordagem, o principal objetivo são as predições por meio de novas observações. Não se assume que o modelo utilizado é o modelo correto. Nesse tipo de modelagem, muitas vezes os algoritmos não envolve nenhuma estrutura probabilística. Muitas vezes, modelos não bem especificado conduzem a boas predições.</li>
</ol>
</section>
<section id="as-duas-culturas-1" class="slide level2">
<h2>As duas culturas</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/paper_breiman.png" style="width:70.0%"></p>
<figcaption>Breiman, L. (2001a). Statistical modeling: The two cultures. Statistical Science, 16(3), 199–231.</figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/leo_breiman.png" style="width:60.0%"></p>
<figcaption>Leo como um probabilista jovem na Universidade da Califórina.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="as-duas-culturas-2" class="slide level2">
<h2>As duas culturas</h2>
<p><br></p>
<p>Há diversos artigos interessantes que são respostas ao artigo do Leo Breiman, como por exemplo, o artigo <a href="https://www.jstor.org/stable/2676682">Statistical Modeling: The Two Cultures: Comment</a> do David Cox e com comentários do Brad Efron.</p>

<img data-src="imgs/david_cox.png" style="width:20.0%" class="r-stretch quarto-figure-center"><p class="caption"><a href="https://en.wikipedia.org/wiki/David_Cox_(statistician)">Sir David Cox.</a></p></section>
<section id="as-duas-culturas-3" class="slide level2">
<h2>As duas culturas</h2>
<p><br></p>
<p>Muito embora exista essa divisão entre as culturas, Breiman foi um estatístico que desempenhou um grande trabalho para unir a área de estatística com aprendizado de máquina. Por conta dessa grande importância, um prêmio concedido em sua homenagem foi criado pela <a href="https://community.amstat.org/slds/awards/breiman-award">American Statistical Association</a>.</p>

<img data-src="imgs/breiman_residencia.png" style="width:30.0%" class="r-stretch quarto-figure-center"><p class="caption"><a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2Fss%2F1009213290">Leo Breiman trabalhando em sua residência, em 1985.</a></p></section></section>
<section>
<section id="section-3" class="title-slide slide level1 title center" data-background-image="imgs/rawpixel/freight.jpg">
<h1></h1>
<div class="r-fit-text">
<p><span class="flow">Regressão / Parte I</span></p>
</div>
</section>
<section id="regressão" class="slide level2">
<h2>Regressão</h2>
<p><br></p>
<p>Métodos de regressão surgiram há mais de dois séculos com Legendre (1805) e Gauss (1809), que exploraram o método dos mínimos quadrados com o objetivo de prever órbitas ao redor do Sol. Hoje em dia, o problema de estimação de uma função de regressão possui papel central em estatística.</p>
<p><br></p>
<blockquote>
<p>Apesar de as primeiras técnicas para solucionar esse problema datarem de ao menos 200 anos, os avanços computacionais recentes permitiram que novas metodologias fossem exploradas. Em particular, com a capacidade cada vez maior de armazenamento de dados, métodos com menos suposições sobre o verdadeiro estado da natureza ganham cada vez mais espaço. Com isso, vários desafios surgiram: por exemplo, métodos tradicionais não são capazes de lidar de forma satisfatória com bancos de dados em que há mais covariáveis que observações, uma situação muito comum nos dias de hoje. Similarmente, são frequentes as aplicações em que cada observação consiste em uma imagem ou um documento de texto, objetos complexos que levam a análises que requerem metodologias mais elaboradas. – Izbick et al.</p>
</blockquote>
</section>
<section id="regressão-1" class="slide level2">
<h2>Regressão</h2>
<p><br></p>
<p>De forma geral, temos que o objetivo de um modelo de regressão é determinar a relação entre uma variável aleatória (label) <span class="math inline">Y \in \mathbb{R}</span> e um vetor de covariáveis (features) <span class="math inline">\mathbf{x} = (x_1, \cdots, x_d) \in \mathbb{R}^d</span>. Mais especificamente, busaca-se estimar</p>
<p><span class="math display">r(\bf{x}) := \mathbb{E}(Y\,|\,\bf{X} = \bf{x}),</span></p>
<p>sendo esta chamada de <span class="red">função de regressão</span>. Temos que:</p>
<p><br></p>
<ol type="1">
<li class="fragment">Se <span class="math inline">Y</span> é uma variável quantitativa, então estamos sob um problema de <span class="red">regressão</span>;</li>
<li class="fragment">Se <span class="math inline">Y</span> é uma variável qualitativa, então teremos um problema de <span class="red">classificação</span>.</li>
</ol>
<p>Em aprendizagem de máquina, assumimos que não temos meios de calcular <span class="math inline">r({\bf{x}})</span>, i.e., não conhecemos a distribuição condicional de <span class="math inline">{\bf{Y}\,|\,X}</span>. Portanto, não temos meios de calcular</p>
<p><span class="math display">\mathbb{E}({\bf X}|Y = y) = \int x\,\mathrm{d}F_{\bf X}({\bf x} | Y = y).</span></p>
</section>
<section id="notações" class="slide level2">
<h2>Notações</h2>
<p><br></p>
<p>A variável <span class="math inline">Y</span> recebe frequentemente o nome de variável resposta, variável dependente, rótulo ou <em>label</em>. Já as observações contidas no vetor <span class="math inline">\bf{x} = (x_1, \cdots, x_d)</span>, são, em geral, denominadas de variáveis explicativas, variáveis independentes, características, atributos, preditores, covariáveis ou <em>features</em>.</p>
<p><br></p>
<p>A ideia, nessa primeira parte do curso, é descrever algumas técnicas para estimar (<strong>treinar</strong>, como é dito em aprendizagem de máquina) <span class="math inline">r(\bf{x})</span>.</p>
<p><br></p>
<p>A menos quando dito o contrário, assumiremos que nossa amostra são i.i.d. (independentes e identicamente distribuídas), ou seja, <span class="math inline">(\bf{X}_1, Y_1), \cdots, (\bf{X}_n, Y_n)</span> são i.i.d.</p>
<p><br></p>
<p>Denota-se por <span class="math inline">x_{i,j}</span> o valor da <span class="math inline">j</span>-ésima covariável na <span class="math inline">i</span>-ésima amostra, com <span class="math inline">j = 1, \cdots, d</span> e <span class="math inline">i = 1, \cdots, n</span>.</p>
</section>
<section id="notações-1" class="slide level2">
<h2>Notações</h2>
<p><br></p>
<table style="width:50%;">
<caption>Notação utilizada nesse material para as variáveis envolvidas em um problema de regressão.</caption>
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Label</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">Y_1</span></td>
<td><span class="math inline">X_{1,1},\cdots, X_{1,d}\,\,\, (= \bf{X}_1)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\vdots</span></td>
<td><span class="math inline">\,\,\,\vdots\,\,\,\,\, \ddots\,\,\ \vdots</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">Y_n</span></td>
<td><span class="math inline">X_{n,1},\cdots, X_{n,d}\,\,\, (= \bf{X}_n)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="regressão-2" class="slide level2">
<h2>Regressão</h2>
<p>Nossa ideia é construir uma boa estimativa <span class="math inline">g</span> da função de regressão <span class="math inline">r(\bf{x}) := \mathbb{E}(Y\,|\,\bf{X} = \bf{x})</span>, para novas observações, i.e., queremos obter uma função <span class="math inline">g</span>, tal que:</p>
<p><span class="math display">g: \mathbb{R}^d \rightarrow \mathbb{R},</span></p>
<p>de tal forma que <span class="math inline">g</span> possua um bom poder preditivo. Em aprendizagem de máquina, só estaremos interessados em obter uma função <span class="math inline">g</span> que estime bem um número real (em problemas de regressão), ou que classifique bem (em um problema de classificação), utilizando as <span class="math inline">d</span> covariáveis. Ou seja, para <span class="math inline">m</span> novas observações, desejamos obter <span class="math inline">g</span>, que</p>
<p><span class="math display">g({\bf{x}}_{n + 1}) \approx y_{n + 1}, \cdots, g({\bf{x}}_{n + m}) \approx y_{n + m}.</span></p>
</section>
<section id="função-de-risco" class="slide level2">
<h2>Função de risco</h2>
<p><br></p>
<p>Para que possamos construir boas funções de predição, é preciso que tenhamos um critério para medir o desempenho de uma dada função <span class="math inline">g:\mathbb{R}^d \rightarrow \mathbb{R}</span>. Em contexto de regressão, usaremos o risco quadrático, muito embora esta não é a única opção. Denotaremos a função de risco quadrático por:</p>
<p><span class="math display">R_{pred}(g) = \mathbb{E}\left[({\bf Y} - g({\bf X}))^2\right],</span> em que <span class="math inline">(\bf X, Y)</span> são observações novas que não foram utilizadas para treinar/estimar <span class="math inline">g</span>. Lê-se <span class="math inline">R_{pred}(g)</span> como “risco preditivo de <span class="math inline">g</span>”. Note que, como <span class="math inline">\bf X</span> são observações conhecidas e <span class="math inline">g(\cdot)</span> é um modelo preditivo, portanto, <span class="math inline">g</span> é conhecido, então, <span class="math inline">\widehat{\bf Y} = g(\bf X)</span> é um estimador dos labels, i.e., de <span class="math inline">\bf Y</span>.</p>
<p><br></p>
<p>Diremos que <span class="math inline">L(g({\bf X}); {\bf Y}) = ({\bf Y} - g({\bf X}))^2</span> é a <span class="red">função de perda quadrática</span>, as vezes chamado de perda <span class="math inline">L_2</span>. Outra funções como a <span class="red">função de perda absoluta</span> denotada por <span class="math inline">L(g({\bf X}); {\bf Y}) = |{\bf Y} - g({\bf X})|</span>, as vezes chamada de perda <span class="math inline">L_1</span> poderiam ser consideradas.</p>
</section>
<section id="função-de-risco-1" class="slide level2">
<h2>Função de risco</h2>
<p><br></p>
<p>Em linhas gerais, seja <span class="math inline">L(\cdot)</span> uma função qualquer, tal que <span class="math inline">\forall \, 0 &lt; u &lt; v</span>, de modo que:</p>
<p><br></p>
<ol type="i">
<li class="fragment"><span class="math inline">0 = L(0) \leq L(u) \leq L(v)</span>;</li>
<li class="fragment"><span class="math inline">0 = L(0) \leq L(-u) \leq L(-v)</span>.</li>
</ol>
<p><br></p>
<p>Qualquer função <span class="math inline">L(\cdot)</span> que satisfaz as propriedades acima é chamada de <a href="https://en.wikipedia.org/wiki/Loss_function">função de perda</a>. Em especial, temos que:</p>
<p><br></p>
<ul>
<li class="fragment">Função de perda quadrática: <span class="math inline">L(u) = u^2</span>;</li>
<li class="fragment">Função de perda absoluta: <span class="math inline">L(u) = |u|</span>;</li>
<li class="fragment">Função de perda degradu: <span class="math inline">L(0) = 0</span>, se <span class="math inline">|u| &lt; \delta</span> e <span class="math inline">1</span> caso contrário, para algum <span class="math inline">\delta &gt; 0</span>;</li>
</ul>
</section>
<section id="função-de-risco-2" class="slide level2">
<h2>Função de risco</h2>
<p><br></p>
<p>Normalmente considera-se a perda <span class="math inline">L_2</span>, uma vez que em modelos de regressão, minimizar <span class="math inline">R_{pred}(g)</span>, em <span class="math inline">g</span>, equivale a encontrar <span class="math inline">r({\bf x}) = \mathbb{E}({\bf X}|{\bf Y})</span>, i.e., equivale a estimar a função de regressão.</p>
<p><br></p>
<p><span class="red">Teorema</span>: Suponha que definimos o risco de uma função de predição <span class="math inline">g: \mathbb{R}^d \rightarrow \mathbb{R}</span> via função perda quadrática, i.e, <span class="math inline">R_{pred}(g) = \mathbb{E}\left[({\bf Y} - g({\bf X}))^2\right]</span>, em que <span class="math inline">\bf (X, Y)</span> são novas observações que não foram utilizadas para estimar <span class="math inline">g</span>. Suponha também que estimaos o risco de um estimador de regressão <span class="math inline">r({\bf X})</span> via função perda quadrática <span class="math inline">R_{reg}(g) = \mathbb{E}\left[(r({\bf X}) - g({\bf X}))^2\right]</span>. Então,</p>
<p><span class="math display">R_{pred}(g) = R_{reg}(g) + \mathbb{E}\left[\mathbb{V}[{\bf Y} | {\bf X}]\right],</span></p>
<p>em que <span class="math inline">\mathbb{E}\left[\mathbb{V}[{\bf Y} | {\bf X}]\right]</span> é a variância média do modelo que não depende de <span class="math inline">g</span>. Portanto, estimar bem <span class="math inline">r({\bf x})</span> é de fundamental importância para criar uma boa função de predição. Em especial, sob a ótica do risco quadrático, a melhor função de predição para <span class="math inline">\bf Y</span> é a função de regressão <span class="math inline">r({\bf x})</span>, de tal modo que:</p>
<p><span class="math display">\argmin_g R_{pred}(g) = \argmin_g R_{reg}(g) = r({\bf x}).</span></p>
</section>
<section id="função-de-risco-3" class="slide level2">
<h2>Função de risco</h2>
<p><br></p>
<p><strong>Lembre-se</strong>: <span class="math inline">r({\bf x}) = \mathbb{E}(Y | \bf{X} = \bf{x})</span> é a nossa <span class="red">função de regressão</span>.</p>
<p><br></p>
<p>A definição de risco preditivo <span class="math inline">R_{pred}</span>, que também denotaremos simplesmente por <span class="math inline">R</span>, tem um apelo frequentista. Dessa forma, para um novo conjunto com <span class="math inline">m</span> novas observaçõs, <span class="math inline">({\bf X}_{n+1}, Y_{n+1}), \cdots, ({\bf X}_{n+m}, Y_{n+m})</span>, temos que que essa nova amostra é i.i.d à amostra observada (utilizada no treinamento do modelo/na estimação). Então, pela Lei dos Grandes Números, temos que um bom estimador para a função para o risco preditivo é dado por:</p>
<p><span id="eq-risco-correto"><span class="math display">\frac{1}{m}\sum_{i = 1}^m (Y_{n + i} - g(X_{n + i}))^2 \approx R(g) := \mathbb{E}\left[(Y - g({\bf X}))^2\right]. \tag{1}</span></span></p>
<p>Chamaremos a quantidade acima de <a href="https://pt.wikipedia.org/wiki/Erro_quadr%C3%A1tico_m%C3%A9dio">Erro Quadrático Médio - EQM</a>. Em aprendizagem de máquina, normalmente estaremos no contexto em que temos muitas observações, e que portanto, poderemos fazer esse apelo frequentista.</p>
<p><br></p>
<p>Desejamos encontrar <span class="math inline">g</span> (encontrar métodos) que minimize de forma satisfatória <span class="math inline">R</span>, i.e., métodos que nos conduzam à um risco baixo.</p>
</section>
<section id="função-de-risco-4" class="slide level2">
<h2>Função de risco</h2>
<p><br></p>
<p>Sendo assim, se <span class="math inline">R(g)</span> possui um valor baixo, então, temos que</p>
<p><span class="math display">g({\bf x}_{n+1}) \approx y_{n+1}, \cdots, g({\bf x}_{n+m}) \approx y_{n+m}.</span> <br> <img data-src="gifs/hum.gif" style="width:25.0%"></p>
</section>
<section id="regressão-linear" class="slide level2">
<h2>Regressão linear</h2>
<p><br></p>
<p>Nesse momento, vamos pensar um pouco em regressão linear. No caso mais simples, queremos prever o comportamento de uma variável de interesse <span class="math inline">Y</span> condicional a uma variável explicativa <span class="math inline">X</span> (regressão linear simples, i.e., <span class="math inline">d = 1</span>). O melhor preditor de <span class="math inline">Y</span> condicional em <span class="math inline">X</span> é aquele que minimiza a função de perda esperada, ou seja, é aquele que resolve:</p>
<p><span class="math display">\argmin_g \mathbb{E}(L(Y - g)\,|\,X).</span></p>
<p>Para o caso da função perda quadrática (função <span class="math inline">L_2</span>), o melhor preditor de <span class="math inline">Y</span> condicional à <span class="math inline">X</span> é a média condicional de <span class="math inline">Y</span> dado <span class="math inline">X</span>, i.e., <span class="math inline">r(X) = \mathbb{E}(Y\,|\,X)</span>. Já, na situação em que considera-se a perda absoluta (função <span class="math inline">L_1</span>), o melhor estimador é a mediana condicional.</p>
<p><br></p>
<p><strong>Os modelos de regressão, em geral, fazem uso da função de perda quadrática.</strong></p>
</section>
<section id="regressão-linear-simples" class="slide level2">
<h2>Regressão linear simples</h2>
<p><br></p>
<p>No caso da regressão linear simples (<span class="math inline">d = 1</span>), temos que o modelo é dado por:</p>
<p><span class="math display">g(x) = \beta_0 + \beta_1 x_{i,1} + \varepsilon_i, \,\, i = 1, \cdots, n.</span></p>
<p>Assumindo que a regressão linear simples é o modelo <span class="math inline">g</span> que iremos utilizar, então, desejamos minimizar:</p>
<p><span class="math display">\argmin_{\beta} R(g_\beta) = \argmin_{\beta} \sum_{i = 1}^n(y_i - \beta_0 - \beta_1x_{i,1})^2.</span> Derivando em relação à <span class="math inline">\beta</span> e igualando a zero, após algumas manipulações algébricas, temos que:</p>
<p><span class="math display">\widehat{\beta} = \frac{\sum_{i = 1}^n (x_i - \overline{x})(y_i - \overline{y})}{\sum_{i=1}^n(x_i - \overline{x})^2} = r_{xy}\frac{s_y}{s_x},</span> em que <span class="math inline">s_x</span> e <span class="math inline">s_y</span> são os desvio-padrão de <span class="math inline">x</span> e <span class="math inline">y</span>, respectivamente, e <span class="math inline">r_{xy}</span> é o coeficiente de correlação da amostra.</p>
</section>
<section id="regressão-linear-simples-1" class="slide level2">
<h2>Regressão linear simples</h2>
<p><br></p>
<p><span class="math display">r_{xy} = \frac{\overline{xy} - \overline{x}\,\overline{y}}{\sqrt{(\overline{x^2} - \overline{x}^2)(\overline{y^2} - \overline{y}^2)}}.</span> O coeficiente de determinação <span class="math inline">R^2</span> do modelo é dado por <span class="math inline">r_{xy}^2</span>, quando o modelo é linear e possue uma única variável independente (feature).</p>
<p><br></p>
<p>Portanto, temos que:</p>
<p><span class="math display">\widehat{\beta_0} = \overline{y} - \widehat{\beta}\overline{x},</span></p>
<p>Na <span class="red"><em>data modeling culture</em></span> (na estatística), normalmente assumimos que o <span class="math inline">\varepsilon_i</span> tem distribuição normal e variância constante, <span class="math inline">\forall\, i = 1, \cdots, n</span>. Assume-se também que <span class="math inline">\mathbb{E}(\varepsilon_i) = 0, \, \forall i</span>.</p>
</section>
<section id="regressão-linear-simples-2" class="slide level2">
<h2>Regressão linear simples</h2>
<p><br></p>
<p>Aqui não iremos nos preocupar com essas suposições, uma vez que em <span class="red"><em>algorithmic modeling culture</em></span>, não estamos preocupados com suposições nem interpretações, ok!?</p>
<p><br></p>

<img data-src="gifs/ok-2.gif" class="r-stretch"></section>
<section id="regressão-linear-multipla" class="slide level2">
<h2>Regressão linear multipla</h2>
<p><br></p>
<p>A função de perda quadrática (função <span class="math inline">L_2</span>) tem algumas vantagens em relação a função de perda absoluta. Listo algumas:</p>
<ol type="1">
<li class="fragment"><p>A função de perda quadrática penaliza mais os erros maiores, devido ao vato dos erros serem levado ao quadrado;</p></li>
<li class="fragment"><p>A função de perda quadrática é mais sensível a presença de <a href="https://en.wikipedia.org/wiki/Outlier">outlier</a>, que em compensação são menos penalizados ao se considerar a função de perda absoluta (função <span class="math inline">L_1</span>);</p></li>
<li class="fragment"><p>Em situações em que o erro tem distribuição normal, a estimativa de mínimos quadrados é a solução de máxima verossimilhança e é a estimativa linear não viesada e com menor variância. Portanto, gozamos de um estimador com ótimas propriedades, muito embora ele também é um bom estimador mesmo quando a suposição de normalidade não é verificada;</p></li>
<li class="fragment"><p>A função de perda quadrática é deferenciável, já a função de perda absoluta não é.</p></li>
</ol>
<p>Para o caso de regerssão linear múltipla, i.e., quando <span class="math inline">d &gt; 1</span>, poderemos utilizar uma notação matricial para representar o modelo linear múltiplo de regressão.</p>
</section>
<section id="regressão-linear-multipla-1" class="slide level2">
<h2>Regressão linear multipla</h2>
<p><br></p>
<p>Considerando o modelo de regressão linear múltiplo, temos que:</p>
<p><span class="math display">Y = g({\bf X}) = \beta^{T}{\bf X} + \varepsilon,</span></p>
<p>em que <span class="math inline">Y</span> é um vetor <span class="math inline">n \times 1</span>, <span class="math inline">{\bf X}</span> é uma matriz fixa e conhecida com os atributos de dimensão <span class="math inline">n \times d</span>, em que a primeira coluna é preenchida de 1, <span class="math inline">\beta = (\beta_0, \cdots, \beta_d)</span>. Na cultura de machine learning, iremos desconsiderar <span class="math inline">\varepsilon</span>, não feremos suposições sobre <span class="math inline">\varepsilon</span>. Portanto, considere</p>
<p><span class="math display">g({\bf x}) = \beta^{T}{\bf X} = \beta_{0}x_0 + \beta_1x_{i,1} + \cdots + \beta_dx_{i,d},</span> em que <span class="math inline">x_0 \equiv 1</span>.</p>
</section>
<section id="regressão-linear-multipla-2" class="slide level2">
<h2>Regressão linear multipla</h2>
<p><br></p>
<p>O método dos mínimos quadrados, para o caso de regressão linear múltipla (<span class="math inline">d &gt; 1</span>) é dado por aquele que minimiza <span class="math inline">R(\beta^{T}{\bf X})</span>, i.e., minimiza:</p>
<p><span class="math display">\argmin_\beta \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1x_{i,1} - \cdots - \beta_dx_{i,d})^2.</span> Temos que</p>
<p><span class="math display">\widehat{\beta} = ({\bf X}^{T}{\bf X})^{-1}{\bf X}^{T}Y.</span></p>
<p>Portanto, a função de regressão estimada é dada por:</p>
<p><span class="math display">g({\bf x}) = \widehat{\beta}^{T}{\bf x}.</span></p>
</section>
<section id="regressão-linear-multipla-3" class="slide level2">
<h2>Regressão linear multipla</h2>
<p><br></p>
<p>Grande parte da literatura estatística é voltada para justificar que o método de mínimos quadrados sob um ponto de vista de um estimador de máxima verossimilhança, assim como também para construção de testes de aderência, métodos para construção de intervalos de confiança e teste de hipótese para <span class="math inline">\beta_i</span> (parâmetros que indexam o modelo), análise de resíduos, entre outros.</p>
<p><br></p>
<p>Assumir que a verdadeira regressão <span class="math inline">r({\bf x}) = \mathbb{E}({\bf X}\,|\,Y)</span> é uma suposição muito forte. Contudo, existe, na literatura, justificativas para o uso de métodos de mínimos quadrados para estimar os coeficientes, mesmo quando a regressão real <span class="math inline">r({\bf x})</span> não satisfaz a suposição de linearidade.</p>
</section>
<section id="regressão-linear-multipla-4" class="slide level2">
<h2>Regressão linear multipla</h2>
<p><br></p>
<p>O estimador de mínimos quadrados <span class="math inline">\widehat{\beta} = ({\bf X}^{T}{\bf X})^{-1}{\bf X}^{T}Y</span> é bom, por alguns motivos:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>É igual ao estimador de máxima verossimilhança sob normalidade, linearidade e homoscedasticidade, portanto, consistente sob essas condições</p></li>
<li class="fragment"><p>É <span class="red"><em>best linear unbiased prediction</em> - BLUE</span> sob linearidade e homoscedasticidade;</p></li>
<li class="fragment"><p>O método de mínimos quadrados tem alguma garantia, mesmo sem assumir muitas suposições.</p></li>
</ol>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="mínimos-quadrados-sem-suposição-de-linearidade" class="slide level2">
<h2>Mínimos quadrados sem suposição de linearidade</h2>
<p><br></p>
<p>Quando a suposição de linearidade falha, ou seja, quando a regressão verdadeira que desconhecemos <span class="math inline">r({\bf x})</span> não é linear, frequentemente existe um vetor <span class="math inline">\beta_{*}</span>, tal que <span class="math inline">g_{\beta_{*}}({\bf x}) = \beta_{*}^{T}{\bf x}</span> tem um bom poder preditivo. Nesses casos, o métrodo dos mínimos quadrados <span class="math inline">\widehat{\beta}</span> tende a produzir estimadores com baixo risco. Isso se deve ao fato que <span class="math inline">\widehat{\beta}</span> converge para o melhor preditor linear (para o oráculo <span class="math inline">\beta_{*}</span>) que é dado por:</p>
<p><span class="math display">\beta_{*} = \argmin_\beta R(g_\beta) =  \argmin_\beta \mathbb{E}\left[(Y - \beta^{T}X)^2\right],</span> mesmo que a verdadeira regressão <span class="math inline">r({\bf x})</span> não seja linear, em que <span class="math inline">({\bf X}, Y)</span> é uma nova observação.</p>
<p><br></p>
<p><span class="red">Teorema</span>: Seja <span class="math inline">\beta_{*}</span> o melhor estimador linear e <span class="math inline">\widehat{\beta}</span> o estimador de mínimos quadrados. Então,</p>
<p><span class="math display">\widehat{\beta}\overset{p}{\longrightarrow}  \beta_{*}\,\, \mathrm{e}\,\, R(g_{\widehat{\beta}})\overset{p}{\longrightarrow} R(g_{\beta_{*}}), </span> quando <span class="math inline">n \longrightarrow \infty</span>. Para uma demonstração, veja <a href="http://www.rizbicki.ufscar.br/AME.pdf" class="uri">http://www.rizbicki.ufscar.br/AME.pdf</a>, página. 29.</p>
</section>
<section id="mínimos-quadrados-sem-suposição-de-linearidade-1" class="slide level2">
<h2>Mínimos quadrados sem suposição de linearidade</h2>
<p><br></p>
<p>Em palavras, o que o Teorema anterior diz é que mesmo quando a regressão verdadeira não é linear, o estimador de mínimos quadrados é consistente para nos conduzir a um bom estimador <strong>linear</strong>, ou seja, ao menos conseguiremos o melhor estimador linear como uma aproximação à <span class="math inline">r({\bf x})</span> que não é linear.</p>
<p><br></p>
<p>Isso não quer dizer que você terá boas estimativas em todas as situações, muito embora o oráculo <span class="math inline">\beta_{*}</span>, em muitas situações, terá bom poder preditivo. Em outras palavras, em situações que um problema, em sua natureza, não linear, poderemos alcançar boas estimativas por uma aproximação linear pelo método dos mínimos quadrados.</p>
<p><br></p>

<img data-src="gifs/uau.gif" class="r-stretch"></section>
<section id="predição-versus-inferência" class="slide level2">
<h2>Predição versus Inferência</h2>
<p><br></p>
<p><strong>Inferência</strong>: assume que o modelo linear é correto. O principal objetivo consiste em interpretar os parâmetros:</p>
<p><br></p>
<ul>
<li class="fragment">Quais são os parâmetros significantes?</li>
<li class="fragment">Qual o efeito do aumento da dose de um remédio no paciente?</li>
</ul>
<p><br></p>
<p><strong>Predição</strong>: queremos criar <span class="math inline">g({\bf x})</span> com bom poder preditivo, mesmo que a especificação do modelo não esteja correta. Não assume que a verdadeira regressão é de fato linear! A interpretação aqui não é o foco. Tudo bem?</p>
<p><br></p>

<img data-src="gifs/ok.gif" style="width:15.0%" class="r-stretch"></section>
<section id="ajustando-uma-regressão-linear-no-r" class="slide level2">
<h2>Ajustando uma regressão linear no R</h2>
<p><br></p>
<p>Caso você não queira implementar o estimador de mínimos quadrados <span class="math inline">\widehat{\beta} = ({\bf X}^{T}{\bf X})^{-1}{\bf X}^{T}Y</span>, você poderá utilizar a famosa função <code>lm</code>. Na verdade é melhor que não implemente o estimador <span class="math inline">\widehat{\beta}</span>, uma vez que a função <code>lm</code>, assim como a função <code>glmnet</code> do pacote <a href="https://glmnet.stanford.edu/articles/glmnet.html">glmnet</a>, utilizam-se de truques numéricos para um cálculo mais eficiente.</p>
<p><br></p>
<p>Falaremos do pacote <a href="https://glmnet.stanford.edu/articles/glmnet.html">glmnet</a>, um pouco mais a frente, quando abordarmos regressão penalizada. Certo!?</p>
<p><br></p>

<img data-src="gifs/bom.gif" class="r-stretch"></section>
<section id="ajustando-uma-regressão-linear-no-r-1" class="slide level2">
<h2>Ajustando uma regressão linear no R</h2>
<p><br></p>
<p>Considere o conjunto de dados de expectativa de vida versus PIB per Capita disponíveis <a href="https://github.com/prdm0/dados/blob/main/dados_expectativa_renda.RData">aqui</a>. O comportamente entre as variáveis <code>LifeExpectancy</code> e <code>GDPercapita</code>, se fizermos um gráfico, não é linear.</p>
<p><br></p>
<p>Todavia, isso não impede que possamos ajustar um modelo de regerssão linear, muito embora o seu poder preditivo será baixo.</p>
<p><br></p>
<p>Porém, como já sabemos, ao menos conseguiremos o melhor oráculo, denotado por <span class="math inline">\beta_{*}</span>, i.e., o melhor estimador dentre os possíveis estimadores lineares, como mostrado em teoremas anteriores.</p>
<p><br></p>
<p>E está tudo bem. Aqui não estou querendo defender que você use uma aproximação linear para esse caso. Em breve, com um pequeno truque, poderemos ajustar uma regressão polinomial à esses dados, e incorporaremos um pouco da tendência não linar presente nos dados.</p>
</section>
<section id="ajustando-uma-regressão-linear-no-r-2" class="slide level2">
<h2>Ajustando uma regressão linear no R</h2>
<p><br></p>
<div class="cell" data-layout-align="center">
<details>
<summary>Veja o código do gráfico</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb3-2"><a href="#cb3-2"></a></span>
<span id="cb3-3"><a href="#cb3-3"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="co"># Criando um arquivo temporário</span></span>
<span id="cb3-6"><a href="#cb3-6"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb3-7"><a href="#cb3-7"></a></span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="co"># Baixando um arquivo temporário</span></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb3-10"><a href="#cb3-10"></a></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="co"># Carregando os dados</span></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb3-13"><a href="#cb3-13"></a></span>
<span id="cb3-14"><a href="#cb3-14"></a>dados_expectativa_renda <span class="sc">|&gt;</span> </span>
<span id="cb3-15"><a href="#cb3-15"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> GDPercapita, <span class="at">y =</span> LifeExpectancy)) <span class="sc">+</span></span>
<span id="cb3-16"><a href="#cb3-16"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb3-17"><a href="#cb3-17"></a>  <span class="fu">labs</span>(</span>
<span id="cb3-18"><a href="#cb3-18"></a>    <span class="at">title =</span> <span class="st">"PIB per Capita versus Expectativa de Vida"</span>,</span>
<span id="cb3-19"><a href="#cb3-19"></a>    <span class="at">x =</span> <span class="st">"PIB per Capita"</span>,</span>
<span id="cb3-20"><a href="#cb3-20"></a>    <span class="at">y =</span> <span class="st">"Expectativa de Vida"</span></span>
<span id="cb3-21"><a href="#cb3-21"></a>  ) <span class="sc">+</span></span>
<span id="cb3-22"><a href="#cb3-22"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb3-23"><a href="#cb3-23"></a>  <span class="fu">theme</span>(</span>
<span id="cb3-24"><a href="#cb3-24"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>),</span>
<span id="cb3-25"><a href="#cb3-25"></a>    <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb3-26"><a href="#cb3-26"></a>  )</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="index_files/figure-revealjs/unnamed-chunk-4-1.png" width="1200" height="700"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="ajustando-uma-regressão-linear-no-r-3" class="slide level2">
<h2>Ajustando uma regressão linear no R</h2>
<p><br></p>
<p>Claramente, a reta de regressão (linha azul) do gráfico anterior não tem um bom poder preditivo. O ajuste foi feito diretamente usando o pacote <a href="https://ggplot2.tidyverse.org/">ggplot2</a>, utilizando a função <code>geom_smooth</code>, em que foi escolhido o método <code>"lm"</code>.</p>
<p><br></p>
<p>Poderíamos ter utilizado a função <code>lm</code>:</p>
<p><br></p>
<div class="cell" data-layout-align="center">
<details>
<summary>Veja o código do gráfico</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb4-2"><a href="#cb4-2"></a></span>
<span id="cb4-3"><a href="#cb4-3"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="co"># Criando um arquivo temporário</span></span>
<span id="cb4-6"><a href="#cb4-6"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb4-7"><a href="#cb4-7"></a></span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="co"># Baixando um arquivo temporário</span></span>
<span id="cb4-9"><a href="#cb4-9"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb4-10"><a href="#cb4-10"></a></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="co"># Carregando os dados</span></span>
<span id="cb4-12"><a href="#cb4-12"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb4-13"><a href="#cb4-13"></a></span>
<span id="cb4-14"><a href="#cb4-14"></a><span class="co"># Ajustando o modelo usando a função lm</span></span>
<span id="cb4-15"><a href="#cb4-15"></a>ajuste <span class="ot">&lt;-</span> <span class="fu">lm</span>(LifeExpectancy <span class="sc">~</span> GDPercapita, <span class="at">data =</span> dados_expectativa_renda)</span>
<span id="cb4-16"><a href="#cb4-16"></a></span>
<span id="cb4-17"><a href="#cb4-17"></a>modelo <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb4-18"><a href="#cb4-18"></a>  novos_dados <span class="ot">&lt;-</span> tibble<span class="sc">::</span><span class="fu">tibble</span>(<span class="at">GDPercapita =</span> x)</span>
<span id="cb4-19"><a href="#cb4-19"></a>  <span class="fu">predict</span>(ajuste, <span class="at">newdata =</span> novos_dados)</span>
<span id="cb4-20"><a href="#cb4-20"></a>}</span>
<span id="cb4-21"><a href="#cb4-21"></a></span>
<span id="cb4-22"><a href="#cb4-22"></a>dados_expectativa_renda <span class="sc">|&gt;</span> </span>
<span id="cb4-23"><a href="#cb4-23"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> GDPercapita, <span class="at">y =</span> LifeExpectancy)) <span class="sc">+</span></span>
<span id="cb4-24"><a href="#cb4-24"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb4-25"><a href="#cb4-25"></a>  <span class="fu">labs</span>(</span>
<span id="cb4-26"><a href="#cb4-26"></a>    <span class="at">title =</span> <span class="st">"PIB per Capita versus Expectativa de Vida"</span>,</span>
<span id="cb4-27"><a href="#cb4-27"></a>    <span class="at">x =</span> <span class="st">"PIB per Capita"</span>,</span>
<span id="cb4-28"><a href="#cb4-28"></a>    <span class="at">y =</span> <span class="st">"Expectativa de Vida"</span></span>
<span id="cb4-29"><a href="#cb4-29"></a>  ) <span class="sc">+</span></span>
<span id="cb4-30"><a href="#cb4-30"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> modelo, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">size =</span> <span class="fl">1.2</span>) <span class="sc">+</span></span>
<span id="cb4-31"><a href="#cb4-31"></a>  <span class="fu">theme</span>(</span>
<span id="cb4-32"><a href="#cb4-32"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>),</span>
<span id="cb4-33"><a href="#cb4-33"></a>    <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb4-34"><a href="#cb4-34"></a>  )</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="index_files/figure-revealjs/unnamed-chunk-5-1.png" width="1200" height="700"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="matriz-esparsa" class="slide level2">
<h2>Matriz esparsa</h2>
<p><br></p>
<p>Para grandes bases de dados, em um problema real que você venha trabalhar, e se o custo computacional você considera elevado, poderá utilizar o pacote <a href="https://cran.r-project.org/web/packages/biglm/index.html">biglm</a>.</p>
<p><br></p>
<p>Em situações em que há muitos zeros na sua matriz, poderá utilizar representação <a href="https://en.wikipedia.org/wiki/Sparse_matrix">esparsa</a>.</p>
<p><br></p>
<p><span class="red">Matrizes esparsas</span> são matrizes com muitas entradas iguais à <span class="math inline">0</span>. Elas ocorrem naturalmente em diversas aplicações, como por exemplo uma matriz de termos presentes em um documento, em que se o termo estiver no documento resebe 1, e zero, caso contrário. Abaixo, <span class="math inline">{\bf X}</span> é um exemplo de matriz esparsa.</p>
<p><br></p>
<p><span class="math display">
{\bf X} =
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 3 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 4 &amp; 0 \\
\end{bmatrix}
</span></p>
</section>
<section id="matriz-esparsa-1" class="slide level2">
<h2>Matriz esparsa</h2>
<p><br></p>
<p>Considere os textos:</p>
<ol type="1">
<li class="fragment"><span class="red">Texto 1</span>: “Eu amo essa disciplina.”</li>
<li class="fragment"><span class="red">Texto 2</span>: “Eu adoro meu professor.”</li>
<li class="fragment"><span class="red">Texto 3</span>: “Eu serei muito bom em aprendizagem de máquina.”</li>
<li class="fragment"><span class="red">Texto 4</span>: “Adoro o departamento de estatística da UFPB.”</li>
</ol>
<p><br></p>
<table>
<colgroup>
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th>Textos</th>
<th>disciplina</th>
<th>amo</th>
<th>aprendizagem</th>
<th>máquina</th>
<th>estatistica</th>
<th>adoro</th>
<th>UFPB</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="red">Texto 1</span></td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td><span class="red">Texto 2</span></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td><span class="red">Texto 3</span></td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td><span class="red">Texto 4</span></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</section>
<section id="matriz-esparsa-2" class="slide level2">
<h2>Matriz esparsa</h2>
<p><br></p>
<p>A matriz com a ocorrência de determinados termos nos textos é dada por:</p>
<p><span class="math display">
{\bf X} =
\begin{bmatrix}
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\
\end{bmatrix}
</span></p>
<p>A representação esparsa de <span class="math inline">{\bf X}</span>, aqui denotada por <span class="math inline">{\bf X_*}</span> é:</p>
<p><span class="math display">
{\bf X_*} =
\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
2 &amp; 6 &amp; 1 \\
3 &amp; 3 &amp; 1 \\
3 &amp; 4 &amp; 1 \\
4 &amp; 5 &amp; 1 \\
4 &amp; 6 &amp; 1 \\
4 &amp; 7 &amp; 1 \\
\end{bmatrix},
</span> em que as duas primeiras colunas, são as linhas e colunas de <span class="math inline">{\bf X}</span> com valor diferente de 0. A última coluna representa o valor.</p>
</section>
<section id="regressão-linear-com-matriz-esparsa" class="slide level2">
<h2>Regressão linear com matriz esparsa</h2>
<p><br></p>
<p><strong>Exemplo</strong>: Ajuste de um modelo de regerssão linear múltiplo, em que <span class="math inline">{\bf X}</span> poderá ter uma representação esparsa. Aqui não estamos interessados em verificar qualidade de predições. Trata-se apenas de um exemplo de como utilizar uma representação esparsa para ajustar um modelo de regessão linear com algumas covariáveis, em R.</p>
<p><br></p>
<div class="cell">
<details>
<summary>Estude o código</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="fu">library</span>(Matrix)</span>
<span id="cb5-3"><a href="#cb5-3"></a></span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="co"># Dados de exemplo</span></span>
<span id="cb5-5"><a href="#cb5-5"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb5-6"><a href="#cb5-6"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">0</span>)</span>
<span id="cb5-7"><a href="#cb5-7"></a>x3 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">7</span>)</span>
<span id="cb5-8"><a href="#cb5-8"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb5-9"><a href="#cb5-9"></a></span>
<span id="cb5-10"><a href="#cb5-10"></a><span class="co"># Criar data frame com as variáveis explicativas</span></span>
<span id="cb5-11"><a href="#cb5-11"></a>dados <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x1, x2, x3)</span>
<span id="cb5-12"><a href="#cb5-12"></a></span>
<span id="cb5-13"><a href="#cb5-13"></a><span class="co"># Converter o data frame para matriz esparsa</span></span>
<span id="cb5-14"><a href="#cb5-14"></a>X <span class="ot">&lt;-</span> <span class="fu">sparse.model.matrix</span>(<span class="sc">~</span> ., <span class="at">data =</span> dados)</span>
<span id="cb5-15"><a href="#cb5-15"></a></span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="co"># Ajustar a regressão linear utilizando glmnet</span></span>
<span id="cb5-17"><a href="#cb5-17"></a>modelo <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> X, <span class="at">y =</span> y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> <span class="dv">0</span>)</span>
<span id="cb5-18"><a href="#cb5-18"></a></span>
<span id="cb5-19"><a href="#cb5-19"></a><span class="co"># Realizar previsões</span></span>
<span id="cb5-20"><a href="#cb5-20"></a>predicoes <span class="ot">&lt;-</span> <span class="fu">predict</span>(modelo, <span class="at">newx =</span> X)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="erro-quadrático-médio" class="slide level2">
<h2>Erro quadrático médio</h2>
<p><br></p>
<p>Como exposto anteriormente, para avaliar o poder preditivo de uma modelo, i.e., a aprendizagem de um modelo, devemos avaliar a função de risco, i.e., devemos avaliar <span class="math inline">R(g) := \mathbb{E}\left[L(g({\bf X}); Y)\right]</span>. Em particular, considere <span class="math inline">L = L_2</span> (função perda quadrática). Então, poderíamos ser levados a acreditar que o melhor estimador de <span class="math inline">R(g)</span>, utilizando a Lei dos Grandes Números seria:</p>
<p><span class="math display">\frac{1}{n}\sum_{i = 1}^n(Y_{i} - g({\bf X_{i}}))^2 \approx R(g) := \mathbb{E}\left[L_2(g({\bf X}); Y)\right].</span></p>
<p><br></p>
<p>Essa quantidade é chamada, de <strong>E</strong>rro <strong>Q</strong>uadrático <strong>M</strong>édio - <strong>EQM</strong>. Desejamos escolher o melhor mode, entre os modelos testados, que minimiza o EQM.</p>
<p><br></p>
<p>O apelo frequentista em utilizar a Lei dos Grandes Números na forma acima não é correto, uma vez que usamos as <span class="math inline">n</span> observações para treinar/ajustar o modelo <span class="math inline">g</span>.</p>
</section>
<section id="erro-quadrático-médio-1" class="slide level2">
<h2>Erro quadrático médio</h2>
<p><br></p>
<p>Por exemplo, no problema de PIB per Capita versus expectativa de vida, em que consideramos uma aproximação linear, não poderíamos utilizar o EQM da forma acima, com as <span class="math inline">n</span> observações utilizadas para treinar o modelo. É um detalhe sutil, mas que muitas pessoas cometem esse erro.</p>
<p><br></p>
<p>Não podemos utilizar as <span class="math inline">n</span> observações para estimar o risco <span class="math inline">R(g)</span> através do EQM, uma vez que estamos utilizando o mesmo conjunto de dados para ajustar e avaliar <span class="math inline">g</span>.</p>
<p><br></p>
<p><strong>Qual o problema?</strong></p>
<p><br></p>
<ol type="1">
<li class="fragment">Não vale a Lei dos Grandes Números;</li>
<li class="fragment">Usamos os mesmos valores de <span class="math inline">{\bf x}</span> e <span class="math inline">y</span> para treinar e avaliar o modelo.</li>
</ol>
</section>
<section id="erro-quadrático-médio-2" class="slide level2">
<h2>Erro quadrático médio</h2>
<p><br></p>
<p>O que diz a Lei dos Grandes Números, em particular, a Lei Forte de Kolmogorov?</p>
<p><br></p>
<p><span class="red">Teorema</span> (<strong>Lei Forte de Kolmogorov</strong>): Sejam <span class="math inline">X_1, \cdots, X_n</span> uma sequência de veriáveis aleatórias - v.a. i.i.d. e integráveis, i.e., com valor esperado limitado, tal que <span class="math inline">\mathbb{E}(X) = \mu\,\, \forall i</span>. Então,</p>
<p><span class="math display">\frac{X_1 + X_2 + \cdots + X_n}{n} \rightarrow \mu,</span></p>
<p>quase certamente, i.e., com probabilidade 1.</p>
<p><br></p>
<p>Note que se desejamos comparar diversos modelos, <span class="math inline">g_1({\bf x}), g_2({\bf x}), \cdots,</span> e se utilizarmos as mesmas <span class="math inline">n</span> obervações para calularmos <span class="math inline">R(g_1({\bf x})), R(g_2({\bf x})), \cdots</span>, os termos de cada uma das somas <strong>não são independentes</strong>. Lembre-se que desejamos obter <span class="math inline">\argmin_g R_{pred}(g)</span>.</p>
</section>
<section id="erro-quadrático-médio-3" class="slide level2">
<h2>Erro quadrático médio</h2>
<p><br></p>
<p>Portanto, nunca utilize as mesmas observações utilizadas para treinar o modelo, como aquelas que serão utilizadas para se estimar <span class="math inline">R(g)</span>. Nunca! Isso é um pecado mortal! Ok?!</p>
<p><br></p>

<img data-src="gifs/thumbs-up-nod.gif" style="width:20.0%" class="r-stretch"></section>
<section id="data-splitting" class="slide level2">
<h2>Data Splitting</h2>
<p><br></p>
<p>Corrigir o problema de dependência que há ao estimarmos o risco usando o EQM é fácil. Uma abordagem muito utilizada é utilizar <span class="red"><em>data splitting</em></span>, também chamado de método <span class="red">hold-out</span>. Algo como a segunda linha da imagem abaixo:</p>
<p><br></p>

<img data-src="imgs/train-and-test-1-min-1.webp" style="width:35.0%" class="r-stretch"></section>
<section id="data-splitting-1" class="slide level2">
<h2>Data Splitting</h2>
<p><br></p>
<p>Essa divisão é feita de forma aleatória, algumas vezes estratificada de acordo com algumas variáváveis. A ideia de aleatorizar é se livrar de problemas de conjunto de dados ordenados. Queremos que tanto no conjunto de treinamento <span class="red"><em>Training</em></span> quanto no conjunto de teste <span class="red"><em>Testing</em></span>, na imagem, contenham a mesma diversidade de observações.</p>
<p><br></p>
<p>Por exemplo, ainda no exemplo de PIB per Capita versus Expectaitiva de Vida, não quero correr o risco de ter no conjunto de treinamento apenas o países com maiores valores de PIB per Capita, caso o conjunto de dados tenha sido ordenado pela variável <code>GDPercapita</code>. Por isso aleatorizar o conjunto de treinamento e teste é simple uma ótima ideia. Certo!?</p>
<p><br></p>

<img data-src="gifs/ok-2.gif" style="width:20.0%" class="r-stretch"></section>
<section id="data-splitting-2" class="slide level2">
<h2>Data Splitting</h2>
<p><br></p>
<p>O percentual de divisão dos dados normalmente é empírico. Usa-se normalmente a proporção de <span class="math inline">70\%</span> para treinamento e <span class="math inline">30\%</span> para teste <span class="math inline">(70\%, 30\%)</span>. Outros esquemas de divisões são bastante utilizados, por exemplo, <span class="math inline">(80\%, 20\%)</span>, <span class="math inline">(99\%, 1\%)</span>, a depender da quantidade de observações (tamanho do conjunto de dados).</p>
<p><br></p>
<p>Portanto, utilizar o EQM sob o conjunto de dados de teste para avaliar <span class="math inline">g_1({\bf x}), g_2({\bf x}), \cdots,</span>, é uma boa estratégia, uma vez que agora não teremos mais uma dependência no numerador do cálculo do EQM. Em notação matemática, poderíamos escrever como já apresentado anteriormente, em <a href="#/função-de-risco-3" class="quarto-xref">Equação&nbsp;1</a>, i.e,</p>
<p><span class="math display">\frac{1}{m}\sum_{i = 1}^m (Y_{n + i} - g(X_{n + i}))^2 \approx R(g) := \mathbb{E}\left[(Y - g({\bf X}))^2\right].</span></p>
<p><br></p>
<p>Esse resultado valeria para qualquer outra função de perda.</p>
</section>
<section id="data-splitting-3" class="slide level2">
<h2>Data Splitting</h2>
<p>Reescrevendo, suponha que o conjunto de dados total possua <span class="math inline">n</span> observações e que separamos aleatoriamente <span class="math inline">s &lt; n</span> observações para o conjunto de treinamento. Assim, temos, algo como:</p>
<p><br></p>
<p><span class="math display">\overbrace{(X_1, Y_1), (X_2, Y_2), \cdots, (X_s, Y_s)}^{70\%}, \,\,\, \overbrace{(X_{s + 1}, Y_{s + 1}), (X_{s + 2}, Y_{s + 2}), \cdots, (X_n, Y_n)}^{30\%}.</span></p>
<p><br></p>
<p>Então, temos que uma boa estimativa de <span class="math inline">R(g)</span> é dada pelo EQM calculado sobre o conjunto de dados de teste, que nesse caso considerei o conjunto com <span class="math inline">30\%</span>, mas esse percentual poderia ser outro. Então, temos que um bom estimador é:</p>
<p><span class="math display">\frac{1}{n - s}\sum_{i = s + 1}^n (Y_{i} - g(X_{i}))^2 \approx R(g) := \mathbb{E}\left[(Y - g({\bf X}))^2\right].</span></p>
</section>
<section id="data-splitting-4" class="slide level2">
<h2>Data Splitting</h2>
<p><br></p>
<p><strong>Agora você entende por que dividimos os dados em treinamento e teste?</strong></p>
<p><br></p>

<img data-src="gifs/yes.gif" style="width:25.0%" class="r-stretch"><p><br></p>
<p>Dividimos para obermos um bom estimador do risco utilizando o <a href="https://en.wikipedia.org/wiki/Mean_squared_error">EQM</a>. 🎊</p>
</section>
<section id="data-splitting-5" class="slide level2">
<h2>Data Splitting</h2>
<p><br></p>
<p>Podemos argumentar que o procedimento de <em>data splitting</em>, em que dividimos o nosso conjunto de dados em treinamento e teste fará com que venhamos perder muitas observações que poderiam ter sido utilizadas para treinar o modelo. E de certa forma isso é verdade, principalmente quando termos um conjunto não muito grande de observações.</p>
<p><br></p>
<p>Portanto, uma melhor abordagem, sendo esta uma variação do método de <em>data splitting</em> é o procedimento de <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)"><em>cross-validation - cv (validação cruzada)</em></a>. Uma versão mais geral de uma validação cruzada é o <span class="red"><em>leave-one-out cross-validation</em></span>.</p>
<p><br></p>
<p>Em palavras, o procedimento consiste em tirar de fora uma única observação das <span class="math inline">n</span> observações da base de dados para ser o nosso conjunto de teste e treinar o modelo com as observações que permaneceram. Daí, calcula-se o <strong>risco observado</strong> (EQM, sob o conjunto de teste). Na segunda iteração, a observação que antes era de teste volta para perterncer ao conjunto de treinamento e uma nova observação é removida para ser teste. Esse procedimento ocorre de forma iterativa até a retirada da última observação como teste.</p>
</section>
<section id="leave-one-out-cross-validation" class="slide level2">
<h2>Leave-one-out cross-validation</h2>
<p><br></p>
<p>Observe a animação abaixo que ilustra o procedimento de <strong>l</strong>eave-<strong>o</strong>ne-<strong>o</strong>ut <strong>c</strong>ross-<strong>v</strong>alidation - LOOCV, em uma amostra de tamanho <span class="math inline">n = 8</span>. Ao fim, teremos <span class="math inline">n</span> modelos ajustados, em que calculamos as suas respectivas performances, i.e., com o risco observado, estimamos o risco de <span class="math inline">R(g)</span>.</p>
<p><br></p>

<img data-src="gifs/LOOCV.gif" class="r-stretch"></section>
<section id="leave-one-out-cross-validation-1" class="slide level2">
<h2>Leave-one-out cross-validation</h2>
<p><br></p>
<p>Vejo muitas pessoas que usam uma validação cruzada, por exemplo, leave-one-out cross-validation - LOOCV comparando com o método Jackknife e algumas inclusive dizendo ser a mesma coisa. Não, não são!</p>
<p><br></p>
<p>O algoritmo Jackknife é um procedimento de estimação e que por sua vez deve estar dentro do conjunto de treinamento. Para haver algum Jackknife, a estimativa com <span class="math inline">n-1</span> observações deve estar dentro do conjunto de treinamento, em que dentro do treinamento teria a remoção de um observação por vez. <strong>Consegue perceber a diferença sutil?</strong></p>
<p><br></p>

<img data-src="gifs/bean_01.gif" style="width:20.0%" class="r-stretch"></section>
<section id="leave-one-out-cross-validation-2" class="slide level2">
<h2>Leave-one-out cross-validation</h2>
<p><br></p>
<p>O método LOOCV foi proposto por Stones (1974), no artigo intitulado Cross-Validatory Choice and Assessment of Statistical Predictions, no Royal Statistical Society, Série B. Clique <a href="https://www.jstor.org/stable/pdf/2984809.pdf?refreqid=excelsior%3A3071b86b3588905b095d44668025b005&amp;ab_segments=&amp;origin=&amp;initiator=&amp;acceptTC=1">aqui</a> se tiver curiosidade em ler o artigo.</p>
<p><br></p>
<p>Escrevendo o estimador do risco em um procedimento de LOOCV, temos que:</p>
<p><span class="math display">\widehat{R}(g) = \frac{1}{n}\sum_{i = 1}^n (Y_i - g_{-i}({\bf X}_i))^2,</span> em que <span class="math inline">g_{-i}(\bf{X}_i)</span>, representa o ajuste do modelo no conjunto de dados sem a <span class="math inline">i</span>-ésima observação.</p>
<p><br></p>
<p>Não é difícil perceber que a depender do valor de <span class="math inline">n</span>, o método LOOCV é computacionalmente intensivo. O método requer que ajustemos <span class="math inline">n</span> modelos. Em algumas situações isso não é um grande problema, porém, em diversas outras pode ser impeditivo utilizar o LOOCV. 🤯</p>
</section>
<section id="k-fold-cross-validation" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p><br></p>
<p>Uma alternativa ao LOOCV é utilizar o método <span class="math inline">k</span>-fold cross-validation. Nessa abordagem, dividimos o conjunto de dados em <span class="math inline">k</span>-folds (lotes) disjuntos e com aproximadamente o mesmo tamanho. Dessa forma, temos <span class="math inline">L_1, \cdots, L_k \subset \{1, \cdots, n\}</span> são, cada um, um conjunto de indices aleatórios associados a cada um dos lotes. A ideia aqui é construir <span class="math inline">k</span> estimadores da função de regressão, denotados por <span class="math inline">\widehat{g}_{-1}, \cdots, \widehat{g}_{-k}</span>, em que <span class="math inline">\widehat{g}_{-j}</span> é criado usando todas as observações do banco de dados, com exceção daquelas do lote <span class="math inline">L_j</span>, utilizado para <strong>validação</strong>. O estimador do risco é dado por:</p>
<p><span class="math display">\widehat{R}(g) = \frac{1}{n}\sum_{j=1}^k \sum_{i \in L_j}(Y_i - g_{-j}({\bf X}_i))^2.</span> Perceba que, que o LOOCV é um caso particular do <span class="math inline">k</span>-fold cross-validation, quando fazemos <span class="math inline">k = n</span>. Em outras palavras, <span class="math inline">L_1, \cdots, L_k \subset \{1, \cdots, n\}</span> representam os índices aleatórios do conjunto de treinamento nos <span class="math inline">k</span> lotes.</p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="k-fold-cross-validation-1" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p>A animação abaixo, ilustra o procedimento de <span class="math inline">3</span>-fold cross-validation (<span class="math inline">k = 3</span>), para uma amostra de tamanho <span class="math inline">n = 12</span> observações. Note que os valores que pertencem a cada um dos lotes são aleatórios. Portanto, o procedimento LOOCV é deterministico, já o procedimento de <span class="math inline">k</span>-fold cross-validation é randomizado.</p>
<p><br></p>

<img data-src="gifs/KfoldCV.gif" class="r-stretch"><p>Perceba que teremos agora apenas <span class="math inline">3</span> modelos. Para cada um desses lotes, calulamos o EQM com o conjunto de teste (parte <span class="trueblue">azul</span>) e treinamos o modelo com o conjunto de treinamento (parte <span class="truered">vermelha</span>).</p>
</section>
<section id="k-fold-cross-validation-2" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p><br></p>
<p>Muitos modelos mais sofisticados apresentam hiperparâmetros (parâmetros de sintonização) que não dependem dos dados. É muito comum os algoritmos de aprendizagem de máquina se utilizarem do procedimento de validação cruzada, para além da estimação do risco <span class="math inline">R(g)</span>.</p>
<p><br></p>
<p>Ao estimar <span class="math inline">k</span> modelos, normalmente faz-se um grid de possíveis valores para esses hiperparâmetros em que ao final, escolhe-se como hiperparâmetro o modelo com menor EQM. Por fim, ajusta-se um modelo final, com todo o conjunto de treinamento usando o valor do hiperparâmetro que retornou o menor EQM no conjunto de validação.</p>
<p><br></p>

<img data-src="gifs/ok.gif" class="r-stretch"></section>
<section id="k-fold-cross-validation-3" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p>A imagem abaixo ilustra o procedimento <span class="math inline">k</span>-fold cross-validation, em que uma <span class="math inline">5</span>-fold cross-validation é realizada dentro do conjunto de treinamento. Em cada <em>split</em>, o conjunto verde de observações (fold <span class="green">verde</span>) são utilizados para treinar/ajustar o modelo e o conjunto <span class="trueblue">azul</span>, em cada um dos <em>splits</em> é utilizado para avaliar o risco preditivo <span class="math inline">R(g)</span> (através, por exemplo do EQM).</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="imgs/grid_search_cross_validation.png" width="800"> <img data-src="gifs/hum.gif"></p>
</div><div class="column" style="width:60%;">
<p>Não confunda os folds azuis com o conjunto de teste (<span class="trueblue">Test data</span>), este último utilizado por fim, depois do modelo pronto, para avaliar o desempenho do modelo treinado.</p>
<p>Note também que a validação cruzada também é utilizada para o ajuste de hiperparâmetros, que são parâmetros de sintonização que não dependem dos dados para serem equalizados. Por exemplo, em uma regressão lasso, que veremos adiante, há o hiperparâmetro <span class="math inline">\lambda</span> que precisamos obter, normalmente por meio de um <span class="red">grid search</span> (sequência finita), por exemplo, <span class="math inline">\lambda \in [0.5, 1, 1.5, 2, 2.5]</span> de possíveis valores. Cada <em>split</em> pode ser utilizado para avaliar um valor de <span class="math inline">\lambda</span>, dos possíveis valores dispostos no grid. Aumentaríamos a quantidade de splits para mais valores de <span class="math inline">\lambda</span> na sequência.</p>
</div>
</div>
</section>
<section id="resumindo-data-splitting-validação-cruzada-e-conjunto-de-validação" class="slide level2">
<h2>Resumindo: data splitting, validação cruzada e conjunto de validação</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="imgs/data_split_validation_cross.png"></p>
</div><div class="column" style="width:50%;">
<p>O simples procedimento de dividir o conjunto de dados em dois, uma parte para treinar o modelo e a outra parte (conjunto de teste) para estimar o risco <span class="math inline">R(g)</span> é denominado de <span class="red">data splitting</span> ou <span class="red">Hold-out Method</span>. É um procedimento mais simples, porém, pode não ser útil em conjunto de dados não muito grandes.</p>
<p>A segunda linha da ilustração, demonstra o procedimento de cross-validation (validação cruzada), procedimento mais utilizado nos treinamentos de modelos de aprendizagem de máquina.</p>
<p>A terceira linha é uma abordagem também utilizada, porém não tão interessante quanto a validação cruzada. Nessa abordagem o banco de dados é dividido aleatoriamente em três partes. Traina-se o modelo com a parte <span class="green">verde</span>, estima-se o risco com o conjunto de validação amarelo e testa-se o modelo com o conjunto de teste.</p>
</div>
</div>
</section>
<section id="resumindo-data-splitting-validação-cruzada-e-conjunto-de-validação-1" class="slide level2">
<h2>Resumindo: data splitting, validação cruzada e conjunto de validação</h2>
<p><br> A abordagem do conjunto de validação envolve dividir o conjunto de treinamento em duas partes: uma parte é usada para treinar o modelo e a outra parte é usada para avaliar o desempenho do modelo. O conjunto de validação é utilizado para ajustar os hiperparâmetros do modelo, como a taxa de aprendizado, o número de camadas ocultas em uma rede neural, entre outros. Após o ajuste dos hiperparâmetros, o modelo final é treinado com o conjunto de treinamento completo e avaliado em um conjunto separado chamado conjunto de teste. Essa abordagem é conhecida como divisão simples de treinamento/validação/teste.</p>
<p><br></p>
<p>Por outro lado, a validação cruzada <span class="math inline">k</span>-fold é uma abordagem que visa obter uma estimativa mais robusta do desempenho do modelo. Nessa abordagem, o conjunto de treinamento é dividido em k subconjuntos (folds) de tamanho aproximadamente igual. O modelo é treinado <span class="math inline">k</span> vezes, cada vez usando <span class="math inline">k-1</span> folds como conjunto de treinamento e <span class="math inline">1</span> fold como conjunto de validação. O desempenho do modelo é então calculado como a média dos resultados obtidos em cada iteração. Isso permite avaliar o modelo de forma mais precisa, pois utiliza todos os dados para treinamento e validação, evitando a dependência de uma única divisão do conjunto de treinamento.</p>
</section>
<section id="resumindo-data-splitting-validação-cruzada-e-conjunto-de-validação-2" class="slide level2">
<h2>Resumindo: data splitting, validação cruzada e conjunto de validação</h2>
<p><br></p>
<p>A validação cruzada <span class="math inline">k</span>-fold é particularmente útil quando o conjunto de dados é limitado, pois aproveita ao máximo os dados disponíveis. Além disso, ela permite verificar se o modelo é estável e se seu desempenho varia significativamente com diferentes divisões dos dados. É importante ressaltar que a validação cruzada <span class="math inline">k</span>-fold pode ser computacionalmente mais cara do que a abordagem do conjunto de validação, uma vez que envolve treinar e avaliar o modelo várias vezes.</p>
<p><br></p>
<p><img data-src="gifs/mr-bean-pivot.gif" style="width:40.0%"></p>

<p><img src="https://www.ufpb.br/de/contents/imagens/logode.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://www.ufpb.br/de">Departamento de Estatística da UFPB</a></p>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-pointer/pointer.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-attribution/attribution.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': true,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'pointer': {"key":"q","color":"red","pointerSize":16,"alwaysVisible":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, RevealPointer, RevealAttribution, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copiada");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copiada");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>