<!DOCTYPE html>
<html lang="pt"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/quarto-contrib/roughnotation-0.5.1/rough-notation.iife.js"></script>
<script src="site_libs/quarto-contrib/roughnotation-init-1.0.0/rough.js"></script><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.176">

  <meta name="author" content="Prof.&nbsp;Dr.&nbsp;Pedro Rafael D. Marinho Departamento de Estatística - UFPB ">
  <meta name="dcterms.date" content="2023-08-09">
  <title>Machine Learning / Aprendizagem de Máquina</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-pointer/pointer.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-attribution/attribution.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="site_libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
  <script src="site_libs/jquery-1.12.4/jquery.min.js"></script>
  <link href="site_libs/leaflet-1.3.1/leaflet.css" rel="stylesheet">
  <script src="site_libs/leaflet-1.3.1/leaflet.js"></script>
  <link href="site_libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet">
  <script src="site_libs/proj4-2.6.2/proj4.min.js"></script>
  <script src="site_libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
  <link href="site_libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet">
  <script src="site_libs/leaflet-binding-2.1.2/leaflet.js"></script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Machine Learning / Aprendizagem de Máquina</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Prof.&nbsp;Dr.&nbsp;Pedro Rafael D. Marinho<br>Departamento de Estatística - UFPB<br> 
</div>
</div>
</div>

  <p class="date">2023-08-09</p>
</section>
<section class="slide level2">

<!-- ```{r} -->
<!-- #| echo: false -->
<!-- #| warning: false -->
<!-- #| eval: true -->
<!-- if(fs::dir_exists("index_files/")) -->
<!--   fs::dir_delete("index_files/") -->
<!-- ``` -->
<div class="r-fit-text">
<p>Aprendizagem de Máquina</p>
<p><span class="flow">Bacharelado em Estatística</span></p>
<p>UFPB</p>
</div>
</section>
<section>
<section id="section" class="title-slide slide level1 title center">
<h1></h1>
<div class="r-fit-text">
<p><span class="flow">Apresentação</span></p>
</div>
</section>
<section id="section-1" class="slide level2" data-background-image="https://raw.githubusercontent.com/prdm0/imagens/main/eu.jpg" data-background-size="contain" data-background-position="left">
<h2></h2>
<div class="columns">
<div class="column" style="width:40%;">

</div><div class="column" style="width:60%;">
<section id="sobre-mim" class="slide level2">
<h2>Sobre mim</h2>
<p><br> <br></p>
<ul>
<li class="fragment"><p>Me chamo <a href="https://prdm.netlify.app/about_pt_br.html" data-preview-link="true">Prof.&nbsp;Dr.&nbsp;Pedro Rafael D. Marinho</a>. Meu currículo Lattes poderá ser acessado clicando <a href="http://lattes.cnpq.br/7185368598935272" data-preview-link="true">aqui</a>.</p></li>
<li class="fragment"><p>Sou docente do Departamento de Estatística da UFPB. 👨‍🏫</p></li>
<li class="fragment"><p>Toda minha formação acadêmica é na área de estatística (bacharelado ao doutorado).</p></li>
<li class="fragment"><p>Tenho entusiasmo por programação, ciência de dados e aprendizagem de máquina 💻📈.</p></li>
<li class="fragment"><p><svg aria-hidden="true" role="img" viewbox="0 0 496 512" style="height:1em;width:0.97em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg> Me acompanhe no GitHub: <a href="https://github.com/prdm0" class="uri">https://github.com/prdm0</a>.</p></li>
<li class="fragment"><p><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg> Me acompanhe no Linkedin: <a href="https://www.linkedin.com/in/prdm0/" class="uri">https://www.linkedin.com/in/prdm0/</a>.</p></li>
</ul>
</section>
</div>
</div>
</section></section>
<section>
<section id="o-departamento" class="title-slide slide level1 title center">
<h1>O Departamento</h1>

</section>
<section id="meu-segundo-lar" class="slide level2" data-background-color="black" data-background-image="https://raw.githubusercontent.com/prdm0/imagens/main/foto_aerea_ufpb.jpeg" data-background-size="1600px" data-background-repeat="repeat" data-background-opacity="0.35">
<h2>Meu segundo lar</h2>
<div class="cell" width="100" height="100">
<div class="cell-output-display">
<div class="leaflet html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-742f20a0c9e6c1d425a6" style="width:960px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-742f20a0c9e6c1d425a6">{"x":{"options":{"crs":{"crsClass":"L.CRS.EPSG3857","code":null,"proj4def":null,"projectedBounds":null,"options":{}}},"calls":[{"method":"addMarkers","args":[-7.1404,-34.846199,null,null,null,{"interactive":true,"draggable":false,"keyboard":true,"title":"","alt":"","zIndexOffset":0,"opacity":1,"riseOnHover":false,"riseOffset":250},null,null,null,null,null,{"interactive":false,"permanent":false,"direction":"auto","opacity":1,"offset":[0,0],"textsize":"10px","textOnly":false,"className":"","sticky":true},null]},{"method":"addTiles","args":["https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png",null,null,{"minZoom":0,"maxZoom":18,"tileSize":256,"subdomains":"abc","errorTileUrl":"","tms":false,"noWrap":false,"zoomOffset":0,"zoomReverse":false,"opacity":1,"zIndex":1,"detectRetina":false,"attribution":"&copy; <a href=\"https://openstreetmap.org\">OpenStreetMap<\/a> contributors, <a href=\"https://creativecommons.org/licenses/by-sa/2.0/\">CC-BY-SA<\/a>"}]}],"limits":{"lat":[-7.1404,-7.1404],"lng":[-34.846199,-34.846199]},"setView":[[-7.1404,-34.846199],37,{"maxWidth":1500,"minWidth":1600,"autoPan":true,"keepInView":false,"closeButton":true,"className":""}]},"evals":[],"jsHooks":[]}</script>
<p>Departamento de Estatística da UFPB.</p>
</div>
</div>
</section>
<section id="que-linguagem-de-programação-utilizar" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M64 96c0-35.3 28.7-64 64-64H512c35.3 0 64 28.7 64 64V352H512V96H128V352H64V96zM0 403.2C0 392.6 8.6 384 19.2 384H620.8c10.6 0 19.2 8.6 19.2 19.2c0 42.4-34.4 76.8-76.8 76.8H76.8C34.4 480 0 445.6 0 403.2zM281 209l-31 31 31 31c9.4 9.4 9.4 24.6 0 33.9s-24.6 9.4-33.9 0l-48-48c-9.4-9.4-9.4-24.6 0-33.9l48-48c9.4-9.4 24.6-9.4 33.9 0s9.4 24.6 0 33.9zM393 175l48 48c9.4 9.4 9.4 24.6 0 33.9l-48 48c-9.4 9.4-24.6 9.4-33.9 0s-9.4-24.6 0-33.9l31-31-31-31c-9.4-9.4-9.4-24.6 0-33.9s24.6-9.4 33.9 0z"></path></svg> Que linguagem de programação utilizar?</h2>
<p><br></p>
<p>Nesse curso, será abordado a linguagem de programação <a href="https://www.r-project.org/">R</a>, mas lembre-se que você poderá utilizar qualquer linguagem de programação para fazer ciência de dados. Porém, R e Python são as minhas sugestões, haja vista que, atualmente, elas são as linguagens com maior quantidade de ferramentas e usuários trabalhando na área de <a href="https://en.wikipedia.org/wiki/Data_science">ciência de dados</a>.</p>
<p><br></p>
<p><span class="black">Outros motivos que me leva a lecionar a disciplina utilizando a linguagem R são:</span></p>
<ol type="1">
<li class="fragment">Possui ferramentas muito bem pensadas para manipulação e tratamento de dados;</li>
<li class="fragment">Normalmente, os <em>frameworks</em> de <em>machine learning</em> de R são menos verbosos que os de Python;</li>
<li class="fragment">Matrizes e data frames são estruturas de dados que já encontra-se definidas dentro da linguagem, não precisando assim de importar bibliotecas.</li>
</ol>
<p><span class="red">Isso é meu gosto pessoal</span>! É um gosto que, talvez, faz mais sentido, em se tratando de alguém que vem da estatística. No mercado de trabalho e em seus estudos, após cursar as disciplinas de R e Python, fornecidas pelo <a href="https://www.ufpb.br/de">Bacharelado em Estatística da UFPB</a>, você terá a capacidade de estudar os <em>frameworks</em> de <em>machine learning</em>, aos seus próprios passos e escolher o que melhor te agrada. A linguagem <a href="https://julialang.org/">Julia</a> também poderá ser uma ótima opção.</p>
</section></section>
<section>
<section id="section-2" class="title-slide slide level1 title center">
<h1></h1>
<div class="r-fit-text">
<p><span class="flow">Aprendizagem de Máquina: O que é?</span></p>
</div>
</section>
<section id="aprendizagem-de-máquina" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M320 0c17.7 0 32 14.3 32 32V96H472c39.8 0 72 32.2 72 72V440c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72H288V32c0-17.7 14.3-32 32-32zM208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H208zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H304zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H400zM264 256a40 40 0 1 0 -80 0 40 40 0 1 0 80 0zm152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80zM48 224H64V416H48c-26.5 0-48-21.5-48-48V272c0-26.5 21.5-48 48-48zm544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48H576V224h16z"></path></svg> Aprendizagem de máquina</h2>
<p><br> <br></p>
<p><img data-src="gifs/am.gif" class="fragment" width="930" height="600"></p>
</section>
<section id="aprendizagem-de-máquina-1" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M320 0c17.7 0 32 14.3 32 32V96H472c39.8 0 72 32.2 72 72V440c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72H288V32c0-17.7 14.3-32 32-32zM208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H208zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H304zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H400zM264 256a40 40 0 1 0 -80 0 40 40 0 1 0 80 0zm152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80zM48 224H64V416H48c-26.5 0-48-21.5-48-48V272c0-26.5 21.5-48 48-48zm544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48H576V224h16z"></path></svg> Aprendizagem de máquina</h2>
<p><br> <br></p>
<p><strong>Alguns pontos</strong>:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>A <strong>A</strong>prendizagem de <strong>M</strong>áquina (AM), também chamada de <strong>M</strong>achine <strong>L</strong>earning (ML), no inglês, nasceu na década de 60 como um campo da inteligênica artificial;</p></li>
<li class="fragment"><p>Em sua origem, as aplicações de AM tinha como objetivo aprender padrões com base nos dados;</p></li>
<li class="fragment"><p>Originalmente, as aplicações de AM eram de cunho estritamente computacional. Todavia, desde o início dos anos 90, a área de aprendizagem de máquina expandiu seus horizontes e começou a se estabelecer como um campo por sim mesma;</p></li>
<li class="fragment"><p>Em particular, a área de aprendizagem de máquina começou a estabelecer muitas intersecções com a estatística. Muitos de seus algoritmos são construídos com base em metodologias que surgiram na estatística;</p></li>
<li class="fragment"><p>Atualmente, a comunidade de AM é bastante interdisciplinar e utiliza-se de ideias desenvolvidas em diversas áreas, sendo a estatística uma delas.</p></li>
</ol>
</section>
<section id="tipos-de-aprendizado" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M320 0c17.7 0 32 14.3 32 32V96H472c39.8 0 72 32.2 72 72V440c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72H288V32c0-17.7 14.3-32 32-32zM208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H208zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H304zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H400zM264 256a40 40 0 1 0 -80 0 40 40 0 1 0 80 0zm152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80zM48 224H64V416H48c-26.5 0-48-21.5-48-48V272c0-26.5 21.5-48 48-48zm544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48H576V224h16z"></path></svg> Tipos de Aprendizado</h2>
<p><br></p>
<p><span class="black">Aprendizado supervisionado</span></p>
<p><br></p>
<p>Nesse curso, inicialmente estudaremos problemas de <span class="red">aprendizado supervisionado</span>, que consiste em aprender a fazer predições a partir de conjunto de dados em que rótulos (valores da variável resposta <span class="math inline">Y</span>) são observados. Trataremos tanto de problemas de regressão (estimar um valor númérico) quanto problemas de classificação (classificar um cliente como aprovado ou reprovado, em um problema de concessão de crédito). Por exemplo, os <span class="red">modelos de regressão</span> são exemplos de aprendizado supervisionado.</p>
<p><br></p>
<p><span class="black">Aprendizado não-supervisionado</span></p>
<p><br></p>
<p>Na segunda parte do curso, aprenderemos alguns métodos de aprendizado <span class="red">não-supervisionado</span>, ou seja, algoritmos que não utilizam-se de rótulos, em que busca-se aprender mais sobre a estrutura dos dados. Por exemplo, os <span class="red">métodos de agrupamento</span> (cluster), são exempĺos de métodos de aprendizado não-supervisionado.</p>
</section>
<section id="tipos-de-aprendizado-1" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 640 512" style="height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M320 0c17.7 0 32 14.3 32 32V96H472c39.8 0 72 32.2 72 72V440c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72H288V32c0-17.7 14.3-32 32-32zM208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H208zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H304zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H400zM264 256a40 40 0 1 0 -80 0 40 40 0 1 0 80 0zm152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80zM48 224H64V416H48c-26.5 0-48-21.5-48-48V272c0-26.5 21.5-48 48-48zm544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48H576V224h16z"></path></svg> Tipos de Aprendizado</h2>
<p><br></p>
<p>Muito embora no nosso curso focaremos nas abordagens de aprendizagem <strong>supervisionada</strong> e <strong>não-supervisionada</strong>, os tipos de aprendizagem, em geral, podem ser mais amplos, em que temos:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Aprendizagem supervisionada</strong>;</li>
<li class="fragment"><strong>Aprendizagem não-supervisionada</strong>;</li>
<li class="fragment">Aprendizagem semi-supervisionada;</li>
<li class="fragment">Aprendizagem por reforço.</li>
</ol>
</section>
<section id="o-que-é-aprender" class="slide level2">
<h2>O que é aprender?</h2>
<p><br></p>
<p>Antes de detalharmos os tipos de aprendizagem de máquina, uma dúvida que poderá surgir é: <span class="red">“O que é aprender?”</span>. <span class="red">“Como a máquina aprende?”</span>.</p>
<p><br></p>
<p><img data-src="gifs/am.gif" class="fragment" width="900" height="600"></p>
</section>
<section id="o-que-é-aprender-1" class="slide level2">
<h2>O que é aprender?</h2>
<p><br></p>
<p>De forma simples, aprender é ganhar conhecimento através de estudo, experiências, por meio de ensinamentos.</p>
<p><br></p>
<p><strong>Tá, mais como é que a <span class="red">máquina</span> aprende?</strong></p>
<p><br></p>
<ol type="1">
<li class="fragment"><p><span class="red">Aprendizagem</span> é o processo em que se adquire conhecimento, isto é, é o processo em que utilizamos de algoritmos e fornecemos dados a esses algoritmos para que possamos extrair conhecimento. Nesse processo de aprendizagem, os algoritmos fazem uso de dados para a extressão de conhecimento, através de procedimentos <strong>supervisionado</strong>, <strong>não-supervisionado</strong>, <strong>semi-supervisionado</strong> ou <strong>por reforço</strong>, a depender do algoritmo que você deseja utilizar.</p></li>
<li class="fragment"><p><span class="red">Aprendizado</span> é o modelo ajustado, isto é, é o conhecimento adquirido após o treinamamento obtido no processo de aprendizagem. Você poderá entender como sendo o modelo ajustado e que utilizamores para a tomada de decisões.</p></li>
</ol>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="o-que-é-aprender-2" class="slide level2">
<h2>O que é aprender?</h2>
<p><br></p>
<p>Portanto, você poderá entender, basiciamente, existe quatro tipos de aprendizagem, sendo os dois primeiros o que mais focaremos nesse curso e que de longe são os mais utilizados:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Aprendizagem supervisionada</strong>;</li>
<li class="fragment"><strong>Aprendizagem não-supervisionada</strong>;</li>
<li class="fragment">Aprendizagem semi-supervisionada;</li>
<li class="fragment">Aprendizagem por reforço.</li>
</ol>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="aprendizagem-supervisionada" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br></p>
<p>Nesse tipo de aprendizagem, o algoritmo irá receber um conjunto de dados em que conhecemos rótulos para a variável de interesse. É como se você soubesse onde um bom modelo deve chegar, para assim ser reconhecido como um bom modelo. Por exemplo,</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p><span class="red">Classificação</span>: precisamos determinar a classe de uma instância de dados, o seu atributo, i.e., <span class="math inline">\widehat{y} = \mathrm{argmax}_y\,P(Y = y\,|\, X = \bf{x})</span>, em que y é um atributo que desejamos prever (cahorro, gato, sapo), e <span class="math inline">\bf{x}</span> é um vetor de características (peso, altura, comprimento, se tem rabo, etc).</p></li>
<li class="fragment"><p><span class="red">Regressão</span>: precisamos estimar uma quantidade numérica, i.e., o valor da variável alvo por meio de uma <strong>instância de dados</strong>, ou seja, precisamos estimar <span class="math inline">Y = \mathbb{E}(Y|X = \bf{x})</span>, i.e., devemos encontrar meios de obter <span class="math inline">\widehat{Y}</span>.</p></li>
</ol>
<p><br></p>

<aside><div>
<p><strong>Algumas observações de nomenclaturas</strong>:</p>
<ol type="1">
<li class="fragment">É comum chamar cada exemplo de dados, i.e., o vetor <span class="math inline">\bf{x}</span> que será passado ao modelo de <span class="red">atributos</span> ou <span class="red"><em>features</em></span>;</li>
<li class="fragment">Também é comum chamarmos de <span class="red">rótulo</span> ou <span class="red"><em>label</em></span> a classe ou valor alvo, ou seja, estas são as formas de nomearmos <span class="math inline">Y</span>, sendo <span class="math inline">Y</span> uma quantidade numérica (modelos de regressão) ou não (modelos de classificação).</li>
</ol>
</div></aside></section>
<section id="aprendizagem-supervisionada-1" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br></p>
<p>Em se tratando de métodos de classificação, podemos ter os métodos:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p><span class="red">Generativos</span>: são os métodos que dada as variáveis <span class="math inline">X</span> e <span class="math inline">Y</span>, o objetivo é encontrar a distribuição de probabilidade conjunta <span class="math inline">P(X, Y)</span>, para então poder determinar <span class="math inline">P(Y|X = \bf{x})</span>. Alguns métodos são:</p>
<ul>
<li class="fragment">Naive Bayes;</li>
<li class="fragment">Descriminante linear.</li>
</ul></li>
</ol>
<p><br></p>
<ol start="2" type="1">
<li class="fragment"><span class="red">Descriminativos</span>: são os métodos que estimam diretamente a probabilidade condicional <span class="math inline">P(Y|X = \bf{x})</span> ou que mesmo nem assumem modelos probabilísticos. Os modelos dessa classe são projetados para aprender a fronteira de decisão que separa as classes diretamente com base nas características de entrada. Podemos citar:
<ul>
<li class="fragment">Regressão logistica;</li>
<li class="fragment">Perceptron;</li>
<li class="fragment"><strong>S</strong>upport <strong>V</strong>ector <strong>M</strong>achine - SVM.</li>
</ul></li>
</ol>
</section>
<section id="aprendizagem-supervisionada-2" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:60%;">
<p><img data-src="gifs/classificacao.webp"></p>
</div><div class="column" style="width:40%;">
<p><br> Poderíamos estar interessados em classificar o tamanho de morangos:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>S (<strong>S</strong>low): pequeno;</p></li>
<li class="fragment"><p>M (<strong>M</strong>edium): médio;</p></li>
<li class="fragment"><p>L (<strong>L</strong>arge): grande.</p></li>
</ol>
</div>
</div>
</section>
<section id="aprendizagem-supervisionada-3" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br> <br></p>

<img data-src="gifs/Classification-Examples.gif" style="width:45.0%" class="r-stretch quarto-figure-center"><p class="caption">Mais dois problemas de classificação (linear x não-linear).</p></section>
<section id="aprendizagem-supervisionada-4" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br> <br></p>

<img data-src="gifs/regression.gif" width="1200" class="r-stretch quarto-figure-center"><p>Um exemplo de de um problema de regressão. Aqui, a ideia é utilizar a equação da reta estimada, a reta que minimiza a soma dos quadrados entre a reta e os ponto seria a melhor, de modo a ter uma estimativa numérica através de novos atributos passado ao modelo, i.e., por meio da equação da reta e de um novo valor de <span class="math inline">x</span>.</p>
</section>
<section id="aprendizagem-supervisionada-5" class="slide level2">
<h2>Aprendizagem supervisionada</h2>
<p><br></p>
<p>Um outro exemplo seria a classificação de imagem/vídeo, utilizando um algoritmo de rede neural, por exemplo, usando uma <strong>C</strong>onvolutional <strong>N</strong>eural <strong>N</strong>etwork - <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a>. Foram utilizados diversas imagens de pessoas “com” e “sem” máscara. Em que “com” representa detecção da máscara na face da pessoa e “sem” a não detecção.</p>
<p><br></p>
<iframe data-external="1" src="https://www.youtube.com/embed/Zt_Fr7YbU1c" width="50%" height="50%" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="aprendizagem-não-supervisionada" class="slide level2">
<h2>Aprendizagem não-supervisionada</h2>
<p><br></p>
<p>Nesse tipo de aprendizagem, os algoritmos trabalham sobre dados não rotulados, por exemplo, em uma trarefa de agrupamento.</p>
<p><br></p>
<p>Os algoritmos verificam se as instâncias observadas poderão ser arranjadas de alguma maneira, por exemplo, usando alguma métrica de distância, formando grupos (<em>clusters</em>).</p>
<p><br></p>
<p>A ideia é maximizar a distância entre os clusters e minimizar a distância entre os elementos no interior do grupo. Em outras palavras, o que se quer é tornar os grupos mais diferentes possíveis e tornar os elementos dos grupos o mais parecido possível.</p>
<p><br></p>
<p>Aqui, por não haver rótulos, um problema comum é determinar a quantidade de grupos ideal que muitas vezes são obtidos de forma subjetiva ou por heurísticas. A quantidade de grupos é um dilema!</p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="aprendizagem-não-supervisionada-1" class="slide level2">
<h2>Aprendizagem não-supervisionada</h2>
<p><br></p>

<img data-src="gifs/kmeans.gif" width="900" class="r-stretch quarto-figure-center"><p>Após a detecção dos grupos, é preciso analisar o resultado de modo a tentar extrair informações coerentes de modo a saber o que cada grupo representa no problema em questão.</p>
</section>
<section id="aprendizagem-semi-supervisionada" class="slide level2">
<h2>Aprendizagem semi-supervisionada</h2>
<p><br></p>
<p>A aprendizagem semi-supervisionada é uma abordagem na área de aprendizagem de máquina, em que um algoritmo utiliza tanto dados rotulados quanto não rotulados para treinamento. Por exemplo, algoritmos que propagam rótulos, como o <em>Label Propagation</em>, em que rótulos conhecidos são propagados para dados não rotulados com base em sua sua proximidade no espaço de características.</p>
<p><br></p>
<p>Uma outra abordagem seria misturar modelos (<em>Model Blending</em>), em que diferentes modelos são treinados em diferentes partes do conjunto de dados, por exemplo, um modelo para a parte roturada e um para a parte não rotulada.</p>
<p><br></p>

<img data-src="gifs/hum.gif" style="width:50.0%" class="r-stretch"></section>
<section id="aprendizagem-por-reforço" class="slide level2">
<h2>Aprendizagem por reforço</h2>
<p><br></p>
<p>Nesse tipo de aprendizagem, não <strong>há uma fonte externa de exemplos</strong>. O agente (modelo) aprende aprende com sua própria experiência, por tentativas e erros, em que você deverá definir uma medida de sucesso, e eventualmente recompensar os acertos. No vídeo abaixo, veja um joguinho que criei em R, em que o carrinho aprendeu a desviar de obstáculos aleatórios que aparecem em sua frente. Utilizou-se uma rede neural cuja a saída poderia ser (“parado”, “para cima” ou “para baixo”). Veja o código clicando <a href="https://github.com/prdm0/desviando_obstaculos"><strong>aqui</strong></a>.</p>
<p><br></p>
<iframe data-external="1" src="https://www.youtube.com/embed/9NXUtwGkkDw" width="50%" height="50%" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="dados-exploração-e-tratamento" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>
<p>Um dos passos mais importante no fluxo de trabalho (<em>workflow</em>) de um modelo de aprendizagem de máquina, consiste na preparação dos dados, em que realizamos transformações, inputações de valores ausentes, identificação de <em>outliers</em>, remoção de variáveis altamente correlacionadas, entre outros.</p>
<p><br></p>
<p>Fazer uma <strong>análise exploratória</strong> dos dados é um passo importante para que se possa entender e detecatar possíveis inconsistências na base de dados. Não adianta fazer uso de modelos muito sofisticados quando se tem uma base de dados cheia de problemas.</p>
<p><br></p>
<p>Normalmente trabalhamos com juntos de dados (tabelas) relacionais, em que cada linha é uma observação e cada coluna representa um atributo do objeto/observação. A linha de uma base de dados relacional, sem sua a variável de interesse, lembre-se que denominamos <span class="math inline">Y</span> de <span class="red">rótulo</span> ou <span class="red"><em>label</em></span>, fornece o vetor de características <span class="math inline">\bf{x}</span> que descreve uma dada observação.</p>
</section>
<section id="dados-exploração-e-tratamento-1" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p>No artigo <a href="https://www.jstatsoft.org/article/view/v059i10">Tidy Data</a>, 2014, publicado no Journal of Statistical Sofware, o Hadley Wickham discute que o princípio de dados organizados estão intimamente relacionados com banco de dados relacional e mais próximo do reciocínio que empregamos na álgebra. Nesse artigo, ele define o que é <span class="red">Tidy Data</span>, sendo essa uma maneira de mapear um conjunto de dados.</p>
<p><br></p>
<p>Segundo o artigo, um conjunto de dados é <span class="red">bagunçado</span> ou <span class="red">arrumado</span>/<span class="red">tidy</span>, dependendo de como as linhas, colunas e tabelas são combinadas com as observações, variáveis e tipos. Em dados arrumados (dados <em>tidy</em>), temos que:</p>
<p><br></p>
<ol type="1">
<li class="fragment">Cada variável forma uma coluna;</li>
<li class="fragment">Cada observação forma uma linha;</li>
<li class="fragment">Cada valor deve ter sua própria célula.</li>
</ol>
<p><br></p>
<p>Embora existam situações em que já podemos começar a analisar uma base de dados real, essa é a exceção e não a regra. Normalmente, nos deparamos com bases de dados que violam uma ou mais dessas regras. Sempre, que possível, procure utilizar dados no formato <span class="red">Tidy</span>.</p>
</section>
<section id="dados-exploração-e-tratamento-2" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>

<img data-src="imgs/tidy-1.png" class="r-stretch quarto-figure-center"><p class="caption">Representação de uma base de dados no formato <em>tidy</em>.</p></section>
<section id="dados-exploração-e-tratamento-3" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>
<blockquote>
<p>“As famílias felizes são todas iguais; toda família infeliz é infeliz à sua maneira.” – <a href="https://en.wikipedia.org/wiki/Leo_Tolstoy">Leo Tolstoy</a></p>
</blockquote>
<blockquote>
<p>“Conjuntos de dados organizados são todos iguais, mas todo conjunto de dados confuso é confuso à sua maneira.” – <a href="https://hadley.nz/">Hadley Wickham</a></p>
</blockquote>
<p><br></p>

<img data-src="imgs/tidy-2.png" style="width:50.0%" class="r-stretch quarto-figure-center"><p class="caption">Trabalhar com a Tabela do lado esquerdo é melhor que a Tabela do lado direito. Prefira, sempre que possível, o formato tidy. Não permita-se ficar estressado tão facilmente. 😃</p></section>
<section id="dados-exploração-e-tratamento-4" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p>A linguagem de programação R possue diversas ferramentas que permite manipular e explorar bases de dados. Enumero algumas:</p>
<ol type="1">
<li class="fragment"><a href="https://dplyr.tidyverse.org/">dplyr</a>: biblioteca que implementa é uma gramática de manipulação de dados, fornecendo um conjunto consistente de verbos que ajudam a resolver os desafios mais comuns de manipulação de dados;</li>
<li class="fragment"><a href="https://tidyr.tidyverse.org/">tidyr</a>: ferramentas para ajudar a criar dados organizados, em que cada coluna é uma variável, cada linha é uma observação e cada célula contém um único valor;</li>
<li class="fragment"><a href="https://ggplot2-book.org/">ggplot2</a>: um sistema para criar gráficos ‘declarativamente’, baseado no livro <a href="https://www.amazon.com.br/Grammar-Graphics-Leland-Wilkinson/dp/0387245448">The Grammar of Graphics</a>, de <a href="https://en.wikipedia.org/wiki/Leland_Wilkinson">Leland Wilkinson</a>;</li>
<li class="fragment"><a href="https://docs.ropensci.org/visdat/">visdat</a>: uma biblioteca útil para um visualização exploratória preliminar de dados;</li>
<li class="fragment"><a href="https://github.com/rolkra/explore">explore</a>: biblioteca que apresenta algumas rotinas de análise para realizar uma análise exploratória nos dados.</li>
</ol>
<p>Todas essas bibliotecas estão muito bem documentadas. É importante que vocês explorem as documentas dessas bibliotecas, pois eventualmente irei utizar alguma delas.</p>

<img data-src="gifs/ok.gif" class="r-stretch"></section>
<section id="dados-exploração-e-tratamento-5" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>
<p>No <a href="https://r4ds.had.co.nz/tidy-data.html">Capítulo 12</a>, do livro <a href="https://r4ds.had.co.nz/index.html">R for Data Science</a>, o autor aborda mais sobre o formato <em>Tidy</em> e como trabalhar com a biblioteca <a href="https://tidyr.tidyverse.org/">tidyr</a>. <a href="https://r4ds.had.co.nz/transform.html?q=dplyr#dplyr-basics">Aqui</a> o autor aborda de forma básica o pacote <a href="https://dplyr.tidyverse.org/">dplyr</a>.</p>
<p><br></p>
<p>Durante o curso, na medida da necessidade de utilização dessas ferramentas, durante a exposição de exemplos, abordaremos alguns conceitos. Você terá a oportunidade de também explorar essas bibliotecas nos exercícios. Ok?!</p>
<p><br></p>

<img data-src="gifs/ok.gif" class="r-stretch"></section>
<section id="dados-exploração-e-tratamento-6" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>
<p>Muitas vezes, no processo de tratamento dos dados, também estamos preocupados em <strong>remover atributos que não são significativo</strong> para a modelagem, em que nesse momento a experiência dos especialistas são fundamentais.</p>
<p><br></p>
<p><strong>É comum enriquercermos a base de dados com informações de outras bases de dados</strong>, em um sistema de gerenciamento de banco de dados relacional, em que as bases de dados estão relacionadas por uma chave. Nesse caso, buscamos por novos atributos para um mesmo objeto (para uma mesma linha da base), em que atributos cruzados devem ter um único valor, para cada objeto, respeitando a regra três de conjuntos de dados <em>tidy</em>.</p>
<p><br></p>
<p><strong>As vezes transformamos variáveis</strong>. Por exemplo, é comum tomar o logaritmo de uma variável numérica que é assimétrica, se <span class="math inline">x \geq 1</span>, em que <span class="math inline">x</span> é um atributo numérico qualquer.</p>
<p><br></p>
<p>Em diveras situações, também é comum a base de dados apresentar <strong>informações faltantes</strong>. Nos data frames de R, a falta de informação na base, normlamente serão representadas por <code>NA</code>.</p>
</section>
<section id="dados-exploração-e-tratamento-7" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>
<p>Poderá ser que um dado atributo apresente informação faltante, e normalmente não optaremos em remover a observação e precisaremos imputar a informação, por exemplo:</p>
<p><br></p>
<ol type="1">
<li class="fragment">Tomando alguma medida de tendência central como média/moda/mediana dos valores que são conhecidos para aquele atributo;</li>
<li class="fragment">Criar um novo valor que é indicação de valor faltante;</li>
<li class="fragment">Usar algoritmos como <span class="math inline">k</span>-nearest neighbors - <span class="math inline">k</span><strong>NN</strong> (<span class="math inline">k</span> vizinhos mais próximos) para imputar valores coerentes;</li>
<li class="fragment">Interpolar os dados.</li>
</ol>
<p>Esses são alguns exemplos de como podemos imputar observações faltantes. Muitas vezes não podemos nos dar o luxo de percer observações de nossa base de dados.</p>

<img data-src="gifs/chapulin-colorado-no.gif" class="r-stretch"></section>
<section id="dados-exploração-e-tratamento-8" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>
<p>É comum ser necessário transformar os dados:</p>
<ol type="1">
<li class="fragment"><p>Pode ser necessário transformar os tipos ou os valores dos atributos para tentar obter um melhor ajuste do modelo;</p></li>
<li class="fragment"><p>Pode-se discretizar valores contínuos ou transformá-los em intervalos;</p></li>
<li class="fragment"><p>É comum transformar atributos categóricos com <span class="math inline">p</span> categorias, em <span class="math inline">p</span> novos atributos binários.</p>
<ul>
<li class="fragment"><a href="https://en.wikipedia.org/wiki/One-hot">One-hot encoding</a></li>
<li class="fragment"><a href="https://en.wikipedia.org/wiki/Dummy_variable_(statistics)">Variáveis dummy</a></li>
</ul></li>
<li class="fragment"><p>Outra transformação muito comum é a normalização dos dados. Normalizar os dados é muito útil quando os atributos numéricos possuem escalas muito diferentes.</p></li>
</ol>
<div class="columns">
<div class="column" style="width:50%;">
<p><span class="math display">X_{novo} = \frac{X - X_{min}}{X_{max} - X_{min}},</span> em que <span class="math inline">X_{novo} \in [0, 1].</span></p>
</div><div class="column" style="width:50%;">
<p><span class="math display">X_{novo} = Z = \frac{X - \mu}{\sigma^2},</span> em que <span class="math inline">\mathbb{E}(X) = \mu</span> é a média dos dados e <span class="math inline">\mathrm{Var}(X) = \sigma^2</span>. Na prática, em um contexto de v.a., iids, usamos <span class="math inline">\overline{x}</span> como estimador de <span class="math inline">\mu</span> e <span class="math inline">S^2</span> (variância amostral) como estimador de <span class="math inline">\sigma^2</span>.</p>
</div>
</div>
</section>
<section id="dados-exploração-e-tratamento-9" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>
<p>Lembre-se, como citado anteriormente, tomar o logaritmo natural, ou mesmo na base 10 de variáveis numéricas muito assimétricas, poderá ajudar um pouco, desde que seja possivel tomar o <span class="math inline">\log(\cdot)</span>.</p>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">rgamma</span>(<span class="dv">1000</span>, <span class="dv">2</span>, <span class="dv">2</span>) <span class="sc">|&gt;</span> </span>
<span id="cb1-3"><a href="#cb1-3"></a>  <span class="fu">hist</span>()</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img data-src="index_files/figure-revealjs/unnamed-chunk-2-1.png" width="960"></p>
</div>
</div>
</div><div class="column" style="width:50%;">
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="fu">rgamma</span>(<span class="dv">1000</span>, <span class="dv">2</span>, <span class="dv">2</span>) <span class="sc">|&gt;</span> </span>
<span id="cb2-3"><a href="#cb2-3"></a>  <span class="fu">log</span>() <span class="sc">|&gt;</span> <span class="fu">hist</span>()</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img data-src="index_files/figure-revealjs/unnamed-chunk-3-1.png" width="960"></p>
</div>
</div>
</div>
</div>
</section>
<section id="dados-exploração-e-tratamento-10" class="slide level2">
<h2><svg aria-hidden="true" role="img" viewbox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:black;overflow:visible;position:relative;"><path d="M448 80v48c0 44.2-100.3 80-224 80S0 172.2 0 128V80C0 35.8 100.3 0 224 0S448 35.8 448 80zM393.2 214.7c20.8-7.4 39.9-16.9 54.8-28.6V288c0 44.2-100.3 80-224 80S0 332.2 0 288V186.1c14.9 11.8 34 21.2 54.8 28.6C99.7 230.7 159.5 240 224 240s124.3-9.3 169.2-25.3zM0 346.1c14.9 11.8 34 21.2 54.8 28.6C99.7 390.7 159.5 400 224 400s124.3-9.3 169.2-25.3c20.8-7.4 39.9-16.9 54.8-28.6V432c0 44.2-100.3 80-224 80S0 476.2 0 432V346.1z"></path></svg> Dados: exploração e tratamento</h2>
<p><br></p>
<p>Anteriormente eu citei algumas bibliotecas úteis de R para explorar os dados, na fase de tratamento das observações. Porém, não estranhe não ter, até o momento, citado bibliotecas do <em>framework</em> <a href="https://www.tidymodels.org/packages/">tidymodels</a>, em especial o <a href="https://recipes.tidymodels.org/">recipes</a> que é muito utilizado no <em>workflow</em> de aprendizagem de máquina, na fase de pré-processamento dos dados. Muitas dessas transformações são aplicadas como receitas de pré-processamento com o pacote <a href="https://recipes.tidymodels.org/">recipes</a>.</p>
<p><br></p>
<p><img data-src="imgs/recipes.png"></p>
<p><br></p>
<p>O <a href="https://www.tidymodels.org/packages/">tidymodels</a> será muito útil para nós, mas, aos poucos, seu uso e explicações mais detalhadas serão apresentadas, apesar que em algumas situações mais simples, poderei não utilizá-lo, para expor detalhes que eventualmente não será possível ou estariam camuflados (<em>black box</em>) na utilização do <a href="https://www.tidymodels.org/packages/">tidymodels</a>.</p>
<p><br></p>
<p>Para não deixar de valar sobre o <a href="https://www.tidymodels.org/packages/">tidymodels</a>, explicarei, agora, a sua filosofia e como ele está dividido em outras bibliotecas que são úteis em cada parte do processo de treinamento de um modelo de <em>machine learning</em>.</p>
<p><img data-src="gifs/ok-2.gif" style="width:20.0%"></p>
</section>
<section id="tidymodels" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>
<p>Diversas bibliotecas na linguagem R são preparadas para trabalharem na área de aprendizagem de máquina. Várias dessas bibliotecas vem sendo desenvolvidas há anos. Por exemplo,as bibliotecas <a href="https://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a>, <a href="https://cran.r-project.org/web/packages/ranger/index.html">ranger</a>, <a href="https://cran.r-project.org/web/packages/kknn/index.html">kknn</a>, <a href="https://cran.r-project.org/web/packages/xgboost/index.html">xgboost</a>, <a href="https://cran.r-project.org/web/packages/keras/index.html">keras</a>, <a href="https://cran.r-project.org/web/packages/rpart/index.html">rpart</a>, <a href="https://cran.r-project.org/web/packages/randomForest/index.html">randomForest</a>, entre diversos outros.</p>
<p><br></p>
<div class="cell">
<details>
<summary>O número de pacotes abaixo é o mais recente. Obtido automaticamente por webscraping.</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="fu">library</span>(xml2)</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="fu">library</span>(httr)</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="fu">library</span>(stringr)</span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a>numero_pacotes_r <span class="ot">&lt;-</span> httr<span class="sc">::</span><span class="fu">GET</span>(<span class="st">"https://cloud.r-project.org/web/packages/index.html"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb3-6"><a href="#cb3-6"></a>  xml2<span class="sc">::</span><span class="fu">read_html</span>() <span class="sc">|&gt;</span> </span>
<span id="cb3-7"><a href="#cb3-7"></a>  xml2<span class="sc">::</span><span class="fu">xml_find_all</span>(<span class="st">"//p[1]"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb3-8"><a href="#cb3-8"></a>  xml2<span class="sc">::</span><span class="fu">xml_text</span>() <span class="sc">|&gt;</span> </span>
<span id="cb3-9"><a href="#cb3-9"></a>  stringr<span class="sc">::</span><span class="fu">str_extract</span>(<span class="at">pattern =</span> <span class="st">"[0-9]+"</span>)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><br></p>
<p>Atualmente, a linguagem R possui <span class="red">19892</span>, em que muitos deles são preparados para para trabalharem em tarefas de aprendizagem de máquina, porém, cada com sua sintaxe específica. Muitos implementam o mesmo modelo, uns com algumas variações, porém, o uso é totalmente diferente, nomes de parâmetros distintos, saídas distintas, etc.</p>

<img data-src="gifs/chaves-isso.gif" class="r-stretch"></section>
<section id="tidymodels-1" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>
<p>Essas diferentes implementações torna confuso trabalhar e testar diferentes modelos ao mesmo tempo.</p>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Uma das primeiras ideias mais conhecidas de unificação de sintaxe do <em>workflow</em> de machine learning, na linguagem R, foi idealizada pelo estatístico <a href="https://www.linkedin.com/in/max-kuhn-864a9110/">Max Khun</a>.</p>
<p><br></p>
<p>Ele criou a biblioteca <a href="https://cran.r-project.org/web/packages/caret/index.html">caret</a> - <strong>C</strong>lassification <strong>A</strong>nd <strong>RE</strong>gression <strong>T</strong>raining de R que é muito bem desenvolvida e abrangente. Você poderá estudar o <a href="https://cran.r-project.org/web/packages/caret/index.html">caret</a> clicando <a href="https://topepo.github.io/caret/">aqui</a>.</p>
<p><br></p>
<p>Não foi um trabalho simples, veja uma tabela com a quantidade de modelos que o <a href="https://cran.r-project.org/web/packages/caret/index.html">caret</a> suporta, clicando <a href="https://topepo.github.io/caret/available-models.html">aqui</a>. Então, “por baixo dos panos”, a ideia era unificar a entrada e saída. A biblioteca <strong>caret</strong> continua sendo mantida, apesar da existência do <a href="https://www.tidymodels.org/">tidymodels</a>.</p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/max_kuhn.jpeg"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="tidymodels-2" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/max_kuhn.jpeg"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<p>Max Kuhn, atualmente, no momento de escrita desse mateiral, é funcionário da <a href="https://posit.co/">Posit Ltda</a> e foi contratado para estar a frente do desenvolvimento de uma versão “arrumada” (<em>tidy</em>) do <a href="https://cran.r-project.org/web/packages/caret/index.html">caret</a>, que é o <a href="https://www.tidymodels.org/">tidymodels</a>.</p>
<p><br></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/tidymodels.png" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="tidymodels-3" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>

<img data-src="imgs/workflow_tidymodels.png" style="width:70.0%" class="r-stretch quarto-figure-center"><p class="caption">O <em>workflow</em> (<em>pipeline</em>) do treinamento de um modelo usando o <em>framework</em> <a href="https://www.tidymodels.org/">tidymodels</a>. Todos os pacotes (rsample, recipes, parsnip, tune, dails, yardstick) são gerenciados pelo pacote <a href="https://www.tidymodels.org/">tidymodels</a>. Cada um desses pacotes fornece um conjunto de funções úteis em tarefas específicas no workflow de machine learning.</p></section>
<section id="tidymodels-4" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>
<ol type="1">
<li class="fragment"><a href="https://rsample.tidymodels.org/">rsample</a>: responsável pela reamostragem dos dados, parte importante para que possamos treinar um modelo de aprendizagem de máquina. É nele que encontra-se funções para realizar reamostragem como bootstrap, <span class="math inline">k</span>-folds cross-validation, nested cross-validation, entre outras.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/rsample.png" style="width:10.0%"></p>
</figure>
</div>
<ol start="2" type="1">
<li class="fragment"><a href="https://recipes.tidymodels.org/">recipes</a>: apresenta diversas funções para transformações de variáveis como criação de variáveis dummy, normalização de variáveis, inputação de dados pela média, mediana, <span class="math inline">k</span>NN, entre outras formas de imputação, transformações de variáveis categórias em numéricas, entre outras funcionalidades. Ele permite que possamos criar uma receita de transformações nos dados para que esses, após transformados, possam entrar no modelo.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/recipes.png" style="width:10.0%"></p>
</figure>
</div>
<ol start="3" type="1">
<li class="fragment"><a href="https://parsnip.tidymodels.org/">parsnip</a>: é o pacote que unifica as entradas e saídas de diversos pacotes de aprendizagem de máquina de R. Ele possui os motores (engines) que são as comunicações com os algoritmos implementados em diversos pacotes de R que trabalham com tarefas de regressão e classificação, em aprendizagem de máquina.</li>
</ol>
</section>
<section id="tidymodels-5" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Os pacotes <a href="https://tune.tidymodels.org/">tune</a>, <a href="https://dials.tidymodels.org/">dails</a> e <a href="https://yardstick.tidymodels.org/">yardstick</a> tomará conta da parte de treino do modelo. Os pacote <a href="https://tune.tidymodels.org/">tune</a> e <a href="https://dials.tidymodels.org/">dails</a> são responsáveis pela “tunagem” dos eventuais hiperparâmetros, já o <a href="https://yardstick.tidymodels.org/">yardstick</a> é responsável pelas métricas de avaliação do modelo.</p>
<p><br></p>
<p>A biblioteca <a href="https://dials.tidymodels.org/">dails</a> está mais relacionada a criação dos <em>grids</em> para os eventuais hiperparâmetros do modelo. Já o pacote <a href="https://tune.tidymodels.org/">tune</a>, utiliza-se da validação cruzada criada pelo pacote <a href="https://rsample.tidymodels.org/">rsample</a> para varrer as combinações de hiperparâmetros criadas pelo <a href="https://dials.tidymodels.org/">dails</a>, i.e., o <a href="https://tune.tidymodels.org/">tune</a> está mais relacionado com a otimização dos hiperparâmetros.</p>
</div><div class="column" style="width:25%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/tune.png"></p>
</figure>
</div>
</div><div class="column" style="width:25%;">
<p><img data-src="imgs/dails.png" data-fig-aling="right"> <img data-src="imgs/yardstick.png" data-fig-aling="right"></p>
</div>
</div>
</section>
<section id="tidymodels-6" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>
<p>Todo o fluxo de trabalho é gerido pela biblioteca <a href="https://workflows.tidymodels.org/">workflows</a> de R. Em especial, as etapas de <em>feature engineering</em> e especificação do modelo.</p>
<p><br></p>

<img data-src="imgs/workflows_lib.png" class="r-stretch quarto-figure-center"></section>
<section id="tidymodels-7" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>

<img data-src="imgs/MachineLearning_tidymodels.png" class="r-stretch quarto-figure-center"><p class="caption">Perceba o papel da biblioteca <a href="https://workflows.tidymodels.org/">workflows</a> de R. Basicamente gostaríamos de ter uma automação da faze do tratamento das <em>features</em> realizada com o <a href="https://recipes.tidymodels.org/">recipes</a> com a modelagem.</p></section>
<section id="tidymodels-8" class="slide level2">
<h2>Tidymodels</h2>
<p><br></p>
<p>Não necessariamente iremos utilizar o <a href="https://www.tidymodels.org/">tidymodels</a> em todos os exemplos e exercícios. Porém, iremos explorar bastante, até o fim do curso, o treinamento de modelos usando o <a href="https://www.tidymodels.org/">tidymodels</a>. Por tanto, aos poucos, a medida em que exemplos são apresentados e exercícios forem passados, o aprendizado do uso do <a href="https://www.tidymodels.org/">tidymodels</a> se dará.</p>
<p><br></p>
<p><img data-src="gifs/thumbs-up-nod.gif" style="width:25.0%"> <img data-src="gifs/teclado-anime.gif" style="width:25.0%"></p>
<p><br></p>
<p>Sempre que possível, deveremos colocar as “mãos na massa” 🍝 para que possamos dominar e compreender uma ferramenta computacional. A prática é importante!</p>
</section>
<section id="as-duas-culturas" class="slide level2">
<h2>As duas culturas</h2>
<p><br></p>
<p>Em <a href="https://projecteuclid.org/journalArticle/Download?urlId=10.1214%2Fss%2F1009213726">Breiman, L. (2001a). <strong>Statistical modeling: The two cultures</strong>. Statistical Science, 16(3), 199–231</a>, o Leo Breiman argumenta que existe duas culturas no uso de modelos estatísticos, em especialmente na área de modelos de regressão. Segundo eles, as culturas são:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><span class="red">Data modeling culture</span>: nela, em geral, se assume que o modelo de regressão utilizado <span class="math inline">r(x)</span>, por exemplo, <span class="math inline">r(x) = \beta_0 + \sum_{i = 1}^d \beta_ix_i</span> é correto. O principal objetivo dessa abordagem é a interpretação dos parâmetros que indexam o modelo <span class="math inline">r(x)</span>. Nesse tipo de cultura, a ideia também é construir intervalos aleatórios e testar hipóteses para os <span class="math inline">\beta_i's</span>. Sob essa ótica, muitas suposições sob o modelo são realizadas, em que formas para checar essas suposições são desenvolvidas, uma vez que elas são fundamentais para esse tipo de modelagem.</li>
</ol>
<p><br></p>
<ol start="2" type="1">
<li class="fragment"><span class="red">Algorithmic modeling culture</span>: essa é a cultura que domina a comunidade de aprendizagem de máquina. Nessa abordagem, o principal objetivo são as predições por meio de novas observações. Não se assume que o modelo utilizado é o modelo correto. Nesse tipo de modelagem, muitas vezes os algoritmos não envolve nenhuma estrutura probabilística. Muitas vezes, modelos não bem especificado conduzem a boas predições.</li>
</ol>
</section>
<section id="as-duas-culturas-1" class="slide level2">
<h2>As duas culturas</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/paper_breiman.png" style="width:70.0%"></p>
<figcaption>Breiman, L. (2001a). Statistical modeling: The two cultures. Statistical Science, 16(3), 199–231.</figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/leo_breiman.png" style="width:60.0%"></p>
<figcaption>Leo, na época em que era um jovem probabilista na Universidade da Califórina.</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="as-duas-culturas-2" class="slide level2">
<h2>As duas culturas</h2>
<p><br></p>
<p>Há diversos artigos interessantes que são respostas ao artigo do Leo Breiman, como por exemplo, o artigo <a href="https://www.jstor.org/stable/2676682">Statistical Modeling: The Two Cultures: Comment</a> do David Cox e com comentários do Brad Efron.</p>

<img data-src="imgs/david_cox.png" style="width:20.0%" class="r-stretch quarto-figure-center"><p class="caption"><a href="https://en.wikipedia.org/wiki/David_Cox_(statistician)">Sir David Cox.</a></p></section>
<section id="as-duas-culturas-3" class="slide level2">
<h2>As duas culturas</h2>
<p><br></p>
<p>Muito embora exista essa divisão entre as culturas, Breiman foi um estatístico que desempenhou um grande trabalho para unir a área de estatística com aprendizado de máquina. Por conta dessa grande importância, um prêmio concedido em sua homenagem foi criado pela <a href="https://community.amstat.org/slds/awards/breiman-award">American Statistical Association</a>.</p>

<img data-src="imgs/breiman_residencia.png" style="width:30.0%" class="r-stretch quarto-figure-center"><p class="caption"><a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2Fss%2F1009213290">Leo Breiman trabalhando em sua residência, em 1985.</a></p></section>
<section id="as-duas-culturas-4" class="slide level2">
<h2>As duas culturas</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:70%;">
<p>Leo Breiman, renomado estatístico, contribuiu significativamente para o campo de aprendizagem de máquina. Ele é conhecido por ter criado métodos populares e influentes para a área. Entre tais métodos famosos, cito dois:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p><strong>Random forest</strong> (<strong>florestas aleatórias</strong>): método que combina a previsão de vários modelos de árvores de decisão (<em>decision tree</em>), que veremos mais a frente, por isso o termo “floresta” para problemas de regressão e classificação;</p></li>
<li class="fragment"><p><strong>Bootstrap aggregating</strong> (<strong>bagging</strong>): técnica de aprendizagem <em>ensemble</em>, em que cria-se multiplos conjuntos de dados obtidos com reposição da amostra de treinamento. O modelo de aprendizagem de máquina é treinado em cada conjunto de dados e as previsões de cada um dos modelos são combinadas por meio da média (em problemas de regressão), ou por voto majoritário, em problemas de classificação. O <em>bagging</em> é utilizado para reduzir a variância e melhorar a estabilidade do modelo.</p></li>
</ol>
</div><div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/sign1357-gra-0003-m.jpg"></p>
</figure>
</div>
</div>
</div>
</section></section>
<section>
<section id="section-3" class="title-slide slide level1 title center" data-background-image="imgs/rawpixel/freight.jpg">
<h1></h1>
<div class="r-fit-text">
<p><span class="flow">Regressão / Parte I</span></p>
</div>
</section>
<section id="regressão" class="slide level2">
<h2>Regressão</h2>
<p><br></p>
<p>Métodos de regressão surgiram há mais de dois séculos com Legendre (1805) e Gauss (1809), que exploraram o método dos mínimos quadrados com o objetivo de prever órbitas ao redor do Sol. Hoje em dia, o problema de estimação de uma função de regressão possui papel central em estatística.</p>
<p><br></p>
<blockquote>
<p>Apesar de as primeiras técnicas para solucionar esse problema datarem de ao menos 200 anos, os avanços computacionais recentes permitiram que novas metodologias fossem exploradas. Em particular, com a capacidade cada vez maior de armazenamento de dados, métodos com menos suposições sobre o verdadeiro estado da natureza ganham cada vez mais espaço. Com isso, vários desafios surgiram: por exemplo, métodos tradicionais não são capazes de lidar de forma satisfatória com bancos de dados em que há mais covariáveis que observações, uma situação muito comum nos dias de hoje. Similarmente, são frequentes as aplicações em que cada observação consiste em uma imagem ou um documento de texto, objetos complexos que levam a análises que requerem metodologias mais elaboradas. – Izbick et al.</p>
</blockquote>
</section>
<section id="regressão-1" class="slide level2">
<h2>Regressão</h2>
<p><br></p>
<p>De forma geral, temos que o objetivo de um modelo de regressão é determinar a relação entre uma variável aleatória (label) <span class="math inline">Y \in \mathbb{R}</span> e um vetor de covariáveis (features) <span class="math inline">\mathbf{x} = (x_1, \cdots, x_d) \in \mathbb{R}^d</span>. Mais especificamente, busaca-se estimar</p>
<p><span class="math display">r(\bf{x}) := \mathbb{E}(Y|\bf{X} = \bf{x}),</span></p>
<p>sendo esta chamada de <span class="red">função de regressão</span>. Temos que:</p>
<p><br></p>
<ol type="1">
<li class="fragment">Se <span class="math inline">Y</span> é uma variável quantitativa, então estamos sob um problema de <span class="red">regressão</span>;</li>
<li class="fragment">Se <span class="math inline">Y</span> é uma variável qualitativa, então teremos um problema de <span class="red">classificação</span>.</li>
</ol>
<p>Em aprendizagem de máquina, assumimos que não temos meios de calcular <span class="math inline">r({\bf{x}})</span>, i.e., não conhecemos a distribuição condicional de <span class="math inline">{\bf{Y}\,|\,X}</span>. Portanto, não temos meios de calcular</p>
<p><span class="math display">\mathbb{E}({\bf X}|Y = y) = \int x\,\mathrm{d}F_{\bf X}({\bf x} | Y = y).</span></p>
</section>
<section id="notações" class="slide level2">
<h2>Notações</h2>
<p><br></p>
<p>A variável <span class="math inline">Y</span> recebe frequentemente o nome de variável resposta, variável dependente, rótulo ou <em>label</em>. Já as observações contidas no vetor <span class="math inline">\bf{x} = (x_1, \cdots, x_d)</span>, são, em geral, denominadas de variáveis explicativas, variáveis independentes, características, atributos, preditores, covariáveis ou <em>features</em>.</p>
<p><br></p>
<p>A ideia, nessa primeira parte do curso, é descrever algumas técnicas para estimar (<strong>treinar</strong>, como é dito em aprendizagem de máquina) <span class="math inline">r(\bf{x})</span>.</p>
<p><br></p>
<p>A menos quando dito o contrário, assumiremos que nossa amostra são i.i.d. (independentes e identicamente distribuídas), ou seja, <span class="math inline">(\bf{X}_1, Y_1), \cdots, (\bf{X}_n, Y_n)</span> são i.i.d.</p>
<p><br></p>
<p>Denota-se por <span class="math inline">x_{i,j}</span> o valor da <span class="math inline">j</span>-ésima covariável na <span class="math inline">i</span>-ésima amostra, com <span class="math inline">j = 1, \cdots, d</span> e <span class="math inline">i = 1, \cdots, n</span>.</p>
</section>
<section id="notações-1" class="slide level2">
<h2>Notações</h2>
<p><br></p>
<table style="width:50%;">
<caption>Notação utilizada nesse material para as variáveis envolvidas em um problema de regressão.</caption>
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Label</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">Y_1</span></td>
<td><span class="math inline">X_{1,1},\cdots, X_{1,d}\,\,\, (= \bf{X}_1)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\vdots</span></td>
<td><span class="math inline">\,\,\,\vdots\,\,\,\,\, \ddots\,\,\ \vdots</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">Y_n</span></td>
<td><span class="math inline">X_{n,1},\cdots, X_{n,d}\,\,\, (= \bf{X}_n)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="regressão-2" class="slide level2">
<h2>Regressão</h2>
<p>Nossa ideia é construir uma boa estimativa <span class="math inline">g</span> da função de regressão <span class="math inline">r(\bf{x}) := \mathbb{E}(Y\,|\,\bf{X} = \bf{x})</span>, para novas observações, i.e., queremos obter uma função <span class="math inline">g</span>, tal que:</p>
<p><span class="math display">g: \mathbb{R}^d \rightarrow \mathbb{R},</span></p>
<p>de tal forma que <span class="math inline">g</span> possua um bom poder preditivo. Em aprendizagem de máquina, só estaremos interessados em obter uma função <span class="math inline">g</span> que estime bem um número real (em problemas de regressão), ou que classifique bem (em um problema de classificação), utilizando as <span class="math inline">d</span> covariáveis. Ou seja, para <span class="math inline">m</span> novas observações, desejamos obter <span class="math inline">g</span>, que</p>
<p><span class="math display">g({\bf{x}}_{n + 1}) \approx y_{n + 1}, \cdots, g({\bf{x}}_{n + m}) \approx y_{n + m}.</span></p>
</section>
<section id="função-de-risco" class="slide level2">
<h2>Função de risco</h2>
<p><br></p>
<p>Para que possamos construir boas funções de predição, é preciso que tenhamos um critério para medir o desempenho de uma dada função <span class="math inline">g:\mathbb{R}^d \rightarrow \mathbb{R}</span>. Em contexto de regressão, usaremos o risco quadrático, muito embora esta não é a única opção. Denotaremos a função de risco quadrático por:</p>
<p><span class="math display">R_{pred}(g) = \mathbb{E}\left[({\bf Y} - g({\bf X}))^2\right],</span> em que <span class="math inline">(\bf X, Y)</span> são observações novas que não foram utilizadas para treinar/estimar <span class="math inline">g</span>. Lê-se <span class="math inline">R_{pred}(g)</span> como “risco preditivo de <span class="math inline">g</span>”. Note que, como <span class="math inline">\bf X</span> são observações conhecidas e <span class="math inline">g(\cdot)</span> é um modelo preditivo, portanto, <span class="math inline">g</span> é conhecido, então, <span class="math inline">\widehat{\bf Y} = g(\bf X)</span> é um estimador dos <em>labels</em>, i.e., de <span class="math inline">\bf Y</span>.</p>
<p><br></p>
<p>Diremos que <span class="math inline">L(g({\bf X}); {\bf Y}) = ({\bf Y} - g({\bf X}))^2</span> é a <span class="red">função de perda quadrática</span>, as vezes chamado de perda <span class="math inline">L_2</span>. Outra funções como a <span class="red">função de perda absoluta</span> denotada por <span class="math inline">L(g({\bf X}); {\bf Y}) = |{\bf Y} - g({\bf X})|</span>, as vezes chamada de perda <span class="math inline">L_1</span> poderiam ser consideradas.</p>
</section>
<section id="função-de-risco-1" class="slide level2">
<h2>Função de risco</h2>
<p><br></p>
<p>Em linhas gerais, seja <span class="math inline">L(\cdot)</span> uma função qualquer, tal que <span class="math inline">\forall \, 0 &lt; u &lt; v</span>, de modo que:</p>
<p><br></p>
<ol type="i">
<li class="fragment"><span class="math inline">0 = L(0) \leq L(u) \leq L(v)</span>;</li>
<li class="fragment"><span class="math inline">0 = L(0) \leq L(-u) \leq L(-v)</span>.</li>
</ol>
<p><br></p>
<p>Qualquer função <span class="math inline">L(\cdot)</span> que satisfaz as propriedades acima é chamada de <a href="https://en.wikipedia.org/wiki/Loss_function">função de perda</a>. Em especial, temos que:</p>
<p><br></p>
<ul>
<li class="fragment">Função de perda quadrática: <span class="math inline">L(u) = u^2</span>;</li>
<li class="fragment">Função de perda absoluta: <span class="math inline">L(u) = |u|</span>;</li>
<li class="fragment">Função de perda degrau: <span class="math inline">L(0) = 0</span>, se <span class="math inline">|u| &lt; \delta</span> e <span class="math inline">1</span> caso contrário, para algum <span class="math inline">\delta &gt; 0</span>;</li>
</ul>
</section>
<section id="função-de-risco-2" class="slide level2">
<h2>Função de risco</h2>
<p><br></p>
<p>Normalmente considera-se a perda <span class="math inline">L_2</span>, uma vez que em modelos de regressão, minimizar <span class="math inline">R_{pred}(g)</span>, em <span class="math inline">g</span>, equivale a encontrar <span class="math inline">r({\bf x}) = \mathbb{E}({\bf X}|{\bf Y})</span>, i.e., equivale a estimar a função de regressão.</p>
<p><br></p>
<p><span class="red">Teorema</span>: Suponha que definimos o risco de uma função de predição <span class="math inline">g: \mathbb{R}^d \rightarrow \mathbb{R}</span> via função perda quadrática, i.e, <span class="math inline">R_{pred}(g) = \mathbb{E}\left[({\bf Y} - g({\bf X}))^2\right]</span>, em que <span class="math inline">\bf (X, Y)</span> são novas observações que não foram utilizadas para estimar <span class="math inline">g</span>. Suponha também que estimamos o risco de um estimador de regressão <span class="math inline">r({\bf X})</span> via função perda quadrática <span class="math inline">R_{reg}(g) = \mathbb{E}\left[(r({\bf X}) - g({\bf X}))^2\right]</span>. Então,</p>
<p><span class="math display">R_{pred}(g) = R_{reg}(g) + \mathbb{E}\left[\mathbb{V}[{\bf Y} | {\bf X}]\right],</span></p>
<p>em que <span class="math inline">\mathbb{E}\left[\mathbb{V}[{\bf Y} | {\bf X}]\right]</span> é a variância média do modelo que não depende de <span class="math inline">g</span>. Portanto, estimar bem <span class="math inline">r({\bf x})</span> é de fundamental importância para criar uma boa função de predição. Em especial, sob a ótica do risco quadrático, a melhor função de predição para <span class="math inline">\bf Y</span> é a função de regressão <span class="math inline">r({\bf x})</span>, de tal modo que:</p>
<p><span class="math display">\argmin_g R_{pred}(g) = \argmin_g R_{reg}(g) = r({\bf x}).</span></p>
</section>
<section id="função-de-risco-3" class="slide level2">
<h2>Função de risco</h2>
<p><br></p>
<p><strong>Lembre-se</strong>: <span class="math inline">r({\bf x}) = \mathbb{E}(Y | \bf{X} = \bf{x})</span> é a nossa <span class="red">função de regressão</span>.</p>
<p><br></p>
<p>A definição de risco preditivo <span class="math inline">R_{pred}</span>, que também denotaremos simplesmente por <span class="math inline">R</span>, tem um apelo frequentista. Dessa forma, para um novo conjunto com <span class="math inline">m</span> novas observaçõs, <span class="math inline">({\bf X}_{n+1}, Y_{n+1}), \cdots, ({\bf X}_{n+m}, Y_{n+m})</span>, temos que essa nova amostra é i.i.d. à amostra observada (utilizada no treinamento do modelo/na estimação). Então, pela Lei dos Grandes Números, temos que um bom estimador para a função para o risco preditivo é dado por:</p>
<p><span id="eq-risco-correto"><span class="math display">\frac{1}{m}\sum_{i = 1}^m (Y_{n + i} - g(X_{n + i}))^2 \approx R(g) := \mathbb{E}\left[(Y - g({\bf X}))^2\right]. \tag{1}</span></span></p>
<p>Chamaremos a quantidade acima de <a href="https://pt.wikipedia.org/wiki/Erro_quadr%C3%A1tico_m%C3%A9dio">Erro Quadrático Médio - EQM</a>. Em aprendizagem de máquina, normalmente estaremos no contexto em que temos muitas observações, e que portanto, poderemos fazer esse apelo frequentista.</p>
<p><br></p>
<p>Desejamos encontrar <span class="math inline">g</span> (encontrar métodos) que minimize de forma satisfatória <span class="math inline">R</span>, i.e., métodos que nos conduzam à um risco baixo.</p>
</section>
<section id="função-de-risco-4" class="slide level2">
<h2>Função de risco</h2>
<p><br></p>
<p>Sendo assim, se <span class="math inline">R(g)</span> possui um valor baixo, então, temos que</p>
<p><span class="math display">g({\bf x}_{n+1}) \approx y_{n+1}, \cdots, g({\bf x}_{n+m}) \approx y_{n+m}.</span> <br> <img data-src="gifs/hum.gif" style="width:25.0%"></p>
</section>
<section id="regressão-linear" class="slide level2">
<h2>Regressão linear</h2>
<p><br></p>
<p>Nesse momento, vamos pensar um pouco em regressão linear. No caso mais simples, queremos prever o comportamento de uma variável de interesse <span class="math inline">Y</span> condicional a uma variável explicativa <span class="math inline">X</span> (regressão linear simples, i.e., <span class="math inline">d = 1</span>). O melhor preditor de <span class="math inline">Y</span> condicional em <span class="math inline">X</span> é aquele que minimiza a função de perda esperada, ou seja, é aquele que resolve:</p>
<p><span class="math display">\argmin_g \mathbb{E}(L(Y - g)|X).</span></p>
<p>Para o caso da função perda quadrática (função <span class="math inline">L_2</span>), o melhor preditor de <span class="math inline">Y</span> condicional à <span class="math inline">X</span> é a média condicional de <span class="math inline">Y</span> dado <span class="math inline">X</span>, i.e., <span class="math inline">r(X) = \mathbb{E}(Y|X)</span>. Já, na situação em que considera-se a perda absoluta (função <span class="math inline">L_1</span>), o melhor estimador é a mediana condicional.</p>
<p><br></p>
<p><strong>Os modelos de regressão, em geral, fazem uso da função de perda quadrática.</strong></p>
</section>
<section id="regressão-linear-simples" class="slide level2">
<h2>Regressão linear simples</h2>
<p><br></p>
<p>No caso da regressão linear simples (<span class="math inline">d = 1</span>), temos que o modelo é dado por:</p>
<p><span class="math display">g(x) = \beta_0 + \beta_1 x_{i,1} + \varepsilon_i, \,\, i = 1, \cdots, n,</span> em que <span class="math inline">\varepsilon_i</span> é um erro aleatório. Na abordagem <em>data modeling culture</em>, várias suposições poderem ser feitas para <span class="math inline">\varepsilon_i</span>.</p>
<p>Assumindo que a regressão linear simples é o modelo <span class="math inline">g</span> que iremos utilizar, então, desejamos minimizar:</p>
<p><span class="math display">\argmin_{\beta} R(g_\beta) = \argmin_{\beta} \sum_{i = 1}^n(y_i - \beta_0 - \beta_1x_{i,1})^2.</span> Derivando em relação à <span class="math inline">\beta</span> e igualando a zero, após algumas manipulações algébricas, temos que:</p>
<p><span class="math display">\widehat{\beta} = \frac{\sum_{i = 1}^n (x_i - \overline{x})(y_i - \overline{y})}{\sum_{i=1}^n(x_i - \overline{x})^2} = r_{xy}\frac{s_y}{s_x},</span> em que <span class="math inline">s_x</span> e <span class="math inline">s_y</span> são os desvio-padrão de <span class="math inline">x</span> e <span class="math inline">y</span>, respectivamente, e <span class="math inline">r_{xy}</span> é o coeficiente de correlação da amostra, em que <span class="math inline">-1 \leq r_{xy} \leq 1</span>.</p>
</section>
<section id="regressão-linear-simples-1" class="slide level2">
<h2>Regressão linear simples</h2>
<p><br></p>
<p><span class="math display">r_{xy} = \frac{\overline{xy} - \overline{x}\,\overline{y}}{\sqrt{(\overline{x^2} - \overline{x}^2)(\overline{y^2} - \overline{y}^2)}}.</span> O coeficiente de determinação <span class="math inline">R^2</span> do modelo é dado por <span class="math inline">r_{xy}^2</span>, quando o modelo é linear e possue uma única variável independente (feature).</p>
<p><br></p>
<p>Portanto, temos que:</p>
<p><span class="math display">\widehat{\beta_0} = \overline{y} - \widehat{\beta}\overline{x},</span></p>
<p>Na <span class="red"><em>data modeling culture</em></span> (na estatística), normalmente assumimos que o <span class="math inline">\varepsilon_i</span> tem distribuição normal e variância constante, <span class="math inline">\forall\, i = 1, \cdots, n</span>. Assume-se também que <span class="math inline">\mathbb{E}(\varepsilon_i) = 0, \, \forall i</span>.</p>
</section>
<section id="regressão-linear-simples-2" class="slide level2">
<h2>Regressão linear simples</h2>
<p><br></p>
<p>Aqui não iremos nos preocupar com essas suposições, uma vez que em <span class="red"><em>algorithmic modeling culture</em></span>, não estamos preocupados com suposições nem interpretações, ok!?</p>
<p><br></p>

<img data-src="gifs/ok-2.gif" class="r-stretch"></section>
<section id="regressão-linear-multipla" class="slide level2">
<h2>Regressão linear multipla</h2>
<p><br></p>
<p>A função de perda quadrática (função <span class="math inline">L_2</span>) tem algumas vantagens em relação a função de perda absoluta. Listo algumas:</p>
<ol type="1">
<li class="fragment"><p>A função de perda quadrática penaliza mais os erros maiores, devido ao fato dos erros serem levado ao quadrado;</p></li>
<li class="fragment"><p>A função de perda quadrática é mais sensível a presença de <a href="https://en.wikipedia.org/wiki/Outlier"><em>outlier</em></a>, que em compensação são menos penalizados ao se considerar a função de perda absoluta (função <span class="math inline">L_1</span>);</p></li>
<li class="fragment"><p>Em situações em que o erro tem distribuição normal, a estimativa de mínimos quadrados é a solução de máxima verossimilhança e é a estimativa linear não viesada e com menor variância. Portanto, gozamos de um estimador com ótimas propriedades, muito embora ele também é um bom estimador mesmo quando a suposição de normalidade não é verificada;</p></li>
<li class="fragment"><p>A função de perda quadrática é deferenciável, já a função de perda absoluta não é.</p></li>
</ol>
<p>Para o caso de regressão linear múltipla, i.e., quando <span class="math inline">d &gt; 1</span>, poderemos utilizar uma notação matricial para representar o modelo linear múltiplo de regressão.</p>
</section>
<section id="regressão-linear-multipla-1" class="slide level2">
<h2>Regressão linear multipla</h2>
<p><br></p>
<p>Considerando o modelo de regressão linear múltiplo, temos que:</p>
<p><span class="math display">Y = g({\bf X}) = \beta^{T}{\bf X} + \varepsilon,</span></p>
<p>em que <span class="math inline">Y</span> é um vetor <span class="math inline">n \times 1</span>, <span class="math inline">{\bf X}</span> é uma matriz fixa e conhecida com os atributos de dimensão <span class="math inline">n \times d</span>, em que a primeira coluna é preenchida de 1, <span class="math inline">\beta = (\beta_0, \cdots, \beta_d)</span>. Na cultura de <em>machine learning</em>, iremos desconsiderar <span class="math inline">\varepsilon</span>, i.e., não feremos suposições sobre <span class="math inline">\varepsilon</span>. Portanto, considere</p>
<p><span class="math display">g({\bf x}) = \beta^{T}{\bf X} = \beta_{0}x_0 + \beta_1x_{i,1} + \cdots + \beta_dx_{i,d},</span> em que <span class="math inline">x_0 \equiv 1</span>.</p>
</section>
<section id="regressão-linear-multipla-2" class="slide level2">
<h2>Regressão linear multipla</h2>
<p><br></p>
<p>O método dos mínimos quadrados, para o caso de regressão linear múltipla (<span class="math inline">d &gt; 1</span>) é dado por aquele que minimiza <span class="math inline">R(\beta^{T}{\bf X})</span>, i.e., minimiza:</p>
<p><span class="math display">\argmin_\beta \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1x_{i,1} - \cdots - \beta_dx_{i,d})^2.</span> Temos que</p>
<p><span class="math display">\widehat{\beta} = ({\bf X}^{T}{\bf X})^{-1}{\bf X}^{T}Y.</span></p>
<p>Portanto, a função de regressão estimada é dada por:</p>
<p><span class="math display">g({\bf x}) = \widehat{\beta}^{T}{\bf x}.</span></p>
</section>
<section id="regressão-linear-multipla-3" class="slide level2">
<h2>Regressão linear multipla</h2>
<p><br></p>
<p>Grande parte da literatura estatística é voltada para justificar que o método de mínimos quadrados sob um ponto de vista de um estimador de máxima verossimilhança, assim como também para construção de testes de aderência, métodos para construção de intervalos de confiança e teste de hipótese para <span class="math inline">\beta_i</span> (parâmetros que indexam o modelo), análise de resíduos, entre outros.</p>
<p><br></p>
<p>Assumir que a verdadeira regressão <span class="math inline">r({\bf x}) = \mathbb{E}({\bf X}\,|\,Y)</span> é uma suposição muito forte. Contudo, existe, na literatura, justificativas para o uso de métodos de mínimos quadrados para estimar os coeficientes, mesmo quando a regressão real <span class="math inline">r({\bf x})</span> não satisfaz a suposição de linearidade.</p>
<p><br></p>

<img data-src="gifs/chapulin-colorado-no.gif" class="r-stretch"></section>
<section id="regressão-linear-multipla-4" class="slide level2">
<h2>Regressão linear multipla</h2>
<p><br></p>
<p>O estimador de mínimos quadrados <span class="math inline">\widehat{\beta} = ({\bf X}^{T}{\bf X})^{-1}{\bf X}^{T}Y</span> é bom, por alguns motivos:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>É igual ao estimador de máxima verossimilhança sob normalidade, linearidade e homoscedasticidade, portanto, consistente sob essas condições</p></li>
<li class="fragment"><p>É <span class="red"><em>best linear unbiased prediction</em> - BLUE</span> sob linearidade e homoscedasticidade;</p></li>
<li class="fragment"><p>O método de mínimos quadrados tem alguma garantia, mesmo sem assumir muitas suposições.</p></li>
</ol>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="mínimos-quadrados-sem-suposição-de-linearidade" class="slide level2">
<h2>Mínimos quadrados sem suposição de linearidade</h2>
<p><br></p>
<p>Quando a suposição de linearidade falha, ou seja, quando a regressão verdadeira que desconhecemos <span class="math inline">r({\bf x})</span> não é linear, frequentemente existe um vetor <span class="math inline">\beta_{*}</span>, tal que <span class="math inline">g_{\beta_{*}}({\bf x}) = \beta_{*}^{T}{\bf x}</span> tem um bom poder preditivo. Nesses casos, o métrodo dos mínimos quadrados <span class="math inline">\widehat{\beta}</span> tende a produzir estimadores com baixo risco. Isso se deve ao fato que <span class="math inline">\widehat{\beta}</span> converge para o melhor preditor linear (para o oráculo <span class="math inline">\beta_{*}</span>) que é dado por:</p>
<p><span class="math display">\beta_{*} = \argmin_\beta R(g_\beta) =  \argmin_\beta \mathbb{E}\left[(Y - \beta^{T}X)^2\right],</span> mesmo que a verdadeira regressão <span class="math inline">r({\bf x})</span> não seja linear, em que <span class="math inline">({\bf X}, Y)</span> é uma nova observação.</p>
<p><br></p>
<p><span class="red">Teorema</span>: Seja <span class="math inline">\beta_{*}</span> o melhor estimador linear e <span class="math inline">\widehat{\beta}</span> o estimador de mínimos quadrados. Então,</p>
<p><span class="math display">\widehat{\beta}\overset{p}{\longrightarrow}  \beta_{*}\,\, \mathrm{e}\,\, R(g_{\widehat{\beta}})\overset{p}{\longrightarrow} R(g_{\beta_{*}}), </span> quando <span class="math inline">n \longrightarrow \infty</span>. Para uma demonstração, veja <a href="http://www.rizbicki.ufscar.br/AME.pdf" class="uri">http://www.rizbicki.ufscar.br/AME.pdf</a>, página. 29.</p>
</section>
<section id="mínimos-quadrados-sem-suposição-de-linearidade-1" class="slide level2">
<h2>Mínimos quadrados sem suposição de linearidade</h2>
<p><br></p>
<p>Em palavras, o que o Teorema anterior diz é que mesmo quando a regressão verdadeira não é linear, o estimador de mínimos quadrados é consistente para nos conduzir a um bom estimador <strong>linear</strong>, ou seja, ao menos conseguiremos o melhor estimador linear como uma aproximação à <span class="math inline">r({\bf x})</span> que não é linear.</p>
<p><br></p>
<p>Isso não quer dizer que você terá boas estimativas em todas as situações, muito embora o oráculo <span class="math inline">\beta_{*}</span>, em muitas situações, terá bom poder preditivo. Em outras palavras, em situações que um problema, em sua natureza, não linear, poderemos alcançar boas estimativas por uma aproximação linear pelo método dos mínimos quadrados.</p>
<p><br></p>

<img data-src="gifs/uau.gif" class="r-stretch"></section>
<section id="leia-mais-sobre-regressão-linear" class="slide level2">
<h2>Leia mais sobre regressão linear</h2>
<p><br></p>
<p>Caso você deseje ler um pouco mais sobre regressão linear sob homocedasticidade e sob heteroscedasticidades, leia o segundo Capítulo de minha dissertação de mestrado intitulada <strong>Estimadores Intervalares sob Heteroscedasticidade de Forma Desconhecida via Bootstrap Duplo</strong>. Apesar do título, o segundo capítulo é uma revisão do conceito de regressão linear é apresentado de forma didática. Clique <a href="pdf/regressao_linear.pdf">aqui</a> para ler.</p>
<p><br></p>

<img data-src="gifs/chapulin-colorado-no.gif" style="width:20.0%" class="r-stretch"></section>
<section id="predição-versus-inferência" class="slide level2">
<h2>Predição versus Inferência</h2>
<p><br></p>
<p><strong>Inferência</strong>: assume que o modelo linear é correto. O principal objetivo consiste em interpretar os parâmetros:</p>
<p><br></p>
<ul>
<li class="fragment">Quais são os parâmetros significantes?</li>
<li class="fragment">Qual o efeito do aumento da dose de um remédio no paciente?</li>
</ul>
<p><br></p>
<p><strong>Predição</strong>: queremos criar <span class="math inline">g({\bf x})</span> com bom poder preditivo, mesmo que a especificação do modelo não esteja correta. Não assume que a verdadeira regressão é de fato linear! A interpretação aqui não é o foco. Tudo bem?!</p>
<p><br></p>

<img data-src="gifs/ok.gif" style="width:15.0%" class="r-stretch"></section>
<section id="ajustando-uma-regressão-linear-no-r" class="slide level2">
<h2>Ajustando uma regressão linear no R</h2>
<p><br></p>
<p>Caso você não queira implementar o estimador de mínimos quadrados <span class="math inline">\widehat{\beta} = ({\bf X}^{T}{\bf X})^{-1}{\bf X}^{T}Y</span>, você poderá utilizar a famosa função <code>lm</code>. Na verdade é melhor que não implemente o estimador <span class="math inline">\widehat{\beta}</span>, uma vez que a função <code>lm</code>, assim como a função <code>glmnet</code> do pacote <a href="https://glmnet.stanford.edu/articles/glmnet.html">glmnet</a>, utilizam-se de truques numéricos para um cálculo mais eficiente.</p>
<p><br></p>
<p>Falaremos do pacote <a href="https://glmnet.stanford.edu/articles/glmnet.html">glmnet</a>, um pouco mais a frente, quando abordarmos regressão penalizada. Certo!?</p>
<p><br></p>

<img data-src="gifs/bom.gif" class="r-stretch"></section>
<section id="ajustando-uma-regressão-linear-no-r-1" class="slide level2">
<h2>Ajustando uma regressão linear no R</h2>
<p><br></p>
<p>Considere o conjunto de dados de expectativa de vida versus PIB per Capita disponíveis <a href="https://github.com/prdm0/dados/blob/main/dados_expectativa_renda.RData">aqui</a>. O comportamente entre as variáveis <code>LifeExpectancy</code> e <code>GDPercapita</code>, se fizermos um gráfico, não é linear.</p>
<p><br></p>
<p>Todavia, isso não impede que possamos ajustar um modelo de regressão linear, muito embora o seu poder preditivo será baixo.</p>
<p><br></p>
<p>Porém, como já sabemos, ao menos conseguiremos o melhor oráculo, denotado por <span class="math inline">\beta_{*}</span>, i.e., o melhor estimador dentre os possíveis estimadores lineares, como mostrado em teoremas anteriores.</p>
<p><br></p>
<p>E está tudo bem. Aqui não estou querendo defender que você use uma aproximação linear para esse caso. Em breve, com um pequeno truque, poderemos ajustar uma regressão polinomial à esses dados, e incorporaremos um pouco da tendência não linar presente nos dados.</p>
</section>
<section id="ajustando-uma-regressão-linear-no-r-2" class="slide level2">
<h2>Ajustando uma regressão linear no R</h2>
<p><br></p>
<div class="cell" data-layout-align="center">
<details>
<summary>Veja o código do gráfico</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb4-2"><a href="#cb4-2"></a></span>
<span id="cb4-3"><a href="#cb4-3"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="co"># Criando um arquivo temporário</span></span>
<span id="cb4-6"><a href="#cb4-6"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb4-7"><a href="#cb4-7"></a></span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="co"># Baixando um arquivo temporário</span></span>
<span id="cb4-9"><a href="#cb4-9"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb4-10"><a href="#cb4-10"></a></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="co"># Carregando os dados</span></span>
<span id="cb4-12"><a href="#cb4-12"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb4-13"><a href="#cb4-13"></a></span>
<span id="cb4-14"><a href="#cb4-14"></a>dados_expectativa_renda <span class="sc">|&gt;</span> </span>
<span id="cb4-15"><a href="#cb4-15"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> GDPercapita, <span class="at">y =</span> LifeExpectancy)) <span class="sc">+</span></span>
<span id="cb4-16"><a href="#cb4-16"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb4-17"><a href="#cb4-17"></a>  <span class="fu">labs</span>(</span>
<span id="cb4-18"><a href="#cb4-18"></a>    <span class="at">title =</span> <span class="st">"PIB per Capita versus Expectativa de Vida"</span>,</span>
<span id="cb4-19"><a href="#cb4-19"></a>    <span class="at">x =</span> <span class="st">"PIB per Capita"</span>,</span>
<span id="cb4-20"><a href="#cb4-20"></a>    <span class="at">y =</span> <span class="st">"Expectativa de Vida"</span></span>
<span id="cb4-21"><a href="#cb4-21"></a>  ) <span class="sc">+</span></span>
<span id="cb4-22"><a href="#cb4-22"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb4-23"><a href="#cb4-23"></a>  <span class="fu">theme</span>(</span>
<span id="cb4-24"><a href="#cb4-24"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>),</span>
<span id="cb4-25"><a href="#cb4-25"></a>    <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb4-26"><a href="#cb4-26"></a>  )</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="index_files/figure-revealjs/unnamed-chunk-5-1.png" width="1200" height="700"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="ajustando-uma-regressão-linear-no-r-3" class="slide level2">
<h2>Ajustando uma regressão linear no R</h2>
<p><br></p>
<p>Claramente, a reta de regressão (linha azul) do gráfico anterior não tem um bom poder preditivo. O ajuste foi feito diretamente usando o pacote <a href="https://ggplot2.tidyverse.org/">ggplot2</a>, utilizando a função <code>geom_smooth</code>, em que foi escolhido o método <code>"lm"</code>.</p>
<p><br></p>
<p>Poderíamos ter utilizado a função <code>lm</code>:</p>
<p><br></p>
<div class="cell" data-layout-align="center">
<details>
<summary>Veja o código do gráfico</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb5-2"><a href="#cb5-2"></a></span>
<span id="cb5-3"><a href="#cb5-3"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="co"># Criando um arquivo temporário</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb5-7"><a href="#cb5-7"></a></span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="co"># Baixando um arquivo temporário</span></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb5-10"><a href="#cb5-10"></a></span>
<span id="cb5-11"><a href="#cb5-11"></a><span class="co"># Carregando os dados</span></span>
<span id="cb5-12"><a href="#cb5-12"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb5-13"><a href="#cb5-13"></a></span>
<span id="cb5-14"><a href="#cb5-14"></a><span class="co"># Ajustando o modelo usando a função lm</span></span>
<span id="cb5-15"><a href="#cb5-15"></a>ajuste <span class="ot">&lt;-</span> <span class="fu">lm</span>(LifeExpectancy <span class="sc">~</span> GDPercapita, <span class="at">data =</span> dados_expectativa_renda)</span>
<span id="cb5-16"><a href="#cb5-16"></a></span>
<span id="cb5-17"><a href="#cb5-17"></a>modelo <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb5-18"><a href="#cb5-18"></a>  novos_dados <span class="ot">&lt;-</span> tibble<span class="sc">::</span><span class="fu">tibble</span>(<span class="at">GDPercapita =</span> x)</span>
<span id="cb5-19"><a href="#cb5-19"></a>  <span class="fu">predict</span>(ajuste, <span class="at">newdata =</span> novos_dados)</span>
<span id="cb5-20"><a href="#cb5-20"></a>}</span>
<span id="cb5-21"><a href="#cb5-21"></a></span>
<span id="cb5-22"><a href="#cb5-22"></a>dados_expectativa_renda <span class="sc">|&gt;</span> </span>
<span id="cb5-23"><a href="#cb5-23"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> GDPercapita, <span class="at">y =</span> LifeExpectancy)) <span class="sc">+</span></span>
<span id="cb5-24"><a href="#cb5-24"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb5-25"><a href="#cb5-25"></a>  <span class="fu">labs</span>(</span>
<span id="cb5-26"><a href="#cb5-26"></a>    <span class="at">title =</span> <span class="st">"PIB per Capita versus Expectativa de Vida"</span>,</span>
<span id="cb5-27"><a href="#cb5-27"></a>    <span class="at">x =</span> <span class="st">"PIB per Capita"</span>,</span>
<span id="cb5-28"><a href="#cb5-28"></a>    <span class="at">y =</span> <span class="st">"Expectativa de Vida"</span></span>
<span id="cb5-29"><a href="#cb5-29"></a>  ) <span class="sc">+</span></span>
<span id="cb5-30"><a href="#cb5-30"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> modelo, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">size =</span> <span class="fl">1.2</span>) <span class="sc">+</span></span>
<span id="cb5-31"><a href="#cb5-31"></a>  <span class="fu">theme</span>(</span>
<span id="cb5-32"><a href="#cb5-32"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>),</span>
<span id="cb5-33"><a href="#cb5-33"></a>    <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb5-34"><a href="#cb5-34"></a>  )</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="index_files/figure-revealjs/unnamed-chunk-6-1.png" width="1200" height="700"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="matriz-esparsa" class="slide level2">
<h2>Matriz esparsa</h2>
<p><br></p>
<p>Para grandes bases de dados, em um problema real que você venha trabalhar, e se o custo computacional você considera elevado, poderá utilizar o pacote <a href="https://cran.r-project.org/web/packages/biglm/index.html">biglm</a>.</p>
<p><br></p>
<p>Em situações em que há muitos zeros na sua matriz, poderá utilizar representação <a href="https://en.wikipedia.org/wiki/Sparse_matrix">esparsa</a>.</p>
<p><br></p>
<p><span class="red">Matrizes esparsas</span> são matrizes com muitas entradas iguais à <span class="math inline">0</span>. Elas ocorrem naturalmente em diversas aplicações, como por exemplo uma matriz de termos presentes em um documento, em que se o termo estiver no documento resebe 1, e zero, caso contrário. Abaixo, <span class="math inline">{\bf X}</span> é um exemplo de matriz esparsa.</p>
<p><br></p>
<p><span class="math display">
{\bf X} =
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 3 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 4 &amp; 0 \\
\end{bmatrix}
</span></p>
</section>
<section id="matriz-esparsa-1" class="slide level2">
<h2>Matriz esparsa</h2>
<p><br></p>
<p>Considere os textos:</p>
<ol type="1">
<li class="fragment"><span class="red">Texto 1</span>: “Eu amo essa disciplina.”</li>
<li class="fragment"><span class="red">Texto 2</span>: “Eu adoro meu professor.”</li>
<li class="fragment"><span class="red">Texto 3</span>: “Eu serei muito bom em aprendizagem de máquina.”</li>
<li class="fragment"><span class="red">Texto 4</span>: “Adoro o departamento de estatística da UFPB.”</li>
</ol>
<p><br></p>
<table>
<colgroup>
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th>Textos</th>
<th>disciplina</th>
<th>amo</th>
<th>aprendizagem</th>
<th>máquina</th>
<th>estatistica</th>
<th>adoro</th>
<th>UFPB</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="red">Texto 1</span></td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td><span class="red">Texto 2</span></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td><span class="red">Texto 3</span></td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td><span class="red">Texto 4</span></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</section>
<section id="matriz-esparsa-2" class="slide level2">
<h2>Matriz esparsa</h2>
<p><br></p>
<p>A matriz com a ocorrência de determinados termos nos textos é dada por:</p>
<p><span class="math display">
{\bf X} =
\begin{bmatrix}
1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\
\end{bmatrix}
</span></p>
<p>A representação esparsa de <span class="math inline">{\bf X}</span>, aqui denotada por <span class="math inline">{\bf X_*}</span> é:</p>
<p><span class="math display">
{\bf X_*} =
\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
2 &amp; 6 &amp; 1 \\
3 &amp; 3 &amp; 1 \\
3 &amp; 4 &amp; 1 \\
4 &amp; 5 &amp; 1 \\
4 &amp; 6 &amp; 1 \\
4 &amp; 7 &amp; 1 \\
\end{bmatrix},
</span> em que as duas primeiras colunas, são as linhas e colunas de <span class="math inline">{\bf X}</span> com valor diferente de 0. A última coluna representa o valor.</p>
</section>
<section id="regressão-linear-com-matriz-esparsa" class="slide level2">
<h2>Regressão linear com matriz esparsa</h2>
<p><br></p>
<p><strong>Exemplo</strong>: Ajuste de um modelo de regerssão linear múltiplo, em que <span class="math inline">{\bf X}</span> poderá ter uma representação esparsa. Aqui não estamos interessados em verificar qualidade de predições. Trata-se apenas de um exemplo de como utilizar uma representação esparsa para ajustar um modelo de regessão linear com algumas covariáveis, em R.</p>
<p><br></p>
<div class="cell">
<details>
<summary>Estude o código</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="fu">library</span>(Matrix)</span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="co"># Dados de exemplo</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb6-6"><a href="#cb6-6"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">0</span>)</span>
<span id="cb6-7"><a href="#cb6-7"></a>x3 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">6</span>, <span class="dv">0</span>, <span class="dv">7</span>)</span>
<span id="cb6-8"><a href="#cb6-8"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb6-9"><a href="#cb6-9"></a></span>
<span id="cb6-10"><a href="#cb6-10"></a><span class="co"># Criar data frame com as variáveis explicativas</span></span>
<span id="cb6-11"><a href="#cb6-11"></a>dados <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x1, x2, x3)</span>
<span id="cb6-12"><a href="#cb6-12"></a></span>
<span id="cb6-13"><a href="#cb6-13"></a><span class="co"># Converter o data frame para matriz esparsa</span></span>
<span id="cb6-14"><a href="#cb6-14"></a>X <span class="ot">&lt;-</span> <span class="fu">sparse.model.matrix</span>(<span class="sc">~</span> ., <span class="at">data =</span> dados)</span>
<span id="cb6-15"><a href="#cb6-15"></a></span>
<span id="cb6-16"><a href="#cb6-16"></a><span class="co"># Ajustar a regressão linear utilizando glmnet</span></span>
<span id="cb6-17"><a href="#cb6-17"></a>modelo <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> X, <span class="at">y =</span> y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> <span class="dv">0</span>)</span>
<span id="cb6-18"><a href="#cb6-18"></a></span>
<span id="cb6-19"><a href="#cb6-19"></a><span class="co"># Realizar previsões</span></span>
<span id="cb6-20"><a href="#cb6-20"></a>predicoes <span class="ot">&lt;-</span> <span class="fu">predict</span>(modelo, <span class="at">newx =</span> X)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="erro-quadrático-médio" class="slide level2">
<h2>Erro quadrático médio</h2>
<p><br></p>
<p>Como exposto anteriormente, para avaliar o poder preditivo de uma modelo, i.e., a aprendizagem de um modelo, devemos avaliar a função de risco, i.e., devemos avaliar <span class="math inline">R(g) := \mathbb{E}\left[L(g({\bf X}); Y)\right]</span>. Em particular, considere <span class="math inline">L = L_2</span> (função perda quadrática). Então, poderíamos ser levados a acreditar que o melhor estimador de <span class="math inline">R(g)</span>, utilizando a Lei dos Grandes Números seria:</p>
<p><span class="math display">\frac{1}{n}\sum_{i = 1}^n(Y_{i} - g({\bf X_{i}}))^2 \approx R(g) := \mathbb{E}\left[L_2(g({\bf X}); Y)\right].</span></p>
<p><br></p>
<p>Essa quantidade é chamada, de <strong>E</strong>rro <strong>Q</strong>uadrático <strong>M</strong>édio - <strong>EQM</strong>. Desejamos escolher o melhor mode, entre os modelos testados, que minimiza o EQM.</p>
<p><br></p>
<p>O apelo frequentista em utilizar a Lei dos Grandes Números na forma acima não é correto, uma vez que usamos as <span class="math inline">n</span> observações para treinar/ajustar o modelo <span class="math inline">g</span>.</p>
</section>
<section id="erro-quadrático-médio-1" class="slide level2">
<h2>Erro quadrático médio</h2>
<p><br></p>
<p>Por exemplo, no problema de PIB per Capita versus expectativa de vida, em que consideramos uma aproximação linear, não poderíamos utilizar o EQM da forma acima, com as <span class="math inline">n</span> observações utilizadas para treinar o modelo. É um detalhe sutil, mas que muitas pessoas cometem esse erro.</p>
<p><br></p>
<p>Não podemos utilizar as <span class="math inline">n</span> observações para estimar o risco <span class="math inline">R(g)</span> através do EQM, uma vez que estamos utilizando o mesmo conjunto de dados para ajustar e avaliar <span class="math inline">g</span>.</p>
<p><br></p>
<p><strong>Qual o problema?</strong></p>
<p><br></p>
<ol type="1">
<li class="fragment">Não vale a Lei dos Grandes Números;</li>
<li class="fragment">Usamos os mesmos valores de <span class="math inline">{\bf x}</span> e <span class="math inline">y</span> para treinar e avaliar o modelo.</li>
</ol>
</section>
<section id="erro-quadrático-médio-2" class="slide level2">
<h2>Erro quadrático médio</h2>
<p><br></p>
<p>O que diz a Lei dos Grandes Números, em particular, a Lei Forte de Kolmogorov?</p>
<p><br></p>
<p><span class="red">Teorema</span> (<strong>Lei Forte de Kolmogorov</strong>): Sejam <span class="math inline">X_1, \cdots, X_n</span> uma sequência de veriáveis aleatórias - v.a. i.i.d. e integráveis, i.e., com valor esperado limitado, tal que <span class="math inline">\mathbb{E}(X) = \mu\,\, \forall i</span>. Então,</p>
<p><span class="math display">\frac{X_1 + X_2 + \cdots + X_n}{n} \rightarrow \mu,</span></p>
<p>quase certamente, i.e., com probabilidade 1.</p>
<p><br></p>
<p>Note que se desejamos comparar diversos modelos, <span class="math inline">g_1({\bf x}), g_2({\bf x}), \cdots,</span> e se utilizarmos as mesmas <span class="math inline">n</span> obervações para calularmos <span class="math inline">R(g_1({\bf x})), R(g_2({\bf x})), \cdots</span>, os termos de cada uma das somas <strong>não são independentes</strong>. Lembre-se que desejamos obter <span class="math inline">\argmin_g R_{pred}(g)</span>.</p>
</section>
<section id="erro-quadrático-médio-3" class="slide level2">
<h2>Erro quadrático médio</h2>
<p><br></p>
<p>Portanto, nunca utilize as mesmas observações utilizadas para treinar o modelo, como aquelas que serão utilizadas para se estimar <span class="math inline">R(g)</span>. Nunca! Isso é um pecado mortal! Ok?!</p>
<p><br></p>

<img data-src="gifs/thumbs-up-nod.gif" style="width:20.0%" class="r-stretch"></section>
<section id="data-splitting" class="slide level2">
<h2>Data Splitting</h2>
<p><br></p>
<p>Corrigir o problema de dependência que há ao estimarmos o risco usando o EQM é fácil. Uma abordagem muito utilizada é utilizar <span class="red"><em>data splitting</em></span>, também chamado de método <span class="red"><em>hold-out</em></span>. Algo como a segunda linha da imagem abaixo:</p>
<p><br></p>

<img data-src="imgs/train-and-test-1-min-1.webp" style="width:35.0%" class="r-stretch"></section>
<section id="data-splitting-1" class="slide level2">
<h2>Data Splitting</h2>
<p><br></p>
<p>Essa divisão é feita de forma aleatória, algumas vezes estratificada de acordo com algumas variáváveis. A ideia de aleatorizar é se livrar de problemas de conjunto de dados ordenados. Queremos que tanto no conjunto de treinamento <span class="red"><em>Training</em></span> quanto no conjunto <span class="red"><em>Testing</em></span>, na imagem, contenham a mesma diversidade de observações.</p>
<p><br></p>
<p>Ainda no exemplo de PIB per Capita versus Expectaitiva de Vida, não quero correr o risco de ter no conjunto de treinamento apenas o países com maiores valores de PIB per Capita, caso o conjunto de dados tenha sido ordenado pela variável <code>GDPercapita</code>. Por isso, aleatorizar o conjunto de treinamento e teste é sempre uma ótima ideia. Certo!?</p>
<p><br></p>

<img data-src="gifs/ok-2.gif" style="width:20.0%" class="r-stretch"></section>
<section id="data-splitting-2" class="slide level2">
<h2>Data Splitting</h2>
<p><br></p>
<p>O percentual de divisão dos dados normalmente é empírico. Usa-se normalmente a proporção de <span class="math inline">70\%</span> para treinamento e <span class="math inline">30\%</span> para teste <span class="math inline">(70\%, 30\%)</span>. Outros esquemas de divisões são bastante utilizados, por exemplo, <span class="math inline">(80\%, 20\%)</span>, <span class="math inline">(99\%, 1\%)</span>, a depender da quantidade de observações (tamanho do conjunto de dados).</p>
<p><br></p>
<p>Portanto, utilizar o EQM sob o conjunto de dados de teste para avaliar <span class="math inline">g_1({\bf x}), g_2({\bf x}), \cdots,</span>, é uma boa estratégia, uma vez que agora não teremos mais uma dependência no numerador do cálculo do EQM. Em notação matemática, poderíamos escrever como já apresentado anteriormente, em <a href="#/função-de-risco-3" class="quarto-xref">Equação&nbsp;1</a>, i.e.,</p>
<p><span class="math display">\frac{1}{m}\sum_{i = 1}^m (Y_{n + i} - g(X_{n + i}))^2 \approx R(g) := \mathbb{E}\left[(Y - g({\bf X}))^2\right].</span></p>
<p><br></p>
<p>Esse resultado valeria para qualquer outra função de perda.</p>
</section>
<section id="data-splitting-3" class="slide level2">
<h2>Data Splitting</h2>
<p>Reescrevendo, suponha que o conjunto de dados total possua <span class="math inline">n</span> observações e que separamos aleatoriamente <span class="math inline">s &lt; n</span> observações para o conjunto de treinamento. Assim, temos, algo como:</p>
<p><br></p>
<p><span class="math display">\overbrace{(X_1, Y_1), (X_2, Y_2), \cdots, (X_s, Y_s)}^{70\%}, \,\,\, \overbrace{(X_{s + 1}, Y_{s + 1}), (X_{s + 2}, Y_{s + 2}), \cdots, (X_n, Y_n)}^{30\%}.</span></p>
<p><br></p>
<p>Então, temos que uma boa estimativa de <span class="math inline">R(g)</span> é dada pelo EQM calculado sobre o conjunto de dados de teste, que nesse caso considerei o conjunto com <span class="math inline">30\%</span>, mas esse percentual poderia ser outro. Então, temos que um bom estimador é:</p>
<p><span class="math display">\frac{1}{n - s}\sum_{i = s + 1}^n (Y_{i} - g(X_{i}))^2 \approx R(g) := \mathbb{E}\left[(Y - g({\bf X}))^2\right].</span></p>
</section>
<section id="data-splitting-4" class="slide level2">
<h2>Data Splitting</h2>
<p><br></p>
<p><strong>Agora você entende por que dividimos os dados em treinamento e teste?</strong></p>
<p><br></p>

<img data-src="gifs/yes.gif" style="width:25.0%" class="r-stretch"><p><br></p>
<p>Dividimos para obermos um bom estimador do risco utilizando o <a href="https://en.wikipedia.org/wiki/Mean_squared_error">EQM</a>. 🎁</p>
</section>
<section id="data-splitting-5" class="slide level2">
<h2>Data Splitting</h2>
<p><br></p>
<p>Podemos argumentar que o procedimento de <em>data splitting</em>, em que dividimos o nosso conjunto de dados em treinamento e teste fará com que venhamos perder muitas observações que poderiam ter sido utilizadas para treinar o modelo. E de certa forma isso é verdade, principalmente quando termos um conjunto não muito grande de observações.</p>
<p><br></p>
<p>Portanto, uma melhor abordagem, sendo esta uma variação do método de <em>data splitting</em> é o procedimento de <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)"><em>cross-validation - cv (validação cruzada)</em></a>. Uma versão mais geral de uma validação cruzada é o <span class="red"><em>leave-one-out cross-validation</em></span>.</p>
<p><br></p>
<p>Em palavras, o procedimento consiste em tirar de fora uma única observação das <span class="math inline">n</span> observações da base de dados para ser o nosso conjunto de teste e treinar o modelo com as observações que permaneceram. Daí, calcula-se o <strong>risco observado</strong> (EQM, sob o conjunto de teste/validação). Na segunda iteração, a observação que antes era de teste volta para perterncer ao conjunto de treinamento e uma nova observação é removida para ser teste. Esse procedimento ocorre de forma iterativa até a retirada da última observação como teste.</p>
</section>
<section id="leave-one-out-cross-validation" class="slide level2">
<h2>Leave-one-out cross-validation</h2>
<p><br></p>
<p>Observe a animação abaixo que ilustra o procedimento de <strong>l</strong>eave-<strong>o</strong>ne-<strong>o</strong>ut <strong>c</strong>ross-<strong>v</strong>alidation - LOOCV, em uma amostra de tamanho <span class="math inline">n = 8</span>. Ao fim, teremos <span class="math inline">n</span> modelos ajustados, em que calculamos as suas respectivas performances, i.e., com o risco observado, estimamos o risco de <span class="math inline">R(g)</span>.</p>
<p><br></p>

<img data-src="gifs/LOOCV.gif" class="r-stretch"></section>
<section id="leave-one-out-cross-validation-1" class="slide level2">
<h2>Leave-one-out cross-validation</h2>
<p><br></p>
<p>Vejo muitas pessoas que usam uma validação cruzada, por exemplo, <em>leave-one-out cross-validation</em> - LOOCV comparando com o método Jackknife e algumas inclusive dizendo ser a mesma coisa. Não, não são!</p>
<p><br></p>
<p>O algoritmo Jackknife é um procedimento de estimação, e que, por sua vez, deve estar dentro do conjunto de treinamento. Para haver algum Jackknife, a estimativa com <span class="math inline">n-1</span> observações deve estar dentro do conjunto de treinamento, em que dentro do treinamento teria a remoção de um observação por vez. <strong>Consegue perceber a diferença sutil?</strong></p>
<p><br></p>

<img data-src="gifs/bean_01.gif" style="width:20.0%" class="r-stretch"></section>
<section id="leave-one-out-cross-validation-2" class="slide level2">
<h2>Leave-one-out cross-validation</h2>
<p><br></p>
<p>O método LOOCV foi proposto por Stones (1974), no artigo intitulado Cross-Validatory Choice and Assessment of Statistical Predictions, no Royal Statistical Society, Série B. Clique <a href="https://www.jstor.org/stable/pdf/2984809.pdf?refreqid=excelsior%3A3071b86b3588905b095d44668025b005&amp;ab_segments=&amp;origin=&amp;initiator=&amp;acceptTC=1">aqui</a> se tiver curiosidade em ler o artigo.</p>
<p><br></p>
<p>Escrevendo o estimador do risco em um procedimento de LOOCV, temos que:</p>
<p><span class="math display">\widehat{R}(g) = \frac{1}{n}\sum_{i = 1}^n (Y_i - g_{-i}({\bf X}_i))^2,</span> em que <span class="math inline">g_{-i}(\bf{X}_i)</span>, representa o ajuste do modelo no conjunto de dados sem a <span class="math inline">i</span>-ésima observação.</p>
<p><br></p>
<p>Não é difícil perceber que a depender do valor de <span class="math inline">n</span>, o método LOOCV é <strong>computacionalmente intensivo</strong>. O método requer que ajustemos <span class="math inline">n</span> modelos. Em algumas situações isso não é um grande problema, porém, em diversas outras pode ser impeditivo utilizar o LOOCV. 🤯</p>
</section>
<section id="leave-one-out-cross-validation-3" class="slide level2">
<h2>Leave-one-out cross-validation</h2>
<p><br></p>
<p>O método LOOCV converge assintotivamente para o AIC, porém, este, muitas vezes não poderemos calcular diretamente, uma vez que não conhecemos a distribuição conjunta dos dados, i.e., não conhecemos a estrutura probabilística.</p>
<p><br></p>

<img data-src="gifs/interesting-batman.gif" class="r-stretch"></section>
<section id="leave-one-out-cross-validation-4" class="slide level2">
<h2>Leave-one-out cross-validation</h2>
<p><br></p>
<p>Essa relação entre o LOOCV e o <em>Akaike Information Criterion</em> - AIC foi provada no paper Stone (1977) intitulado <strong>An Asymptotic Equivalence of Choice of Model by Cross-Validation and Akaike’s Criterion</strong> e publicado no <em>Journal of the Royal Statistical Society, Series B</em>. Clique <a href="https://iri.columbia.edu/~tippett/cv_papers/Stone1977.pdf">aqui</a>, se quiser ler o artigo.</p>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="imgs/loocn_aic.png"></p>
</div><div class="column" style="width:50%;">
<p><br> <br> <img data-src="gifs/thumbs-up-nod.gif"></p>
</div>
</div>
</section>
<section id="k-fold-cross-validation" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p><br></p>
<p>Uma alternativa ao LOOCV é utilizar o método <span class="math inline">k</span>-<em>fold cross-validation</em>. Nessa abordagem, dividimos o conjunto de dados em <span class="math inline">k</span>-<em>folds</em> (lotes) disjuntos e com aproximadamente o mesmo tamanho. Dessa forma, temos <span class="math inline">L_1, \cdots, L_k \subset \{1, \cdots, n\}</span> são, cada um, um conjunto de índices aleatórios associados a cada um dos lotes. A ideia aqui é construir <span class="math inline">k</span> estimadores da função de regressão, denotados por <span class="math inline">\widehat{g}_{-1}, \cdots, \widehat{g}_{-k}</span>, em que <span class="math inline">\widehat{g}_{-j}</span> é criado usando todas as observações do banco de dados, com exceção daquelas do lote <span class="math inline">L_j</span>, utilizado para <strong>validação</strong>. O estimador do risco é dado por:</p>
<p><span class="math display">\widehat{R}(g) = \frac{1}{n}\sum_{j=1}^k \sum_{i \in L_j}(Y_i - g_{-j}({\bf X}_i))^2.</span> Perceba que, que o LOOCV é um caso particular do <span class="math inline">k</span>-<em>fold cross-validation</em>, quando fazemos <span class="math inline">k = n</span>. Em outras palavras, <span class="math inline">L_1, \cdots, L_k \subset \{1, \cdots, n\}</span> representam os índices aleatórios do conjunto de treinamento nos <span class="math inline">k</span> lotes.</p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="k-fold-cross-validation-1" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p>A animação abaixo, ilustra o procedimento de <span class="math inline">3</span>-<em>fold cross-validation</em> (<span class="math inline">k = 3</span>), para uma amostra de tamanho <span class="math inline">n = 12</span> observações. Note que os valores que pertencem a cada um dos lotes são aleatórios. Portanto, o procedimento LOOCV é deterministico, já o procedimento de <span class="math inline">k</span>-<em>fold cross-validation</em> é randomizado.</p>
<p><br></p>

<img data-src="gifs/KfoldCV.gif" class="r-stretch"><p>Perceba que teremos agora apenas <span class="math inline">3</span> modelos. Para cada um desses lotes, calulamos o EQM com o conjunto de teste (parte <span class="trueblue">azul</span>) e treinamos o modelo com o conjunto de treinamento (parte <span class="truered">vermelha</span>).</p>
</section>
<section id="k-fold-cross-validation-2" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p><br></p>
<p>Muitos modelos mais sofisticados apresentam hiperparâmetros (parâmetros de sintonização) que não dependem dos dados. É muito comum os algoritmos de aprendizagem de máquina se utilizarem do procedimento de validação cruzada, para além da estimação do risco <span class="math inline">R(g)</span> no conjunto de validação.</p>
<p><br></p>
<p>Ao estimar <span class="math inline">k</span> modelos, normalmente faz-se um <em>grid</em> de possíveis valores para esses hiperparâmetros em que ao final, escolhe-se como hiperparâmetro o modelo com menor EQM. Por fim, ajusta-se um modelo final, com todo o conjunto de treinamento usando o valor do hiperparâmetro que retornou o menor EQM no conjunto de validação.</p>
<p><br></p>
<p>Alias, utilizamos um procedimento de validação para selecionar a melhor combinação de hiperparâmetros e é será risco preditivo observado sob o conjunto de teste que realmente irá nos fornecer uma estimativa válida do risco preditivo <span class="math inline">R(g)</span>.</p>
<p><br></p>

<img data-src="gifs/ok.gif" class="r-stretch"></section>
<section id="k-fold-cross-validation-3" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p>O termo <span class="red">validação</span> refere-se à parcela do conjunto de treinamento incial que dividimos em validação e treinamento, dentro de uma validação cruzada.</p>
<p><br></p>
<p>O conjunto <span class="red"><em>Testing</em></span> na segunda hierarquia da árvore ao lado, só usamos no final para avaliar o desempenho do modelo nesse conjunto. Isto é, usamos o <span class="red"><em>Testing</em></span> para o cálculo do <strong>risco observado</strong>.</p>
<p><br></p>
<p>Perceba que o conjunto de treinamento (<span class="red"><em>Not Testing</em></span>) é particionado em treinamento e validação. Poderíamos fazer uma única partição, mas o procedimento comumente utilizado é particionar entre <span class="red"><em>Training</em></span> e <span class="red"><em>Validation</em></span> uzando algum procedimento de validação cruzada.</p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/validation-split.svg" style="width:64.0%"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="k-fold-cross-validation-4" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p><br></p>
<p>Em alguns livros o conjunto de treinamento é denominado de <em>not-testing</em> (não-teste). Isso, porquê eles querem enfatizar o fato de que o conjunto <em>not-testing</em> é utilizado para treinar/ensinar o modelo e jamais deverá ser utilizado para avaliar o risco preditivo <span class="math inline">R(g)</span>.</p>
<p><br></p>
<p>Ao usar essa terminologia, os autores dos livros tentam enfatizar que o conjunto de treinamento é usado exclusivamente para ensinar o modelo a aprender os padrões nos dados, enquanto o conjunto de teste é usado para medir quão bem o modelo generaliza esses padrões para dados não vistos anteriormente.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="k-fold-cross-validation-5" class="slide level2">
<h2><span class="math inline">k</span>-fold cross-validation</h2>
<p>A imagem abaixo ilustra o procedimento <span class="math inline">k</span>-<em>fold cross-validation</em>, em que uma <span class="math inline">5</span>-<em>fold cross-validation</em> é realizada dentro do conjunto de treinamento. Em cada <em>split</em>, o conjunto verde de observações (fold <span class="green">verde</span>) são utilizados para treinar/ajustar o modelo e o conjunto <span class="trueblue">azul</span>, em cada um dos <em>splits</em> é utilizado para avaliar o risco preditivo <span class="math inline">R(g)</span> (através, por exemplo do EQM).</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="imgs/grid_search_cross_validation.png" width="800"> <img data-src="gifs/hum.gif"></p>
</div><div class="column" style="width:60%;">
<p>Não confunda os folds azuis com o conjunto de teste (<span class="trueblue">Test data</span>), este último utilizado por fim, depois do modelo pronto, para avaliar o desempenho do modelo treinado.</p>
<p>Note também que a validação cruzada também é utilizada para o ajuste de hiperparâmetros, que são parâmetros de sintonização que não dependem dos dados para serem equalizados. Por exemplo, em uma regressão lasso, que veremos adiante, há o hiperparâmetro <span class="math inline">\lambda</span> que precisamos obter, normalmente por meio de um <span class="red"><em>grid search</em></span> (sequência finita), por exemplo, <span class="math inline">\lambda \in [0.5, 1, 1.5, 2, 2.5]</span> de possíveis valores. Cada <em>split</em> pode ser utilizado para avaliar um valor de <span class="math inline">\lambda</span>, dos possíveis valores dispostos no grid. Aumentaríamos a quantidade de splits para mais valores de <span class="math inline">\lambda</span> na sequência.</p>
</div>
</div>
</section>
<section id="resumindo-data-splitting-validação-cruzada-e-conjunto-de-validação" class="slide level2">
<h2>Resumindo: data splitting, validação cruzada e conjunto de validação</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="imgs/data_split_validation_cross.png"></p>
</div><div class="column" style="width:50%;">
<p>O simples procedimento de dividir o conjunto de dados em dois, uma parte para treinar o modelo e a outra parte (conjunto de teste) para estimar o risco <span class="math inline">R(g)</span> é denominado de <span class="red"><em>data splitting</em></span> ou <span class="red"><em>hold-out method</em></span>. É um procedimento mais simples, porém, pode não ser útil em conjunto de dados não muito grandes.</p>
<p>A segunda linha da ilustração, demonstra o procedimento de <em>cross-validation</em> (validação cruzada), procedimento mais utilizado nos treinamentos de modelos de aprendizagem de máquina.</p>
<p>A terceira linha é uma abordagem também utilizada, porém não tão interessante quanto a validação cruzada. Nessa abordagem o banco de dados é dividido aleatoriamente em três partes. Treina-se o modelo com a parte <span class="green">verde</span>, estima-se o risco com o conjunto de validação amarelo e testa-se o modelo com o conjunto de teste.</p>
</div>
</div>
</section>
<section id="resumindo-data-splitting-validação-cruzada-e-conjunto-de-validação-1" class="slide level2">
<h2>Resumindo: data splitting, validação cruzada e conjunto de validação</h2>
<p><br> A abordagem do conjunto de validação envolve <strong>dividir o conjunto de treinamento em duas partes</strong>: uma parte é usada para treinar o modelo e a outra parte é usada para avaliar o desempenho do modelo para uma dada combinação. de hiperparâmetros. O conjunto de validação é utilizado para avaliar os hiperparâmetros do modelo, como a taxa de aprendizado, o número de camadas ocultas em uma rede neural, entre outros. Após o ajuste dos hiperparâmetros, o modelo final é treinado com o conjunto de treinamento completo e avaliado em um conjunto separado chamado conjunto de teste. Essa abordagem é conhecida como divisão simples de treinamento/validação/teste.</p>
<p><br></p>
<p>Por outro lado, a validação cruzada <span class="math inline">k</span>-<em>fold</em> é uma abordagem que visa obter uma estimativa mais robusta do desempenho do modelo. Nessa abordagem, o conjunto de treinamento é dividido em <span class="math inline">k</span> subconjuntos (<em>folds</em>) de tamanho aproximadamente igual. O modelo é treinado <span class="math inline">k</span> vezes, cada vez usando <span class="math inline">k-1</span> <em>folds</em> como conjunto de treinamento e <span class="math inline">1</span> <em>fold</em> como conjunto de validação. O desempenho do modelo é então calculado como a média dos resultados obtidos em cada iteração. Isso permite avaliar o modelo, em diferentes combinações dos hiperparâmetros, de forma mais precisa, pois utiliza todos os dados para treinamento e validação, evitando a dependência de uma única divisão do conjunto de treinamento.</p>
</section>
<section id="resumindo-data-splitting-validação-cruzada-e-conjunto-de-validação-2" class="slide level2">
<h2>Resumindo: data splitting, validação cruzada e conjunto de validação</h2>
<p><br></p>
<p>A validação cruzada <span class="math inline">k</span>-<em>fold</em> é particularmente útil quando o conjunto de dados é limitado, pois aproveita ao máximo os dados disponíveis. Além disso, <strong>ela permite verificar se o modelo é estável e se seu desempenho varia significativamente com diferentes divisões dos dados</strong>. É importante ressaltar que a validação cruzada <span class="math inline">k</span>-<em>fold</em> pode ser computacionalmente mais cara do que a abordagem do conjunto de validação, uma vez que envolve treinar e avaliar o modelo várias vezes.</p>
<p><br></p>

<img data-src="gifs/chapulin-colorado-no.gif" style="width:40.0%" class="r-stretch"></section>
<section id="resumindo-data-splitting-validação-cruzada-e-conjunto-de-validação-3" class="slide level2">
<h2>Resumindo: data splitting, validação cruzada e conjunto de validação</h2>
<p><br></p>
<p>Um outro detalhe que muitas vezes não é falado é que apesar de temos duas tarefas de estimação, uma envolvendo o conjunto de <strong>treinamento</strong>, em que treinamos o modelo e outra envolvendo o conjunto de <strong>teste</strong>, em que queremos estimar o risco <span class="math inline">R(g)</span>, de modo a poder selecionar o melhor modelo, a segunda tarefa é bem mais fácil. É por isso que o conjunto de treinamento tende a ser menor que o conjunto de teste.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="resumindo-data-splitting-validação-cruzada-e-conjunto-de-validação-4" class="slide level2">
<h2>Resumindo: data splitting, validação cruzada e conjunto de validação</h2>
<p><br></p>
<p>No <a href="https://www.tidymodels.org/">tidymodels</a>, utilizamos a biblioteca <a href="https://rsample.tidymodels.org/">rsample</a> para realizar procedimentos de validação cruzada:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p><code>initial_split()</code>: útil para uma divisão inicial dos dados, i.e., para aplicação do <em>hold-out</em>;</p></li>
<li class="fragment"><p><code>loo_cv()</code>: se desejar realizar um procedimento de <em>leave-one-out cross-validation</em>;</p></li>
<li class="fragment"><p><code>vfold_cv()</code>: para um procedimento de <span class="math inline">k</span>-<em>folds cross-validation</em>. Podemos inclusive realizar várias repetições de validação cruzada o que poderá remelhorar ainda mais a seleção da melhor combinação de hiperparâmetros. O número de repetições de validação cruzada poderá ser especificado no argumento <code>repeats</code> que por padrão é <code>1L</code>;</p></li>
<li class="fragment"><p><code>bootstraps()</code>: se for desejado utilizar um procedimento de <em>bootstrap</em> não-paramétrico ao invés de uma validação cruzada. O papel do <em>bootstrap</em> é o mesmo da validação cruzada, porém, a seleção das amostras de treinamento é de mesmo tamanho da amostra de treinamento original e o procedimento é feito com reposição, i.e., os mesmos dados podem aparecer nas amostras. O conjunto de avaliação é definido como as linhas dos dados originais que não foram incluídos na amostra bootstrap. Isso geralmente é chamado de amostra <span class="red"><em>out-of-bag</em></span> - OOB. Podemos alterar o número de amostras <em>bootstrap</em> modificando o argumento <code>times</code> que por padrão é <code>25L</code>.</p></li>
</ol>
<p><br></p>
<p><strong>Lembre-se que qualquer procedimento de validação cruzada que você escolher deve ser realizado no conjunto de treinamento</strong>! 📌</p>
</section>
<section id="balanço-viés-e-variância" class="slide level2">
<h2>⚖️ Balanço viés e variância</h2>
<p><br></p>
<p>A ideia de precisão e exatidão estão ligadas ao viés e variância do modelo <span class="math inline">g</span>, em que precisão está ligado a ideia de variância pequena e exatidão está ligada a ideia de baixo viés. A ideia é termos um estimador próximo o que ilustra o item <span class="red">d</span>. Muitas vezes temos um estimador nas situações <span class="red">b</span> e <span class="red">c</span>. O ideal é o balanço de viés e variância, que seria o estimador ilustrado pelo item <span class="red">d</span>.</p>

<img data-src="imgs/precisao_exatidao.png" style="width:55.0%" class="r-stretch quarto-figure-center"></section>
<section id="balanço-viés-e-variância-1" class="slide level2">
<h2>⚖️ Balanço viés e variância</h2>
<p><br></p>
<p>Um grande apelo para o uso do risco quadrático, i.e., risco que utiliza a função de perda <span class="math inline">L_2</span> é sua interpretabilidade. Temos que o risco quadrático <span class="math inline">R(g)</span> condicional a um novo <span class="math inline">{\bf x}</span> poderá ser decomposto por:</p>
<p><span id="eq-decomposicao-risco-l2"><span class="math display">\mathbb{E}\left[(Y - \widehat{g}({\bf X}))^2| {\bf X} = {\bf x}\right] = \underbrace{\mathbb{V}[Y | {\bf X = x}]}_{\mathrm{i - Variância\,\, intrínseca}} + \overbrace{(r({\bf x}) - \mathbb{E}[\widehat{g}({\bf x})])^2}^{\mathrm{ii - Viés\, ao\, quadrado\, do\, modelo}} + \underbrace{\mathbb{V}[\widehat{g}({\bf x})]}_{\mathrm{iii - Variância\, do\, modelo}}. \tag{2}</span></span> <strong>Temos que</strong>:</p>
<p><br></p>
<p>i - É a variância intrínseca da vairável resposta (<em>label</em>), que não depende da função <span class="math inline">\widehat{g}</span> escolhida e, assim, não poderá ser reduzida. Na verdade, poderemos reduzir <span class="math inline">i</span>, se incluirmos mais <em>features</em> (covariáveis/variáveis explicativas) ao nosso modelo;</p>
<p>ii - É o viés ao quadrado do estimador <span class="math inline">\widehat{g}</span> (viés ao quadrado do modelo);</p>
<p>iii - É a variância do estimador <span class="math inline">\widehat{g}</span>.</p>
</section>
<section id="balanço-viés-e-variância-2" class="slide level2">
<h2>⚖️ Balanço viés e variância</h2>
<p><br></p>
<p>Assim, lembre-se que uma escolha adequada de <span class="math inline">\widehat{g}</span> nos garante que conseguiremos reduzir o risco preditivo <span class="math inline">R(g)</span>, pois a escolha apropriada implica em escolhermos um estimador de <span class="math inline">\widehat{g}</span> com balanço entre víes e variância.</p>
<p><br></p>
<p>Modelos com muitos parâmetros possuem viés relativamente baixo, porém, tendem a ter variância muito alta, em geral, uma vez que precisamos estimar muitos parâmetros. Já modelos com poucos parâmetros, normalmente tendem a ter variância baixa, acompanhados normalmente de um alto viés.</p>
<p><br></p>
<p>Geralmente, modelos com muitos parâmetros nos levam a termos <span class="red"><em>overffiting</em></span> (super-ajuste), o que não é bom pois são acompanhados de alta variância. Já modelos muito simplistas nos conduzem à um ajuste muito ruim (<span class="red"><em>underffiting</em></span> ou sub-ajuste). Entendeu!?</p>

<img data-src="gifs/mais_ou_menos.gif" class="r-stretch"></section>
<section id="balanço-viés-e-variância-3" class="slide level2">
<h2>⚖️ Balanço viés e variância</h2>
<p><br></p>
<div class="cell">
<details>
<summary>Estude o código</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="fu">library</span>(tibble)</span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb7-5"><a href="#cb7-5"></a></span>
<span id="cb7-6"><a href="#cb7-6"></a><span class="co"># Função de regressão verdadeira. Na prática é desconhecida.</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>regressao_verdadeira <span class="ot">&lt;-</span> <span class="cf">function</span>(x)</span>
<span id="cb7-8"><a href="#cb7-8"></a>  <span class="dv">45</span> <span class="sc">*</span> <span class="fu">tanh</span>(x<span class="sc">/</span><span class="fl">1.9</span> <span class="sc">-</span> <span class="dv">7</span>) <span class="sc">+</span> <span class="dv">57</span></span>
<span id="cb7-9"><a href="#cb7-9"></a></span>
<span id="cb7-10"><a href="#cb7-10"></a>observacoes_regressao_real <span class="ot">&lt;-</span> <span class="cf">function</span>(n, <span class="at">desvio_padrao =</span> <span class="fl">0.2</span>) {</span>
<span id="cb7-11"><a href="#cb7-11"></a>  <span class="co"># Permitindo que o mesmo x possa ter dois pontos de y, como ocorre na </span></span>
<span id="cb7-12"><a href="#cb7-12"></a>  <span class="co"># pratica</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>  seq_x <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">17.5</span>, <span class="at">length.out =</span> n), <span class="at">size =</span> n, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb7-14"><a href="#cb7-14"></a>  </span>
<span id="cb7-15"><a href="#cb7-15"></a>  step <span class="ot">&lt;-</span> <span class="cf">function</span>(x)</span>
<span id="cb7-16"><a href="#cb7-16"></a>    <span class="fu">regressao_verdadeira</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> 1L, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> desvio_padrao)</span>
<span id="cb7-17"><a href="#cb7-17"></a>  </span>
<span id="cb7-18"><a href="#cb7-18"></a>  tibble<span class="sc">::</span><span class="fu">tibble</span>(<span class="at">y =</span> purrr<span class="sc">::</span><span class="fu">map_vec</span>(<span class="at">.x =</span> seq_x, <span class="at">.f =</span> step), <span class="at">x =</span> seq_x)</span>
<span id="cb7-19"><a href="#cb7-19"></a>}</span>
<span id="cb7-20"><a href="#cb7-20"></a></span>
<span id="cb7-21"><a href="#cb7-21"></a><span class="co"># Usaremos uma regressão polinomial para tentar ajustar à regressão -------</span></span>
<span id="cb7-22"><a href="#cb7-22"></a>regressao_polinomial <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">n =</span> 30L, <span class="at">desvio_padrao =</span> <span class="dv">4</span>, <span class="at">grau =</span> 1L) {</span>
<span id="cb7-23"><a href="#cb7-23"></a>  </span>
<span id="cb7-24"><a href="#cb7-24"></a>  dados <span class="ot">&lt;-</span> <span class="fu">observacoes_regressao_real</span>(<span class="at">n =</span> n, <span class="at">desvio_padrao =</span> desvio_padrao)</span>
<span id="cb7-25"><a href="#cb7-25"></a>    </span>
<span id="cb7-26"><a href="#cb7-26"></a>  iteracoes <span class="ot">&lt;-</span> <span class="cf">function</span>(tibble_data, grau) {</span>
<span id="cb7-27"><a href="#cb7-27"></a>      x <span class="ot">&lt;-</span> tibble_data<span class="sc">$</span>x</span>
<span id="cb7-28"><a href="#cb7-28"></a>      iteracoes <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="at">X =</span> 2L<span class="sc">:</span>grau, <span class="at">FUN =</span> <span class="cf">function</span>(i) x<span class="sc">^</span>i)</span>
<span id="cb7-29"><a href="#cb7-29"></a>      </span>
<span id="cb7-30"><a href="#cb7-30"></a>      result <span class="ot">&lt;-</span> <span class="fu">cbind</span>(tibble_data, <span class="fu">do.call</span>(cbind, iteracoes))</span>
<span id="cb7-31"><a href="#cb7-31"></a>      <span class="fu">colnames</span>(result)[(<span class="fu">ncol</span>(tibble_data) <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span><span class="fu">ncol</span>(result)] <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">"x"</span>, 2L<span class="sc">:</span>grau)</span>
<span id="cb7-32"><a href="#cb7-32"></a>      </span>
<span id="cb7-33"><a href="#cb7-33"></a>      <span class="fu">as_tibble</span>(result)</span>
<span id="cb7-34"><a href="#cb7-34"></a>  }  </span>
<span id="cb7-35"><a href="#cb7-35"></a>  </span>
<span id="cb7-36"><a href="#cb7-36"></a>  <span class="cf">if</span>(grau <span class="sc">&gt;=</span> 2L)</span>
<span id="cb7-37"><a href="#cb7-37"></a>    dados <span class="ot">&lt;-</span> <span class="fu">iteracoes</span>(dados, <span class="at">grau =</span> grau)</span>
<span id="cb7-38"><a href="#cb7-38"></a>  </span>
<span id="cb7-39"><a href="#cb7-39"></a>  ajuste <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> y <span class="sc">~</span> ., <span class="at">data =</span> dados)</span>
<span id="cb7-40"><a href="#cb7-40"></a>  dados<span class="sc">$</span>y_chapeu <span class="ot">&lt;-</span> <span class="fu">predict</span>(ajuste, <span class="at">new.data =</span> dados)</span>
<span id="cb7-41"><a href="#cb7-41"></a>  </span>
<span id="cb7-42"><a href="#cb7-42"></a>  dados <span class="sc">|&gt;</span> </span>
<span id="cb7-43"><a href="#cb7-43"></a>    dplyr<span class="sc">::</span><span class="fu">relocate</span>(y_chapeu, <span class="at">.before =</span> x)</span>
<span id="cb7-44"><a href="#cb7-44"></a>}</span>
<span id="cb7-45"><a href="#cb7-45"></a></span>
<span id="cb7-46"><a href="#cb7-46"></a>plotando <span class="ot">&lt;-</span> <span class="cf">function</span>(dados){</span>
<span id="cb7-47"><a href="#cb7-47"></a>  dados <span class="sc">|&gt;</span>  </span>
<span id="cb7-48"><a href="#cb7-48"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y_chapeu)) <span class="sc">+</span></span>
<span id="cb7-49"><a href="#cb7-49"></a>    <span class="fu">geom_point</span>()</span>
<span id="cb7-50"><a href="#cb7-50"></a>}</span>
<span id="cb7-51"><a href="#cb7-51"></a></span>
<span id="cb7-52"><a href="#cb7-52"></a>mc_ajustes <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">mc =</span> 100L, <span class="at">n =</span> 50L, <span class="at">desvio_padrao =</span> <span class="dv">5</span>, <span class="at">grau =</span> 1L){</span>
<span id="cb7-53"><a href="#cb7-53"></a></span>
<span id="cb7-54"><a href="#cb7-54"></a>  p <span class="ot">&lt;-</span> </span>
<span id="cb7-55"><a href="#cb7-55"></a>    <span class="fu">ggplot</span>(<span class="at">data =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb7-56"><a href="#cb7-56"></a>      <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">17.5</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">110</span>)) <span class="sc">+</span>      </span>
<span id="cb7-57"><a href="#cb7-57"></a>      <span class="fu">ylab</span>(<span class="st">"Valores estimados"</span>)</span>
<span id="cb7-58"><a href="#cb7-58"></a>  </span>
<span id="cb7-59"><a href="#cb7-59"></a>  df <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb7-60"><a href="#cb7-60"></a>  <span class="cf">for</span>(i <span class="cf">in</span> 1L<span class="sc">:</span>mc){</span>
<span id="cb7-61"><a href="#cb7-61"></a>    df <span class="ot">&lt;-</span> <span class="fu">regressao_polinomial</span>(<span class="at">n =</span> n, <span class="at">desvio_padrao =</span> desvio_padrao, <span class="at">grau =</span> grau)</span>
<span id="cb7-62"><a href="#cb7-62"></a>    p <span class="ot">&lt;-</span> p <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y_chapeu))</span>
<span id="cb7-63"><a href="#cb7-63"></a>  }</span>
<span id="cb7-64"><a href="#cb7-64"></a>  p <span class="sc">+</span> </span>
<span id="cb7-65"><a href="#cb7-65"></a>    <span class="fu">stat_function</span>(<span class="at">fun =</span> regressao_verdadeira, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">size=</span> <span class="fl">1.4</span>) <span class="sc">+</span></span>
<span id="cb7-66"><a href="#cb7-66"></a>    <span class="fu">labs</span>(</span>
<span id="cb7-67"><a href="#cb7-67"></a>      <span class="at">title =</span> <span class="st">"Regressão Polinomial"</span>,</span>
<span id="cb7-68"><a href="#cb7-68"></a>      <span class="at">subtitle =</span> <span class="fu">paste</span>(<span class="st">"Grau: "</span>, grau)</span>
<span id="cb7-69"><a href="#cb7-69"></a>    ) <span class="sc">+</span></span>
<span id="cb7-70"><a href="#cb7-70"></a>    <span class="fu">theme</span>(</span>
<span id="cb7-71"><a href="#cb7-71"></a>      <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>),</span>
<span id="cb7-72"><a href="#cb7-72"></a>      <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb7-73"><a href="#cb7-73"></a>    )</span>
<span id="cb7-74"><a href="#cb7-74"></a>}</span>
<span id="cb7-75"><a href="#cb7-75"></a></span>
<span id="cb7-76"><a href="#cb7-76"></a><span class="co"># Fixando uma semente</span></span>
<span id="cb7-77"><a href="#cb7-77"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb7-78"><a href="#cb7-78"></a></span>
<span id="cb7-79"><a href="#cb7-79"></a>p1 <span class="ot">&lt;-</span> <span class="fu">mc_ajustes</span>(<span class="at">grau =</span> <span class="dv">1</span>, <span class="at">n =</span> <span class="dv">100</span>, <span class="at">desvio_padrao =</span> <span class="dv">10</span>)</span>
<span id="cb7-80"><a href="#cb7-80"></a>p2 <span class="ot">&lt;-</span> <span class="fu">mc_ajustes</span>(<span class="at">grau =</span> <span class="dv">7</span>, <span class="at">n =</span> <span class="dv">100</span>, <span class="at">desvio_padrao =</span> <span class="dv">10</span>)</span>
<span id="cb7-81"><a href="#cb7-81"></a>p3 <span class="ot">&lt;-</span> <span class="fu">mc_ajustes</span>(<span class="at">grau =</span> <span class="dv">70</span>, <span class="at">n =</span> <span class="dv">100</span>, <span class="at">desvio_padrao =</span> <span class="dv">10</span>)</span>
<span id="cb7-82"><a href="#cb7-82"></a>p4 <span class="ot">&lt;-</span> <span class="fu">mc_ajustes</span>(<span class="at">grau =</span> <span class="dv">200</span>, <span class="at">n =</span> <span class="dv">100</span>, <span class="at">desvio_padrao =</span> <span class="dv">10</span>)</span>
<span id="cb7-83"><a href="#cb7-83"></a></span>
<span id="cb7-84"><a href="#cb7-84"></a>p <span class="ot">&lt;-</span> ((p1 <span class="sc">|</span> p2) <span class="sc">/</span> (p3 <span class="sc">|</span> p4)) <span class="sc">+</span> <span class="fu">plot_annotation</span>(<span class="at">tag_levels =</span> <span class="st">"A"</span>)</span>
<span id="cb7-85"><a href="#cb7-85"></a></span>
<span id="cb7-86"><a href="#cb7-86"></a><span class="fu">ggsave</span>(p, <span class="at">file =</span> <span class="st">"imgs/vies_variancia.png"</span>, <span class="at">device =</span> <span class="st">"png"</span>, <span class="at">width =</span> <span class="dv">40</span>, <span class="at">height =</span> <span class="dv">30</span>, <span class="at">units =</span> <span class="st">"cm"</span>)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>

<img data-src="imgs/vies_variancia.png" style="width:60.0%" class="r-stretch quarto-figure-center"></section>
<section id="balanço-viés-e-variância-4" class="slide level2">
<h2>⚖️ Balanço viés e variância</h2>
<p><br></p>
<p>Experimente de forma interativa altera a complefixade do modelo.</p>
<p><br></p>
 <iframe id="example1" src="https://pedro-rafael.shinyapps.io/shiny_apps/" style="border: none; width: 100%; height: 90%" frameborder="0"></iframe>
<p><br></p>
<p>Para ampliar a aplicação, clique <a href="https://pedro-rafael.shinyapps.io/shiny_apps/">aqui</a>.</p>
</section>
<section id="tuning-parameters" class="slide level2">
<h2>🎛 Tuning Parameters</h2>
<p><br></p>
<p>No exemplo anterior, note que os parâmetros dos modelos são os coeficientes <span class="math inline">\beta</span>’s que indexam a regressão polinomial. Porém, perceba que á um <strong>parâmetro de sintonização</strong> (<span class="red"><em>tuning parameter</em></span>) que é o valor de <span class="math inline">p</span>, isto é, qual o grau do polinômio que iremos utilizar.</p>
<p><br></p>
<p>Normalmente a escolha é feita realizando um <span class="red"><em>grid search</em></span> por meio de um <span class="red"><em>cross-validation</em></span>.</p>
<p><br></p>
<p>No exemplo anterior, fizemos uma simulação e observamos que ao considerar graus nem muito grandes nem muito pequenos, aparentemente teremos escolhas razoáveis.</p>
<p><br></p>

<img data-src="gifs/interesting-batman.gif" class="r-stretch"></section>
<section id="tuning-parameters-1" class="slide level2">
<h2>🎛 Tuning Parameters</h2>
<p><br></p>
<p><span class="red">Exemplo</span>: Consedere os dados de expectativa de vida versus PIB per Capita, disponíveis <a href="https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData">aqui</a>. Selecione o melhor estimador <span class="math inline">g</span> da classe <span class="math inline">\mathbb{G}</span>, em que</p>
<p><span class="math display">\mathbb{G} = \left\{g(x)\,\,:\,\, \beta_0 + \sum_{i = 1}^p \beta_i x^i\,\, \text{para } p \in \{1, 2, \cdots,11\} \right\}.</span> Note que selecionar o melhor polinômio é uma busca em <span class="math inline">p</span>. Devemos utilizar o erro quadrático médio - EQM sob o conjunto de validação, uma vez que sabemos que apenas em um conjunto de validação ou em novas observações o estimador do risco pelo EQM é consistente, pela Lei dos Grandes Números.</p>
<p><br></p>
<p>Vamos utilizar a biblioteca <a href="https://rsample.tidymodels.org/index.html">rsample</a> para a tarefa de validação cruzada. Leia a documentação da biblioteca, em especial, a da função <code>vfold_cv</code>, responsável por construir a validação cruzada. Na verdade ela faz a divisão da base de dados em <span class="math inline">v</span> <em>splits</em> de tamanho aproximadamente iguais. Por padrão, <span class="math inline">v = 10</span>. Esse é o procedimento de <span class="math inline">k</span>-<em>fold cross-validation</em> que apresentamos aqui, em que <span class="math inline">v = k</span>.</p>
</section>
<section id="tuning-parameters-2" class="slide level2">
<h2>🎛 Tuning Parameters</h2>
<p><br></p>
<p>Algumas observações gerais a respetio da biblioteca <a href="https://rsample.tidymodels.org/index.html">rsample</a>, que são úteis para resolver esse problema:</p>

<img data-src="imgs/rsample.png" class="r-stretch quarto-figure-center"><ol type="1">
<li class="fragment">Para realizar uma <strong>primeira divisão</strong> do conjunto de dados (<em>data splitting/hold-out</em>), utiliza-se a função <code>initial_split</code>;</li>
<li class="fragment">Para acessar o conjunto de <strong>treinamento</strong> dos dados, usamos a função <code>training</code>;</li>
<li class="fragment">Para acessar o conjunto de <strong>teste</strong>, usamos a função <code>testing</code>;</li>
<li class="fragment">Para constuir todas as divisões da validação cruzada, entre treinamento e validação, no conjunto de treinamento inicial, usamos a função <code>vfold_cv</code> já mencionada;</li>
<li class="fragment">Para acessar o conjunto de <strong>treinamento</strong> de um <em>split</em> da validação cruzada, usamos a função <code>analysis</code>;</li>
<li class="fragment">Para acessar o conjunto de <strong>validação</strong>, utilizamos a função <code>assessment</code>.</li>
</ol>
</section>
<section id="tuning-parameters-3" class="slide level2">
<h2>🎛 Tuning Parameters</h2>

<img data-src="imgs/rsample.png" style="width:5.0%" class="r-stretch quarto-figure-center"><p><br> Note que realizar uma validação cruzada é importante para podemos selecionar o melhor polinômio, i.e., o melhor valor de <span class="math inline">p</span>. Caso venhamos negligenciar esse aspecto da análise, iremos cair na falácia de acreditarmos que quanto maior o grau do polinômio, maior será o poder preditivo do modelo. Isso não é verdade e você deverá selecionar o melhor modelo dentro de um esquema de validação cruzada.</p>
<p><br></p>
<p>No mundo de aprendizagem de máquina, muitos chamam o processo de encontrar o melhor hiperparâmetro de <span class="red">“tunagem”</span>. Em vários modelos, podemos ter mais de um.</p>
</section>
<section id="tuning-parameters-4" class="slide level2">
<h2>🎛 Tuning Parameters</h2>
<p><br></p>
<p>As Figuras abaixo, mostram a avaliação dos polinômios da classe <span class="math inline">\mathbb{G}</span>, usando o risco estimado <span class="math inline">\widehat{R}(g)</span> pelo erro quadrático médio - EQM. Porém, a Figura <span class="red">A</span> aprenseta a avaliação dos modelos, usando simplesmente o conjunto de treinamento e a Figura <span class="red">B</span> aprensenta a avaliação do grau do polinômio considerando uma validação cruzada dentro do conjunto de treinamento.</p>
<p><br> <img data-src="imgs/avaliacao_risco.png" data-fig-align="center"></p>
<p><br></p>
<p>A mensagem equivocada passada pela Figura <span class="red">A</span> é que supostamente aumentar a complexidade do modelo seria uma uma boa alternativa e nos conduziríamos à bons modelos preditivos. Mas sempre se lembre do equilíbrio que temos que ter entre viés e variância. A Figura <span class="red">B</span> mostra que um polinômio com grau próximo à <span class="math inline">p = 8</span> é a melhor alternativa.</p>
</section>
<section id="tuning-parameters-5" class="slide level2">
<h2>🎛 Tuning Parameters</h2>
<p><br></p>
<p>Observe o <em>dashboard</em> interativo! Sabemos que para que tenhamos uma boa estimativa do risco preditivo, devemos utilizar novas observações. No <em>dashboard</em>, é possível observar que a forma errada (usando o conjunto de treinamento para avaliar o risco), sugere que sempre será bom adicionar mais parâmetros ao modelo, levando a <em>overfitting</em>. Perceba que usando a forma correta (usando validação cruzada), o EQM (risco estimado) sugere que não podemos aumentar muito a quantidade de parâmetros.</p>
<p><br></p>
 <iframe id="example1" src="https://pedro-rafael.shinyapps.io/shiny_apps/#section-avalia%C3%A7%C3%A3o-do-risco-preditivo" style="border: none; width: 100%; height: 90%" frameborder="0"></iframe>
</section>
<section id="tuning-parameters-6" class="slide level2">
<h2>🎛 Tuning Parameters</h2>
<p><br></p>
<p>Abaixo você poderá acessar o código que soluciona o problema. O parâmetro <code>errado = FALSE</code> da função validação no código que segue, conduz a solução correta (usando a validação cruzada), que sempre você deverá considerar na prática.</p>
<p><br></p>
<div class="cell">
<details>
<summary>Estude o código da solução de exemplo</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="fu">library</span>(rsample)</span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="fu">library</span>(yardstick)</span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="fu">library</span>(tibble)</span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="fu">library</span>(purrr)</span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb8-7"><a href="#cb8-7"></a></span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="co"># Lendo dados</span></span>
<span id="cb8-9"><a href="#cb8-9"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb8-10"><a href="#cb8-10"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb8-12"><a href="#cb8-12"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb8-13"><a href="#cb8-13"></a></span>
<span id="cb8-14"><a href="#cb8-14"></a>dados <span class="ot">&lt;-</span> </span>
<span id="cb8-15"><a href="#cb8-15"></a>  dados_expectativa_renda <span class="sc">|&gt;</span> </span>
<span id="cb8-16"><a href="#cb8-16"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>CountryName) <span class="sc">|&gt;</span> </span>
<span id="cb8-17"><a href="#cb8-17"></a>  dplyr<span class="sc">::</span><span class="fu">rename</span>(<span class="at">y =</span> LifeExpectancy, <span class="at">x =</span> GDPercapita)</span>
<span id="cb8-18"><a href="#cb8-18"></a>  </span>
<span id="cb8-19"><a href="#cb8-19"></a>iteracoes <span class="ot">&lt;-</span> <span class="cf">function</span>(tibble_data, grau) {</span>
<span id="cb8-20"><a href="#cb8-20"></a>  x <span class="ot">&lt;-</span> tibble_data<span class="sc">$</span>x</span>
<span id="cb8-21"><a href="#cb8-21"></a>  iteracoes <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="at">X =</span> 2L<span class="sc">:</span>grau, <span class="at">FUN =</span> <span class="cf">function</span>(i) x<span class="sc">^</span>i)</span>
<span id="cb8-22"><a href="#cb8-22"></a>  </span>
<span id="cb8-23"><a href="#cb8-23"></a>  result <span class="ot">&lt;-</span> <span class="fu">cbind</span>(tibble_data, <span class="fu">do.call</span>(cbind, iteracoes))</span>
<span id="cb8-24"><a href="#cb8-24"></a>  <span class="fu">colnames</span>(result)[(<span class="fu">ncol</span>(tibble_data) <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span><span class="fu">ncol</span>(result)] <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">"x"</span>, 2L<span class="sc">:</span>grau)</span>
<span id="cb8-25"><a href="#cb8-25"></a>  </span>
<span id="cb8-26"><a href="#cb8-26"></a>  <span class="fu">as_tibble</span>(result)</span>
<span id="cb8-27"><a href="#cb8-27"></a>}  </span>
<span id="cb8-28"><a href="#cb8-28"></a></span>
<span id="cb8-29"><a href="#cb8-29"></a>regressao_polinomial <span class="ot">&lt;-</span> <span class="cf">function</span>(dados, <span class="at">grau =</span> 1L) {</span>
<span id="cb8-30"><a href="#cb8-30"></a>  <span class="cf">if</span>(grau <span class="sc">&gt;=</span> 2L)</span>
<span id="cb8-31"><a href="#cb8-31"></a>    dados <span class="ot">&lt;-</span> <span class="fu">iteracoes</span>(dados, <span class="at">grau =</span> grau)</span>
<span id="cb8-32"><a href="#cb8-32"></a>  </span>
<span id="cb8-33"><a href="#cb8-33"></a>  <span class="fu">lm</span>(<span class="at">formula =</span> y <span class="sc">~</span> ., <span class="at">data =</span> dados)</span>
<span id="cb8-34"><a href="#cb8-34"></a>}</span>
<span id="cb8-35"><a href="#cb8-35"></a></span>
<span id="cb8-36"><a href="#cb8-36"></a><span class="co"># Divisão dos dados</span></span>
<span id="cb8-37"><a href="#cb8-37"></a>divisao_inicial <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(dados)</span>
<span id="cb8-38"><a href="#cb8-38"></a>treinamento <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(divisao_inicial)</span>
<span id="cb8-39"><a href="#cb8-39"></a>teste <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">testing</span>(divisao_inicial) <span class="co"># Teste final</span></span>
<span id="cb8-40"><a href="#cb8-40"></a></span>
<span id="cb8-41"><a href="#cb8-41"></a><span class="co"># v-folds cross-validation</span></span>
<span id="cb8-42"><a href="#cb8-42"></a>validacao <span class="ot">&lt;-</span> <span class="cf">function</span>(dados, <span class="at">grau =</span> 1L, <span class="at">errado =</span> <span class="cn">FALSE</span>, ...){</span>
<span id="cb8-43"><a href="#cb8-43"></a>  </span>
<span id="cb8-44"><a href="#cb8-44"></a>  <span class="co"># Todas as divisões da validacao cruzada</span></span>
<span id="cb8-45"><a href="#cb8-45"></a>  cv <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">vfold_cv</span>(dados, ...)</span>
<span id="cb8-46"><a href="#cb8-46"></a>  </span>
<span id="cb8-47"><a href="#cb8-47"></a>  hiper <span class="ot">&lt;-</span> <span class="cf">function</span>(i){</span>
<span id="cb8-48"><a href="#cb8-48"></a>    treino <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">analysis</span>(cv<span class="sc">$</span>splits[[i]]) <span class="co"># Treinamento</span></span>
<span id="cb8-49"><a href="#cb8-49"></a>    validacao <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">assessment</span>(cv<span class="sc">$</span>splits[[i]]) <span class="co"># Validacação</span></span>
<span id="cb8-50"><a href="#cb8-50"></a>    ajuste <span class="ot">&lt;-</span> <span class="fu">regressao_polinomial</span>(<span class="at">dados =</span> treino, <span class="at">grau =</span> grau)</span>
<span id="cb8-51"><a href="#cb8-51"></a>    </span>
<span id="cb8-52"><a href="#cb8-52"></a>    <span class="cf">if</span>(errado){</span>
<span id="cb8-53"><a href="#cb8-53"></a>      df_treino <span class="ot">&lt;-</span> <span class="fu">iteracoes</span>(treino, <span class="at">grau =</span> grau)</span>
<span id="cb8-54"><a href="#cb8-54"></a>      df_treino <span class="ot">&lt;-</span> df_treino <span class="sc">|&gt;</span> dplyr<span class="sc">::</span><span class="fu">mutate</span>(<span class="at">y_chapeu =</span> <span class="fu">predict</span>(ajuste, <span class="at">newdata =</span> df_treino))</span>
<span id="cb8-55"><a href="#cb8-55"></a>      yardstick<span class="sc">::</span><span class="fu">rmse</span>(<span class="at">data =</span> df_treino, <span class="at">truth =</span> y, <span class="at">estimate =</span> y_chapeu)<span class="sc">$</span>.estimate</span>
<span id="cb8-56"><a href="#cb8-56"></a>    } <span class="cf">else</span> {</span>
<span id="cb8-57"><a href="#cb8-57"></a>      df_validacao <span class="ot">&lt;-</span> <span class="fu">iteracoes</span>(validacao, <span class="at">grau =</span> grau)</span>
<span id="cb8-58"><a href="#cb8-58"></a>      df_validacao <span class="ot">&lt;-</span> df_validacao <span class="sc">|&gt;</span> dplyr<span class="sc">::</span><span class="fu">mutate</span>(<span class="at">y_chapeu =</span> <span class="fu">predict</span>(ajuste, <span class="at">newdata =</span> df_validacao))</span>
<span id="cb8-59"><a href="#cb8-59"></a>      yardstick<span class="sc">::</span><span class="fu">rmse</span>(<span class="at">data =</span> df_validacao, <span class="at">truth =</span> y, <span class="at">estimate =</span> y_chapeu)<span class="sc">$</span>.estimate</span>
<span id="cb8-60"><a href="#cb8-60"></a>    }</span>
<span id="cb8-61"><a href="#cb8-61"></a>  }</span>
<span id="cb8-62"><a href="#cb8-62"></a>  purrr<span class="sc">::</span><span class="fu">map_dbl</span>(<span class="at">.x =</span> <span class="fu">seq_along</span>(cv<span class="sc">$</span>splits), <span class="at">.f =</span> hiper) <span class="sc">|&gt;</span> </span>
<span id="cb8-63"><a href="#cb8-63"></a>    <span class="fu">mean</span>()</span>
<span id="cb8-64"><a href="#cb8-64"></a>}</span>
<span id="cb8-65"><a href="#cb8-65"></a></span>
<span id="cb8-66"><a href="#cb8-66"></a>plot_avaliacao <span class="ot">&lt;-</span> <span class="cf">function</span>(dados, <span class="at">errado =</span> <span class="cn">FALSE</span>){</span>
<span id="cb8-67"><a href="#cb8-67"></a>  <span class="co"># Testando iterativamente, vários valores de p:</span></span>
<span id="cb8-68"><a href="#cb8-68"></a>  p <span class="ot">&lt;-</span> <span class="fu">seq</span>(1L<span class="sc">:</span>11L)</span>
<span id="cb8-69"><a href="#cb8-69"></a>  risco <span class="ot">&lt;-</span> purrr<span class="sc">::</span><span class="fu">map_dbl</span>(<span class="at">.x =</span> p, <span class="at">.f =</span> \(p) <span class="fu">validacao</span>(<span class="at">dados =</span> dados, <span class="at">grau =</span> p, <span class="at">errado =</span> errado))</span>
<span id="cb8-70"><a href="#cb8-70"></a>  df_risco <span class="ot">&lt;-</span> tibble<span class="sc">::</span><span class="fu">tibble</span>(<span class="at">p =</span> p, <span class="at">risco =</span> risco)</span>
<span id="cb8-71"><a href="#cb8-71"></a>  </span>
<span id="cb8-72"><a href="#cb8-72"></a>  <span class="co"># Plotando</span></span>
<span id="cb8-73"><a href="#cb8-73"></a>  df_risco <span class="sc">|&gt;</span> </span>
<span id="cb8-74"><a href="#cb8-74"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> p, <span class="at">y =</span> risco, <span class="at">color =</span> risco)) <span class="sc">+</span></span>
<span id="cb8-75"><a href="#cb8-75"></a>    <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb8-76"><a href="#cb8-76"></a>    <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> p) <span class="sc">+</span></span>
<span id="cb8-77"><a href="#cb8-77"></a>    <span class="fu">scale_y_continuous</span>(<span class="at">breaks =</span> p) <span class="sc">+</span></span>
<span id="cb8-78"><a href="#cb8-78"></a>    <span class="fu">labs</span>(</span>
<span id="cb8-79"><a href="#cb8-79"></a>      <span class="at">title =</span> <span class="st">"Valiando o risco estimado para diversos graus do polinômio"</span>,</span>
<span id="cb8-80"><a href="#cb8-80"></a>      <span class="at">subtitle =</span> <span class="st">"EQM no conjunto de validação"</span></span>
<span id="cb8-81"><a href="#cb8-81"></a>    ) <span class="sc">+</span></span>
<span id="cb8-82"><a href="#cb8-82"></a>    <span class="fu">theme</span>(</span>
<span id="cb8-83"><a href="#cb8-83"></a>      <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">18</span>, <span class="at">face =</span> <span class="st">"bold"</span>),</span>
<span id="cb8-84"><a href="#cb8-84"></a>      <span class="at">plot.subtitle =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">16</span>),</span>
<span id="cb8-85"><a href="#cb8-85"></a>      <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>), </span>
<span id="cb8-86"><a href="#cb8-86"></a>      <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">14</span>, <span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb8-87"><a href="#cb8-87"></a>    )</span>
<span id="cb8-88"><a href="#cb8-88"></a>}</span>
<span id="cb8-89"><a href="#cb8-89"></a></span>
<span id="cb8-90"><a href="#cb8-90"></a><span class="co"># Avaliacão errada versus correta</span></span>
<span id="cb8-91"><a href="#cb8-91"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb8-92"><a href="#cb8-92"></a>grafico <span class="ot">&lt;-</span> </span>
<span id="cb8-93"><a href="#cb8-93"></a>  <span class="fu">plot_avaliacao</span>(dados, <span class="at">errado =</span> <span class="cn">TRUE</span>) <span class="sc">+</span> </span>
<span id="cb8-94"><a href="#cb8-94"></a>  <span class="fu">plot_avaliacao</span>(dados, <span class="at">errado =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb8-95"><a href="#cb8-95"></a>  <span class="fu">plot_annotation</span>(<span class="at">tag_levels =</span> <span class="fu">c</span>(<span class="st">"A"</span>, <span class="st">"B"</span>))</span>
<span id="cb8-96"><a href="#cb8-96"></a></span>
<span id="cb8-97"><a href="#cb8-97"></a><span class="fu">ggsave</span>(grafico, <span class="at">file =</span> <span class="st">"imgs/avaliacao_risco.png"</span>, <span class="at">device =</span> <span class="st">"png"</span>, <span class="at">width =</span> <span class="dv">50</span>, <span class="at">height =</span> <span class="dv">20</span>, <span class="at">units =</span> <span class="st">"cm"</span>, <span class="at">limitsize =</span> F)</span>
<span id="cb8-98"><a href="#cb8-98"></a></span>
<span id="cb8-99"><a href="#cb8-99"></a>plot_bar <span class="ot">&lt;-</span> <span class="cf">function</span>(grau){</span>
<span id="cb8-100"><a href="#cb8-100"></a>  ruim <span class="ot">&lt;-</span> <span class="fu">validacao</span>(dados, <span class="at">errado =</span> <span class="cn">TRUE</span>, <span class="at">grau =</span> grau)</span>
<span id="cb8-101"><a href="#cb8-101"></a>  bom <span class="ot">&lt;-</span> <span class="fu">validacao</span>(dados, <span class="at">errado =</span> <span class="cn">FALSE</span>, <span class="at">grau =</span> grau)</span>
<span id="cb8-102"><a href="#cb8-102"></a>  df <span class="ot">&lt;-</span> tibble<span class="sc">::</span><span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="st">"Errado"</span>, <span class="st">"Certo"</span>), <span class="at">y =</span> <span class="fu">c</span>(<span class="fu">log</span>(ruim), <span class="fu">log</span>(bom)))</span>
<span id="cb8-103"><a href="#cb8-103"></a>  </span>
<span id="cb8-104"><a href="#cb8-104"></a>  df <span class="sc">|&gt;</span> </span>
<span id="cb8-105"><a href="#cb8-105"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb8-106"><a href="#cb8-106"></a>    <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>) <span class="sc">+</span></span>
<span id="cb8-107"><a href="#cb8-107"></a>    <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">label =</span> y), <span class="at">vjust =</span> <span class="dv">0</span>)</span>
<span id="cb8-108"><a href="#cb8-108"></a>}</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>

<img data-src="gifs/mr-bean-pivot.gif" class="r-stretch"></section>
<section id="exercícios" class="slide level2">
<h2>📚 Exercícios</h2>
<p><br></p>
<p><span class="red">Exercício</span>: Explique resumidamente o que é aprendizagem supervisionada e não-supervisionada. Cite um problema de aprendizagem supervisionada e um outro de aprendizagem não-supervisionada.</p>
<p><br></p>
<p><span class="red">Exercício</span>: Considere o conjunto de dados de Expectativa de vida versus PIB per Capita, disponível <a href="https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData">aqui</a>. Considere a função <span class="math inline">g</span>, da seguinte forma:</p>
<p><span class="math display">g(x) = \beta_0 + \sum_{i = 1}^p \beta_i x^i,</span> com <span class="math inline">p \in \{1, 2, ..., 50\}</span>. Utilizando o erro quadrático médio observado, sem fazer nenhuma estratégia de divisão dos dados, implemente um código em R para checar qual o melhor modelo.</p>
<p><br></p>
<p><span class="red">Exercício</span>: Explique qual o motivo que faz com que o Erro Quadrático Médio - EQM para avaliar o desempenho de um modelo é ruim quando não adotamos nenhuma estratégia de divisão do conjunto de dados em treinamento e teste.</p>
</section>
<section id="exercícios-1" class="slide level2">
<h2>📚 Exercícios</h2>
<p><br></p>
<p><span class="red">Exercício</span>: Com suas palavras, explique o dilema de balanço entre víes e variância.</p>
<p><br></p>
<p><span class="red">Exercício</span>: Refaça o exercício do polinômio, utilizando a estratégia de <span class="red"><em>data splitting</em></span>, em que divide-se o conjunto de dados em treinamento e teste. Utilize o conjunto de teste para calcular a estimativa do risco, usando o EQM.</p>
<p><br></p>
<p><span class="red">Exercício</span>: Ainda considerando o exercício do polinômio, implemente uma estratégia de <span class="red"><em>leave-one-out cross-validation</em></span> e selecione o melhor modelo minimizando a função de risco.</p>
<p><br></p>
<p><span class="red">Exercício</span>: Por fim, considerando o exercício do polinômio, rafaça-o utilizando um procedimento de <span class="red"><span class="math inline">k</span>-fold cross-validation</span>. Considere <span class="math inline">k = 5</span>. <strong>Dica</strong>: considere utiliza a biblioteca <a href="https://rsample.tidymodels.org/">rsample</a>.</p>

<img data-src="imgs/rsample.png" class="r-stretch"></section>
<section id="melhor-subconjunto-de-covariáveis" class="slide level2">
<h2>Melhor subconjunto de covariáveis</h2>
<p><br></p>
<p>O estimador de mínimos quadrados - EMQ, na presença de muitas <em>features</em> (covariáveis), i.e., quando temos <span class="math inline">d</span> grande, possui um baixo poder preditivo devido <em>overfitting</em> (super-ajuste). Isso, porquê haverá muitos parâmetros a serem estimados, e portanto, a função de regressão estimada <span class="math inline">\widehat{r}({\bf x})</span> terá baixo poder preditivo.</p>
<p><br></p>
<p>Isso se deve ao fato do balanço de viés e variância. Havendo muitos parâmetros, como já tinhamos visto, a variância do modelo será muito alta.</p>
<p><br></p>
<p><strong>Portanto, deveremos buscar meios de encontrar o melhor (ao menos um bom) comjunto de covariáveis.</strong></p>
<p><br></p>

<img data-src="gifs/chapulin-colorado-no.gif" style="width:20.0%" class="r-stretch"></section>
<section id="melhor-subconjunto-de-covariáveis-1" class="slide level2">
<h2>Melhor subconjunto de covariáveis</h2>
<p><br></p>
<p>A ideia para resolver esse problema é retirar algumas covariáveis do modelo de regressão, com o objetivo de diminuir a variância de <span class="math inline">\widehat{g}</span>.</p>
<p><br></p>
<p>Você poderá entender que estamos em busca de um estimador <span class="math inline">\widehat{g}</span> de <span class="math inline">g</span> um <strong>pouco</strong> mais viesado. Trata-se de uma troca em que desejamos reduzir substancialmente a variabilidade do estiamdor do modelo em troca de um pouco mais de viés.</p>
<p><br></p>

<img data-src="gifs/kiko.gif" class="r-stretch"></section>
<section id="melhor-subconjunto-de-covariáveis-2" class="slide level2">
<h2>Melhor subconjunto de covariáveis</h2>
<p><br></p>
<p>Matematicamente, uma maneira de fazer isso, é buscar a estimativa para</p>
<p><span id="eq-risco-penalizacao"><span class="math display">\widehat{\beta}_{L_0} = \argmin_{\beta_0 \in \mathbb{R}, \beta \in \mathbb{R}^d}\sum_{k = 1}^n\overbrace{\left(y_k - \beta_0 - \sum_{i = 1}^d \beta_i x_{k,i}\right)^2}^{n \times EQM} + \lambda \,\,\underbrace{\sum_{i = 1}^d \mathbb{I}(\beta_i \neq 0)}_{\text{Penalização}}. \tag{3}</span></span></p>
<p>Note que a penalização <span class="math inline">\sum_{i = 1}^d \mathbb{I}(\beta_i \neq 0)</span> nos conduz na direção de modelos com poucas covariáveis, quando <span class="math inline">\lambda</span> é um valor alto. Em particualr, quando <span class="math inline">\lambda \to \infty</span>, forçamos a retirada de todas as covariáveis <span class="math inline">\beta_i</span>’s, i.e., a solução para o problema seria <span class="math inline">\widehat{\beta}_{L_0} \equiv (\overline{y}, {\bf 0})</span>. Note que não há penalização para o intercepto <span class="math inline">\beta_0</span>.</p>
<p><br> No outro extremo, para <span class="math inline">\lambda = 0</span>, temos o estimador de mínimos quadrados, em que nenhuma penalização será considerada.</p>
<p><br> <span class="red">Não há uma forma fácil de minimizar <span class="math inline">\widehat{\beta}_{L_0}</span>.</span></p>
</section>
<section id="melhor-subconjunto-de-covariáveis-3" class="slide level2">
<h2>Melhor subconjunto de covariáveis</h2>
<p><br></p>
<p>Poderíamos, ingenuamente, pensar em ajustar todas as combinações possíveis de parâmetros e utilizar algum critério, por exemplo, o EQM em novas observações para escolher o melhor modelo de todas as combinações possíveis. Isto é, escolher o melhor modelo entre todas as <span class="math inline">2^d</span> combinações possíveis de modelos em <span class="math inline">\mathbb{G}</span>.</p>
<p><br></p>
<p>Se <span class="math inline">\widehat{\lambda} = \frac{2}{n}\widehat{\sigma}^2</span>, estimar <span class="math inline">\widehat{\beta}_{L_0}</span> equivale uma busca entre <span class="math inline">2^d</span> modelos da classe <span class="math inline">\mathbb{G}</span>:</p>
<p><br></p>
<span class="math display">\begin{align*}
\mathbb{G} = \{
&amp;g({\bf x}) = \widehat{\beta}_0, \\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_1x_1,\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_2x_2,\\
&amp;\cdots\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_dx_d,\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_2x_2,\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_3x_3,\\
&amp;\cdots\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_dx_d,\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_2x_2 + \widehat{\beta}_3x_3,\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_2x_2 + \widehat{\beta}_dx_d,\\
&amp;\cdots\\
&amp;g({\bf x}) = \widehat{\beta}_0 + \widehat{\beta}_1x_1 + \widehat{\beta}_2x_2 + \widehat{\beta}_3x_3 + \cdots + &amp;\widehat{\beta}_dx_d
\}.
\end{align*}</span>
</section>
<section id="melhor-subconjunto-de-covariáveis-4" class="slide level2">
<h2>Melhor subconjunto de covariáveis</h2>
<p><br></p>
<p>Utilizar <span class="math inline">\lambda = \frac{2}{n}\widehat{\sigma}^2</span> é o mesmo que utilizar o critério AIC para determinar o melhor modelo, em que dado</p>
<p><span class="math display">\widehat{R}(g) = \frac{1}{m}\sum_{k = 1}^m \underbrace{(\widetilde{Y}_k - g({\bf \widetilde{X}}_k))^2}_{W_k},</span> em que <span class="math inline">(\widetilde{{\bf X}}_1, \widetilde{Y}_1), \cdots, (\widetilde{{\bf X}}_m, \widetilde{Y}_m)</span>, representa o conjunto de teste, i.e., calculado com base em <span class="math inline">m</span> observações em um conjundo de dados não utilizados para treinar o modelo, independentemente da estratégia de divisão utilizada, em que</p>
<p><span class="math display">\widehat{\sigma}^2 = \frac{1}{m}\sum_{k = 1}^m (W_k - \overline{W})^2,</span> com <span class="math inline">\overline{W} = \frac{1}{m}\sum_{k=1}^m W_k</span>.</p>
</section>
<section id="melhor-subconjunto-de-covariáveis-5" class="slide level2">
<h2>Melhor subconjunto de covariáveis</h2>
<p><br></p>
<p>Temos que <span class="math inline">\widehat{R}(g)</span>, calculado em novas observações (oboservações não utilizadas no treinamento), pode se valer do Teorema Central do Limite, uma vez que <span class="math inline">\widehat{R}(g)</span> é calculado em uma sequência de variáveis aleatórias i.i.d.’s. Então:</p>
<p><span class="math display">\widehat{R}(g) \sim \text{Normal}\left(R(g), \frac{1}{m}\mathbb{V}[W_1]\right).</span></p>
<p>Portanto, um intervalo aleatório de aproximadamente <span class="math inline">95\%</span> de confiança para o erro preditivo <span class="math inline">R(g)</span> poderá ser calculado como:</p>
<p><span class="math display">\widehat{R}(g) \pm 1,645 \sqrt{\frac{1}{m}\widehat{\sigma}^2}.</span></p>
<p>O cálculo de um intervalo de confiança poderá ser útil para entendermos como está variando o risco preditivo do nosso modelo. Gostamos de ter modelos com intervalo de amplitude pequena. O intervalo poderá ser utilizado para fornecer insight de como escolher a divisão de treinamento e teste. Por exemplo, pode-se escolher o menor valor de <span class="math inline">m</span> de modo que a amplitude seja a menor possível.</p>
</section>
<section id="melhor-subconjunto-de-covariáveis-6" class="slide level2">
<h2>Melhor subconjunto de covariáveis</h2>
<p><br></p>
<p>Por que essa seria uma escolha ingênua? Pense na situação em que temos <span class="math inline">d = 30</span>, i.e., trinta covariáveis. Teríamos portanto <span class="math inline">2^{30}</span> modelos para ajustar, ou seja, um bilhão e setenta e três milhões, setecentos e quarenta e um mil, oitocentos e vinte e quatro modelos para ajustar. É um a quantidade absurda de modelos para serem estimados!</p>
<p><br></p>
<p>Se <span class="math inline">d = 100</span>, teríamos que estimar uma quantidade de modelos que a quantidade estimada de estrelas no universo. Alias, seriam mais modelos para ajustar que a quantidade de átomos no universo.</p>
<p><br></p>

<img data-src="gifs/side-eyeing-chloe-chloe.gif" class="r-stretch"></section>
<section id="regressão-stepwise" class="slide level2">
<h2>Regressão <em>Stepwise</em></h2>
<p><br> Devido a impossibilidade de experimentar uma grande quantidade de modelos (<span class="math inline">2^d</span>), existe uma série de algoritmos (heurísticas), que visam reduzir a quantidade de modelos avaliados. Um dos mais conhecidos é o <span class="red">forward stepwise</span>. Trata-se de um algoritmo sequencial, que em cada passo, apenas uma variável é adicionada:</p>
<p><br></p>
<p>1 - Para <span class="math inline">j = 1, \cdots, d</span>, ajuste a regressão de <span class="math inline">Y</span> na <span class="math inline">j</span>-ésima variável <span class="math inline">X_j</span>. Seja <span class="math inline">\widehat{R}(g_j)</span> o risco estimado desta função. Então,</p>
<p><span class="math display">\widehat{j} = \argmin_j \widehat{R}(g_j)\,\,\,\,\,\, \text{e}\,\,\,\,\,\, S = \{\widehat{j}\}.</span> 2 - Para cada <span class="math inline">j \in S^c</span>, ajuste a regressão <span class="math inline">Y = \beta_jX_j + \sum_{s \in S}\beta_sX_S</span>, em que <span class="math inline">\widehat{R}(g_j)</span> é o risco estimado desta função. Defina</p>
<p><span class="math display">\widehat{j} = \argmin_{j \in S^c} \widehat{R}(g_j)\,\,\,\,\,\, \text{e atualize}\,\,\,\,\,\, S \leftarrow \{S \cup \widehat{j}\}.</span></p>
<p>3 - Repita os passos anteriores até que todas as variáveis estejam em <span class="math inline">S</span> ou até quando não seja mais possível ajustar o modelo de regressão.</p>
<p>4 - Selecione o modelo com menor risco estimado.</p>
</section>
<section id="regressão-stepwise-1" class="slide level2">
<h2>Regressão <em>Stepwise</em></h2>
<p><br></p>
<p>Utilizar o algoritmo de seleção de variáveis <span class="red"><em>foward stepwise</em></span>, ao invés de buscarmos o melhor ajuste entre <span class="math inline">2^d</span> possíveis modelos, que muitas vezes é impossível, precisaremos investigar apenas <span class="math inline">1 + d(d + 1)/2</span> modelos. Reduzimos a complexidade da seleção que antes era um problema exponencial. Melhor, não?!</p>
<p><br></p>

<img data-src="gifs/bom.gif" class="r-stretch"></section>
<section id="penalização" class="slide level2">
<h2>Penalização</h2>
<p><br></p>
<p>Quando temos modelos que envolve <span class="math inline">d</span> parâmetros e que temos controle sobre eles (conhecemos muito bem cada um deles), acrescentar algum tipo de penalização à função objetivo poderá ser útil. A penalização é uma <span class="red">medida de complexidade</span>, em que é útil para equilibrar o modelo, de modo a tentar buscar um equilibrio entre viés e variância, discutidos anteriormente. Assim, sob novas observações, desejamos estimar o risco <span class="math inline">R(g)</span>, por</p>
<p><br></p>
<p><span class="math display">R(g) \approx EQM(g) + \mathcal{P}(g).</span></p>
<p><br></p>
<p>Desejamos minimiar <span class="math inline">R(g)</span>, mas não a custa de muitos parâmetros, pois assim teríamos <span class="red">overfitting</span>. Portanto, para muitos parâmetros temos que ter EQM baixo, porém, <span class="math inline">\mathcal{P}(g)</span> deve ser alto. Já em modelos viesados, quando temos poucos parâmetros, o EQM normalmente é alto, mas a complexidade <span class="math inline">\mathcal{P}(g)</span> deve ser baixo, pois temos um modelo mais simplista.</p>
</section>
<section id="aic-e-bic" class="slide level2">
<h2>AIC e BIC</h2>
<p><br></p>
<p>Existem diversas penalizações, em que o AIC (<strong>A</strong>kaike <strong>I</strong>nformation <strong>C</strong>riterion) e BIC (<strong>B</strong>ayesian <strong>I</strong>nformation <strong>C</strong>riterion) são as mais conhecidas. Com base nesses critérios, temos que</p>
<p><br></p>
<div class="{columns}">
<div class="column" style="width:30%;">
<ol type="1">
<li class="fragment"><span class="red">AIC</span>: <span class="math display">EQM + \frac{2}{n\,d\, \widehat{\sigma}^2}.</span></li>
<li class="fragment"><span class="red">BIC</span>: <span class="math display">EQM + \frac{\log(n)}{n\, d\, \widehat{\sigma}^2}.</span></li>
</ol>
</div>
<div class="column" style="width:70%;">
<p><span class="math display">\widehat{\sigma}^2 = \frac{1}{m}\sum_{k = 1}^m (W_k - \overline{W})^2.</span></p>
</div>
</div>
<p><br></p>
<p>Aqui, <span class="math inline">d</span> é a quantidade de parâmetros no modelo e <span class="math inline">\widehat{\sigma}^2</span> é uma estimativa da variância do erro, que para um conjunto de teste suficientemente grande, poderá ser considerado o estimador de <span class="math inline">\widehat{\sigma}^2</span> conforme descrito anteriormente. Segundo <a href="https://hastie.su.domains/ISLR2/ISLRv2_corrected_June_2023.pdf">James, Gareth, et al.&nbsp;An introduction to statistical learning. Ed. 2, p. 233</a>, assume-se o modelo com todos os preditores para o cálculo de <span class="math inline">\widehat{\sigma}^2</span>.</p>
</section>
<section id="lasso" class="slide level2">
<h2>Lasso</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p>O lasso foi desenvolvido pelo Robert Tibshirani, em um artigo publicado no artigo <a href="https://www.jstor.org/stable/2346178">Regression Shrinkage and Selection via the Lasso</a>.</p>
<p><img data-src="imgs/lasso.png"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="imgs/P55268-Robert-Tibshirani.jpg" style="width:60.0%"></p>
</div>
</div>
</section>
<section id="lasso-1" class="slide level2">
<h2>Lasso</h2>
<p><br></p>
<p>O lasso tem como objetivo encontrar um estimador de uma regressão linear que possui risco menor que o de mínimos quadrados, possuindo duas grandes vantagens, em relação ao <em>stepwise</em>:</p>
<p><br></p>
<ol type="1">
<li class="fragment">Sua solução é mais rápida, ainda que <em>stepwise</em> seja consideravalmente mais rápido do que avaliar <span class="math inline">2^d</span> modelos;</li>
<li class="fragment">O lasso é capaz de selecionar automaticamente as variáveis mais relevantes para o modelo, reduzindo a dimensionalidade dos dados.</li>
</ol>
<p><br></p>
<p>A segunda vantagem ocorre, uma vez que ele realiza uma penalização que leva à estimativas de alguns coeficientes <span class="math inline">\beta_i</span> igual a zero, eliminando as variáveis menos importantes.</p>
<p><br></p>

<img data-src="gifs/uau.gif" class="r-stretch"></section>
<section id="lasso-2" class="slide level2">
<h2>Lasso</h2>
<p><br></p>
<p>No lasso, ao invés de reduzir a variância do estimador de mínimos quadrados usando a complexidade (<span class="math inline">L_0 = \sum_{i = 1}^d\mathbb{I}(\beta_i) \neq 0</span>) em <a href="#/melhor-subconjunto-de-covariáveis-2" class="quarto-xref">Equação&nbsp;3</a>, usa-se a penalização <span class="math inline">L_1 = \sum_{i = 1}^d|\beta_i|</span>. No lasso, buscamos:</p>
<p><span class="math display">\widehat{\beta}_{L_1,\lambda} = \argmin_{\beta_0 \in \mathbb{R}, \beta \in \mathbb{R}^d}\sum_{k = 1}^n\overbrace{\left(y_k - \beta_0 - \sum_{i = 1}^d \beta_i x_{x,i}\right)^2}^{n \times EQM} + \lambda \,\,\underbrace{\sum_{j = 1}^d|\beta_j|}_{\text{Penalização}},</span> em que <span class="math inline">\lambda</span> é um <em>tuning parameter</em>. Perceba que quando <span class="math inline">\lambda = 0</span>, caímos no caso do modelo de regressão por mínimos quadrados sem penalização. Já, quando <span class="math inline">\lambda \rightarrow \infty</span>, temos um modelo em que todas as variáveis são removidas, uma vez que a primeira parte do modelo torna-se insignificante.</p>
</section>
<section id="lasso-3" class="slide level2">
<h2>Lasso</h2>
<p><br> <br></p>
<p>Quando <span class="math inline">\lambda</span> é grande, temos que</p>
<p><span class="math display">\sum_{k = 1}^n \left(y_k - \beta_0 - \sum_{j = 1}^d \beta_j x_{k,j}\right)^2 + \lambda \sum_{j = 1}^d |\beta_j| \approx \lambda \sum_{j = 1}^d |\beta_j|,</span> e portanto, <span class="math inline">\widehat{\beta}_1 = 0, \cdots, \widehat{\beta}_d = 0</span>.</p>
<p><br></p>
<p>A escolha de <span class="math inline">\lambda</span>, em geral, é feita utilizado algum método de validação cruzada.</p>
</section>
<section id="lasso-4" class="slide level2">
<h2>Lasso</h2>
<p><br></p>
<p>O lasso é extremamente rápido, e nos últimos anos, diversos algoritmos foram construídos para fazer essa taréfa de forma eficiente. O LARS foi um dos primeiros algoritmos desenvolvidos em 2010. Para detalhes, ler <a href="https://www.jstor.org/stable/2699986">Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of statistics, 1189–1232</a>.</p>
<p><br></p>
<p>No R, a regressão lasso poderá ser feita usando a biblioteca <a href="https://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a>, assim:</p>
<p><br></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb9-2"><a href="#cb9-2"></a>ajuste <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">1</span>)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>

<img data-src="gifs/teclado-anime.gif" style="width:21.0%" class="r-stretch"></section>
<section id="ridge" class="slide level2">
<h2>Ridge</h2>
<p><br></p>
<p>Uma alternativa que surgiu antes do lasso é a <span class="red">regressão ridge</span>. Ela foi proposta no artigo <a href="https://www.jstor.org/stable/1267351">Hoerl, A. E. &amp; Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1), 55–67</a>. Aqui, utiliza-se como medida de complexidade a norma <span class="math inline">L_2</span>, em que, o estimador é dado por:</p>
<p><span class="math display">\widehat{\beta}_{L_2,\lambda} = \argmin_{\beta_0 \in \mathbb{R}, \beta \in \mathbb{R}^d}\sum_{k = 1}^n\overbrace{\left(y_k - \beta_0 - \sum_{i = 1}^d \beta_i x_{x,i}\right)^2}^{n \times EQM} + \lambda \,\,\underbrace{\sum_{j = 1}^d\beta_j^2}_{\text{Penalização}}.</span> Diferentemente do lasso, a regressão ridge possui solução analítica, dada por:</p>
<p><span class="math display">\widehat{\beta}_{L_2,\lambda} = ({\bf X}^{T}{\bf X} + \lambda\mathbb{\bf I}_0)^{-1}{\bf X}^{T}Y,</span> em que <span class="math inline">\mathbb{\bf I}_0</span> é a matriz identidade <span class="math inline">(d + 1) \times (d + 1)</span> com <span class="math inline">\mathbb{\bf I}_0(1,1) = 0</span>.</p>
<p><br></p>
</section>
<section id="ridge-1" class="slide level2">
<h2>Ridge</h2>
<p><br></p>
<p>A regressão ridge poderá ter uma variância menor que a regressão lasso, porém seu viés poderá ser maior. Outra característica da regressão ridge é que ela possue uma única solução, enquanto a regressão lasso poderá ter multiplas soluções. Os autores também demonstram que a regressão ridge lida melhor com multicolinearidade.</p>
<p><br></p>
<p>No R, também poderemos utilizar a biblioteca <a href="https://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a>:</p>
<p><br></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb10-2"><a href="#cb10-2"></a>ajuste <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>

<img data-src="gifs/teclado-anime.gif" style="width:21.0%" class="r-stretch"></section>
<section id="elastic-net" class="slide level2">
<h2>Elastic Net</h2>
<p><br></p>
<p>Nesse tipo de modelo de gressão, combina-se as penalizações da regressão ridge com a utilizada na regressão lasso, herdando os benefícios do uso de cada um dos métodos isoladamente, melhorando a estabilidade das estimativas do lasso, em situações de multicolinearidade entre as variáveis e também permitindo a seleção automática de variáveis.</p>
<p><span class="math display">(1-\alpha)\widehat{\beta}_{L_2,\lambda} + \alpha\widehat{\beta}_{L_1,\lambda},</span> em que <span class="math inline">0 \leq \alpha \leq1</span>. Em R, basta especificar para a função <code>glmnet</code> um valor de <span class="math inline">\alpha</span> diferente de 0 e 1.</p>
<p><br></p>

<img data-src="gifs/teclado-anime.gif" style="width:21.0%" class="r-stretch"></section>
<section id="exemplo-emq-ridge-lasso-e-elastic-net" class="slide level2">
<h2>Exemplo: EMQ, Ridge, Lasso e Elastic Net</h2>
<p><br></p>
<p><span class="red">Exemplo</span>: Considere uma base de dados simulada, com <span class="math inline">n = 500</span> observações, de tal forma que</p>
<p><span class="math display">Y = 3X_{1} - 2X_2 + X_3 + -3X_4 + X_5 + \sum_{i = 6}^{20}0X_i + \varepsilon,</span> em que <span class="math inline">\varepsilon \sim \text{Normal}(0, 0.5^2)</span> e <span class="math inline">X_i \sim \text{Normal}(0, 1)</span>, independentes, com <span class="math inline">i = 1, \cdots, 20</span>. Desejamos ajustar quatro modelos de regressão. Para o caso do Estimador de Mínimos Quadrados - EMQ e do modelo Ridge, que não tem seleção automática de variáveis, usaremos <strong>todas</strong> as variáveis. Desejamos avaliar o risco estimado de cada uma das regressões.</p>

<img data-src="gifs/teclado-anime.gif" style="width:21.0%" class="r-stretch"></section>
<section id="exemplo-emq-ridge-lasso-e-elastic-net-1" class="slide level2">
<h2>Exemplo: EMQ, Ridge, Lasso e Elastic Net</h2>
<p><br></p>
<div class="cell">
<details>
<summary>Solução utilizando o tidymodels</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="fu">library</span>(tibble)</span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="fu">library</span>(purrr)</span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb11-6"><a href="#cb11-6"></a></span>
<span id="cb11-7"><a href="#cb11-7"></a><span class="co"># Removendo possíveis conflitos de pacotes --------------------------------</span></span>
<span id="cb11-8"><a href="#cb11-8"></a>tidymodels<span class="sc">::</span><span class="fu">tidymodels_prefer</span>()</span>
<span id="cb11-9"><a href="#cb11-9"></a></span>
<span id="cb11-10"><a href="#cb11-10"></a><span class="co"># Função para gerar os dados ----------------------------------------------</span></span>
<span id="cb11-11"><a href="#cb11-11"></a>gerando_dados <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">n =</span> 300L){</span>
<span id="cb11-12"><a href="#cb11-12"></a>  regressao <span class="ot">&lt;-</span> <span class="cf">function</span>(i){</span>
<span id="cb11-13"><a href="#cb11-13"></a>    x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="at">n =</span> 5L)</span>
<span id="cb11-14"><a href="#cb11-14"></a>    y <span class="ot">&lt;-</span> <span class="dv">3</span><span class="sc">*</span>x[1L] <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>x[2L] <span class="sc">+</span> x[3L] <span class="sc">-</span> <span class="dv">3</span><span class="sc">*</span>x[4L] <span class="sc">+</span> x[5L] <span class="sc">+</span> <span class="fu">rnorm</span>(1L, <span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb11-15"><a href="#cb11-15"></a>    <span class="fu">tibble</span>(</span>
<span id="cb11-16"><a href="#cb11-16"></a>      <span class="at">y =</span> y,</span>
<span id="cb11-17"><a href="#cb11-17"></a>      <span class="at">x1 =</span> x[1L],</span>
<span id="cb11-18"><a href="#cb11-18"></a>      <span class="at">x2 =</span> x[2L],</span>
<span id="cb11-19"><a href="#cb11-19"></a>      <span class="at">x3 =</span> x[3L],</span>
<span id="cb11-20"><a href="#cb11-20"></a>      <span class="at">x4 =</span> x[4L],</span>
<span id="cb11-21"><a href="#cb11-21"></a>      <span class="at">x5 =</span> x[5L]</span>
<span id="cb11-22"><a href="#cb11-22"></a>    )</span>
<span id="cb11-23"><a href="#cb11-23"></a>  }</span>
<span id="cb11-24"><a href="#cb11-24"></a>  dados <span class="ot">&lt;-</span> purrr<span class="sc">::</span><span class="fu">map</span>(<span class="at">.x =</span> 1L<span class="sc">:</span>n, <span class="at">.f =</span> regressao) <span class="sc">|&gt;</span> </span>
<span id="cb11-25"><a href="#cb11-25"></a>    purrr<span class="sc">::</span><span class="fu">list_rbind</span>()</span>
<span id="cb11-26"><a href="#cb11-26"></a>  </span>
<span id="cb11-27"><a href="#cb11-27"></a>  parte_esparsa <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, n, <span class="dv">15</span>)</span>
<span id="cb11-28"><a href="#cb11-28"></a>  </span>
<span id="cb11-29"><a href="#cb11-29"></a>  dados <span class="ot">&lt;-</span> <span class="fu">cbind</span>(dados, parte_esparsa)</span>
<span id="cb11-30"><a href="#cb11-30"></a>  <span class="fu">colnames</span>(dados) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"y"</span>, <span class="fu">paste0</span>(<span class="st">"x"</span>, 2L<span class="sc">:</span><span class="fu">ncol</span>(dados)))</span>
<span id="cb11-31"><a href="#cb11-31"></a>  <span class="fu">as_tibble</span>(dados)</span>
<span id="cb11-32"><a href="#cb11-32"></a>}</span>
<span id="cb11-33"><a href="#cb11-33"></a></span>
<span id="cb11-34"><a href="#cb11-34"></a>dados <span class="ot">&lt;-</span> <span class="fu">gerando_dados</span>(<span class="at">n =</span> <span class="dv">500</span>)</span>
<span id="cb11-35"><a href="#cb11-35"></a></span>
<span id="cb11-36"><a href="#cb11-36"></a><span class="co"># Divisão inicial da base -------------------------------------------------</span></span>
<span id="cb11-37"><a href="#cb11-37"></a>hod_out <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(dados, <span class="at">prop =</span> <span class="fl">0.7</span>)</span>
<span id="cb11-38"><a href="#cb11-38"></a>treinamento <span class="ot">&lt;-</span> <span class="fu">training</span>(hod_out)</span>
<span id="cb11-39"><a href="#cb11-39"></a>teste <span class="ot">&lt;-</span> <span class="fu">testing</span>(hod_out)</span>
<span id="cb11-40"><a href="#cb11-40"></a></span>
<span id="cb11-41"><a href="#cb11-41"></a><span class="co"># Setando o modelo (set engine) -------------------------------------------</span></span>
<span id="cb11-42"><a href="#cb11-42"></a>modelo_eqm <span class="ot">&lt;-</span> </span>
<span id="cb11-43"><a href="#cb11-43"></a>  <span class="fu">linear_reg</span>(<span class="at">penalty =</span> <span class="dv">0</span>, <span class="at">mixture =</span> <span class="dv">0</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-44"><a href="#cb11-44"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-45"><a href="#cb11-45"></a>  <span class="fu">set_engine</span>(<span class="st">"glmnet"</span>)</span>
<span id="cb11-46"><a href="#cb11-46"></a>  </span>
<span id="cb11-47"><a href="#cb11-47"></a>modelo_ridge <span class="ot">&lt;-</span> </span>
<span id="cb11-48"><a href="#cb11-48"></a>  <span class="fu">linear_reg</span>(<span class="at">penalty =</span> tune<span class="sc">::</span><span class="fu">tune</span>(), <span class="at">mixture =</span> <span class="dv">0</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-49"><a href="#cb11-49"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-50"><a href="#cb11-50"></a>  <span class="fu">set_engine</span>(<span class="st">"glmnet"</span>)</span>
<span id="cb11-51"><a href="#cb11-51"></a></span>
<span id="cb11-52"><a href="#cb11-52"></a>modelo_lasso <span class="ot">&lt;-</span> </span>
<span id="cb11-53"><a href="#cb11-53"></a>  parsnip<span class="sc">::</span><span class="fu">linear_reg</span>(<span class="at">penalty =</span> tune<span class="sc">::</span><span class="fu">tune</span>(), <span class="at">mixture =</span> <span class="dv">1</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-54"><a href="#cb11-54"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-55"><a href="#cb11-55"></a>  parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"glmnet"</span>)</span>
<span id="cb11-56"><a href="#cb11-56"></a>  </span>
<span id="cb11-57"><a href="#cb11-57"></a>modelo_elastic <span class="ot">&lt;-</span> </span>
<span id="cb11-58"><a href="#cb11-58"></a>  parsnip<span class="sc">::</span><span class="fu">linear_reg</span>(<span class="at">penalty =</span> tune<span class="sc">::</span><span class="fu">tune</span>(), <span class="at">mixture =</span> tune<span class="sc">::</span><span class="fu">tune</span>()) <span class="sc">|&gt;</span> </span>
<span id="cb11-59"><a href="#cb11-59"></a>  <span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-60"><a href="#cb11-60"></a>  parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"glmnet"</span>)</span>
<span id="cb11-61"><a href="#cb11-61"></a></span>
<span id="cb11-62"><a href="#cb11-62"></a><span class="co"># Criando workflows -------------------------------------------------------</span></span>
<span id="cb11-63"><a href="#cb11-63"></a>all_wf <span class="ot">&lt;-</span> </span>
<span id="cb11-64"><a href="#cb11-64"></a>  <span class="fu">workflow_set</span>(</span>
<span id="cb11-65"><a href="#cb11-65"></a>    <span class="at">preproc =</span> <span class="fu">list</span>(y <span class="sc">~</span> . ),</span>
<span id="cb11-66"><a href="#cb11-66"></a>    <span class="at">models =</span> <span class="fu">list</span>(<span class="at">eqm =</span> modelo_eqm, <span class="at">ridge =</span> modelo_ridge, <span class="at">lasso =</span> modelo_lasso, <span class="at">elastic =</span> modelo_elastic), </span>
<span id="cb11-67"><a href="#cb11-67"></a>    <span class="at">cross =</span> <span class="cn">TRUE</span></span>
<span id="cb11-68"><a href="#cb11-68"></a>  )</span>
<span id="cb11-69"><a href="#cb11-69"></a></span>
<span id="cb11-70"><a href="#cb11-70"></a><span class="co"># Validação cruzada -------------------------------------------------------</span></span>
<span id="cb11-71"><a href="#cb11-71"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb11-72"><a href="#cb11-72"></a>cv <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">vfold_cv</span>(treinamento, <span class="at">v =</span> 5L)</span>
<span id="cb11-73"><a href="#cb11-73"></a></span>
<span id="cb11-74"><a href="#cb11-74"></a><span class="co"># Setando a métrica -------------------------------------------------------</span></span>
<span id="cb11-75"><a href="#cb11-75"></a>metrica <span class="ot">&lt;-</span> yardstick<span class="sc">::</span><span class="fu">metric_set</span>(rmse)</span>
<span id="cb11-76"><a href="#cb11-76"></a></span>
<span id="cb11-77"><a href="#cb11-77"></a><span class="co"># Tunagem dos hiperparâmetros ---------------------------------------------</span></span>
<span id="cb11-78"><a href="#cb11-78"></a><span class="co"># A semente (seed = 0) faz com que dentro da validação cruzada para cada modelo</span></span>
<span id="cb11-79"><a href="#cb11-79"></a><span class="co"># a semente seja sempre a mesma.</span></span>
<span id="cb11-80"><a href="#cb11-80"></a>tunagem <span class="ot">&lt;-</span> </span>
<span id="cb11-81"><a href="#cb11-81"></a>  all_wf <span class="sc">|&gt;</span> </span>
<span id="cb11-82"><a href="#cb11-82"></a>  <span class="fu">workflow_map</span>(</span>
<span id="cb11-83"><a href="#cb11-83"></a>    <span class="at">seed =</span> <span class="dv">0</span>, </span>
<span id="cb11-84"><a href="#cb11-84"></a>    <span class="at">verbose =</span> <span class="cn">TRUE</span>,</span>
<span id="cb11-85"><a href="#cb11-85"></a>    <span class="at">resamples =</span> cv,</span>
<span id="cb11-86"><a href="#cb11-86"></a>    <span class="at">grid =</span> <span class="dv">50</span>,</span>
<span id="cb11-87"><a href="#cb11-87"></a>    <span class="at">metrics =</span> metrica</span>
<span id="cb11-88"><a href="#cb11-88"></a>  )</span>
<span id="cb11-89"><a href="#cb11-89"></a></span>
<span id="cb11-90"><a href="#cb11-90"></a><span class="co"># Rank dos melhores modelos -----------------------------------------------</span></span>
<span id="cb11-91"><a href="#cb11-91"></a>modelos_rank <span class="ot">&lt;-</span> tunagem <span class="sc">|&gt;</span> <span class="fu">rank_results</span>()</span>
<span id="cb11-92"><a href="#cb11-92"></a></span>
<span id="cb11-93"><a href="#cb11-93"></a>melhor_eqm <span class="ot">&lt;-</span> </span>
<span id="cb11-94"><a href="#cb11-94"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-95"><a href="#cb11-95"></a>  <span class="fu">extract_workflow_set_result</span>(<span class="st">"formula_eqm"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-96"><a href="#cb11-96"></a>  <span class="fu">select_best</span>(<span class="st">"rmse"</span>)</span>
<span id="cb11-97"><a href="#cb11-97"></a></span>
<span id="cb11-98"><a href="#cb11-98"></a>melhor_ridge <span class="ot">&lt;-</span> </span>
<span id="cb11-99"><a href="#cb11-99"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-100"><a href="#cb11-100"></a>  <span class="fu">extract_workflow_set_result</span>(<span class="st">"formula_ridge"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-101"><a href="#cb11-101"></a>  <span class="fu">select_best</span>(<span class="st">"rmse"</span>)</span>
<span id="cb11-102"><a href="#cb11-102"></a></span>
<span id="cb11-103"><a href="#cb11-103"></a>melhor_lasso <span class="ot">&lt;-</span> </span>
<span id="cb11-104"><a href="#cb11-104"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-105"><a href="#cb11-105"></a>  <span class="fu">extract_workflow_set_result</span>(<span class="st">"formula_lasso"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-106"><a href="#cb11-106"></a>  <span class="fu">select_best</span>(<span class="st">"rmse"</span>)</span>
<span id="cb11-107"><a href="#cb11-107"></a></span>
<span id="cb11-108"><a href="#cb11-108"></a>melhor_elastic <span class="ot">&lt;-</span> </span>
<span id="cb11-109"><a href="#cb11-109"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-110"><a href="#cb11-110"></a>  <span class="fu">extract_workflow_set_result</span>(<span class="st">"formula_elastic"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-111"><a href="#cb11-111"></a>  <span class="fu">select_best</span>(<span class="st">"rmse"</span>)</span>
<span id="cb11-112"><a href="#cb11-112"></a></span>
<span id="cb11-113"><a href="#cb11-113"></a>finalizando_eqm <span class="ot">&lt;-</span> </span>
<span id="cb11-114"><a href="#cb11-114"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-115"><a href="#cb11-115"></a>  <span class="fu">extract_workflow</span>(<span class="st">"formula_eqm"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-116"><a href="#cb11-116"></a>  <span class="fu">finalize_workflow</span>(melhor_eqm) <span class="sc">|&gt;</span> </span>
<span id="cb11-117"><a href="#cb11-117"></a>  <span class="fu">last_fit</span>(<span class="at">split =</span> hod_out)</span>
<span id="cb11-118"><a href="#cb11-118"></a></span>
<span id="cb11-119"><a href="#cb11-119"></a>finalizando_ridge <span class="ot">&lt;-</span> </span>
<span id="cb11-120"><a href="#cb11-120"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-121"><a href="#cb11-121"></a>  <span class="fu">extract_workflow</span>(<span class="st">"formula_ridge"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-122"><a href="#cb11-122"></a>  <span class="fu">finalize_workflow</span>(melhor_ridge) <span class="sc">|&gt;</span> </span>
<span id="cb11-123"><a href="#cb11-123"></a>  <span class="fu">last_fit</span>(<span class="at">split =</span> hod_out)</span>
<span id="cb11-124"><a href="#cb11-124"></a></span>
<span id="cb11-125"><a href="#cb11-125"></a>finalizando_lasso <span class="ot">&lt;-</span> </span>
<span id="cb11-126"><a href="#cb11-126"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-127"><a href="#cb11-127"></a>  <span class="fu">extract_workflow</span>(<span class="st">"formula_lasso"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-128"><a href="#cb11-128"></a>  <span class="fu">finalize_workflow</span>(melhor_lasso) <span class="sc">|&gt;</span> </span>
<span id="cb11-129"><a href="#cb11-129"></a>  <span class="fu">last_fit</span>(<span class="at">split =</span> hod_out)</span>
<span id="cb11-130"><a href="#cb11-130"></a></span>
<span id="cb11-131"><a href="#cb11-131"></a>finalizando_elastic <span class="ot">&lt;-</span> </span>
<span id="cb11-132"><a href="#cb11-132"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb11-133"><a href="#cb11-133"></a>  <span class="fu">extract_workflow</span>(<span class="st">"formula_elastic"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb11-134"><a href="#cb11-134"></a>  <span class="fu">finalize_workflow</span>(melhor_elastic) <span class="sc">|&gt;</span> </span>
<span id="cb11-135"><a href="#cb11-135"></a>  <span class="fu">last_fit</span>(<span class="at">split =</span> hod_out)</span>
<span id="cb11-136"><a href="#cb11-136"></a></span>
<span id="cb11-137"><a href="#cb11-137"></a><span class="co"># Visualizando as métricas</span></span>
<span id="cb11-138"><a href="#cb11-138"></a>finalizando_eqm <span class="sc">|&gt;</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb11-139"><a href="#cb11-139"></a>finalizando_ridge <span class="sc">|&gt;</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb11-140"><a href="#cb11-140"></a>finalizando_lasso <span class="sc">|&gt;</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb11-141"><a href="#cb11-141"></a>finalizando_elastic <span class="sc">|&gt;</span> <span class="fu">collect_metrics</span>()</span>
<span id="cb11-142"><a href="#cb11-142"></a></span>
<span id="cb11-143"><a href="#cb11-143"></a><span class="co"># Visualizando predições:</span></span>
<span id="cb11-144"><a href="#cb11-144"></a>finalizando_eqm <span class="sc">|&gt;</span> <span class="fu">collect_predictions</span>()</span>
<span id="cb11-145"><a href="#cb11-145"></a>finalizando_ridge <span class="sc">|&gt;</span> <span class="fu">collect_predictions</span>()</span>
<span id="cb11-146"><a href="#cb11-146"></a>finalizando_lasso <span class="sc">|&gt;</span> <span class="fu">collect_predictions</span>()</span>
<span id="cb11-147"><a href="#cb11-147"></a>finalizando_elastic <span class="sc">|&gt;</span> <span class="fu">collect_predictions</span>()</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="exercícios-2" class="slide level2">
<h2>📚 Exercícios</h2>
<p><br> <span class="red">Exercício</span>: Utilizando os dados de vinho vermelho🍷, disponíveis <a href="https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009">aqui</a>, faça uma pequena análise exploratória dos dados. No <a href="https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009">link</a> do Kaggle você consegue uma explicação sobre o que significa cada uma das variáveis.</p>

<img data-src="gifs/mr-bean-test.gif" width="211" class="r-stretch"></section>
<section id="exercícios-3" class="slide level2">
<h2>📚 Exercícios</h2>
<p><br></p>
<p><span class="red">Exercício</span>: Utilizando os dados de vinho vermelho🍷, disponíveis <a href="https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009">aqui</a>, obtenha o melhor modelo de regressão linar para modelar a qualidade do vinho, considerando:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><span class="math inline">g_1</span> - Método dos mínimos quadrados;</li>
<li class="fragment"><span class="math inline">g_2</span> - Regressão ridge;</li>
<li class="fragment"><span class="math inline">g_3</span> - Regressao lasso;</li>
<li class="fragment"><span class="math inline">g_4</span> - Elastic Net.</li>
</ol>
<p><br></p>
<p>Você deverá selecionar o melhor modelo de cada uma das classes de modelos de regressão e construir uma tabela com o risco estimado <span class="math inline">\widehat{R}(g_i),\,\, i = 1, \cdots, 4</span>, em que aqui <span class="math inline">g_i</span> representa o modelo geral não estimado. Ao fim, construa quatro gráficos mostrando o ajuste de cada um dos modelos.</p>
<p><br></p>
<p><span class="red">Exercício</span>: Se você utilizou o <a href="https://www.tidymodels.org/">tidymodels</a> para resolver o exercício anterior, rafaça usando a biblioteca <a href="https://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a>. Caso contrário, resolva-o utilizando o <a href="https://www.tidymodels.org/">tidymodels</a>.</p>
</section>
<section id="exercícios-4" class="slide level2">
<h2>📚 Exercícios</h2>
<p><br></p>
<p><span class="red">Exercício</span>: Considere agora o conjunto de dados de despesas médicas, disponível <a href="https://www.kaggle.com/datasets/mirichoi0218/insurance">aqui</a>, refaça o mesmo exercício dos dados de vinho vermelho, em que aqui, o objetivo é prever a variável <code>charges</code>. Perceba que algumas variáveis são qualitativas, e portão, você deverá transformá-las em <span class="red"><em>dummy</em></span>. Indique os melhores cenários dos quatro modelos e informe qual modelo você utilizaria. Explique!</p>
<p><br></p>

<img data-src="gifs/bean_simple.gif" class="r-stretch"></section>
<section id="métodos-não-paramétricos" class="slide level2">
<h2>Métodos não-paramétricos</h2>
<p><br></p>
<p>Métodos paramétricos podem impor muitas limitações na solução de um problema de regressão, i.e., em problemas que desejamos estimar a função de regressão <span class="math inline">r({\bf x})</span>. Por exemplo, nem sempre o melhor estimador linear é um bom estimador para <span class="math inline">r({\bf x})</span>.</p>
<p><br></p>
<p>Métodos paramétricos são muitas vezes simplistas e restritivos, em que normalmente abrimos mãos para se ter um estimador um pouco mais viesado, em detrimento da diminuição da variância do modelo. Por exemplo, nas regressões penalizadas que vimos anteriormente (<strong>ridge</strong>, <strong>lasso</strong> e <strong>elastic-net</strong>), penalizamos modelos com muitas covariáveis o que naturalmente aumentará o viés, na maioria das vezes.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="métodos-não-paramétricos-1" class="slide level2">
<h2>Métodos não-paramétricos</h2>
<p><br></p>
<p>Em situações em que temos muitos dados (<span class="math inline">n</span> grande), os modelos não-paramétricos possuem, em geral, boa peformance, uma vez que apesar de que nessa classe de modelos existir um aumento da variância, porém, seguida de uma redução de viés, a variância do modelo não aumenta muito.</p>
<p><br></p>
<p>De um lado, nos modelos de regressão que vimos até o momento, introduzimos uma penalização para diminuir a variância do modelo frente ao método dos mínimos quadrados (quando não usamos penalização) em troca de um aumento no víes. Aqui, em modelos não paramétricos, desejamos fazer a troca oposta, i.e., diminuir o viés, em troca de um ganho na variância no modelo.</p>
<p><br></p>
<p>Qualquer abordagem paramétrica trás consigo a possibilidade de que a forma funcional <span class="math inline">g</span> para estimar <span class="math inline">f</span> seja muito diferente da verdadeira, claro, se o modelo resultante não se ajustar bem aos dados, isto é <span class="math inline">\widehat{g}</span> não tem boa capacidade preditiva, muito embora, também é possível que tenhamos <span class="math inline">\widehat{g}</span> com boa capacidade preditiva, porém, não represente a estrutura real de <span class="math inline">f</span> (sempre desconhecida).</p>
</section>
<section id="métodos-não-paramétricos-2" class="slide level2">
<h2>Métodos não-paramétricos</h2>
<p><br> <br></p>
<p>Isso não é regra, porém ajuda a entender um pouco o dilema entre essas classes de modelos (paramétricos e não-paramétricos).</p>
<p><br></p>

<img data-src="imgs/vies_variancia_nao_parametrico.png" class="r-stretch quarto-figure-center"></section>
<section id="k-vizinhos-mais-próximo-próximos" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais próximo próximos</h2>
<p><br></p>
<p>Também chamado de <span class="red"><span class="math inline">k</span>-nearest neighbours - <span class="math inline">k</span>NN</span>, o <span class="math inline">k</span>NN é um dos métodos mais populares na comunidade de aprendizagem de máquina. O método foi formulado em uma sequência de dois artigos:</p>
<p><br></p>
<p>1 - Benedetti, J. K. (1977). <strong>On the nonparametric estimation of regression functions</strong>. Journal of the Royal Statistical Society. Series B (Methodological), 248–253;</p>
<p><br></p>
<p>2 - Stone, C. J. (1977). <strong>Consistent nonparametric regression</strong>. The Annals of Statistics, 595– 620.</p>
<p><br></p>
<p>A ideia do método é estimar a função de regressão <span class="math inline">r({\bf x})</span>, para um dado <span class="math inline">{\bf x}</span> com base nas respostas <span class="math inline">Y</span> dos <span class="math inline">k</span>-vizinhos mais próximos de <span class="math inline">{\bf x}</span>.</p>
</section>
<section id="k-vizinhos-mais-próximo-próximos-1" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais próximo próximos</h2>
<p><br></p>
<p>Formalmente, temos que</p>
<p><span class="math display">g({\bf x^*}) = \frac{1}{k}\sum_{i \in \mathcal{N}_{\bf x^*}}y_i,</span> em que <span class="math inline">\mathcal{N}_{\bf x}</span> é o conjunto índices das <span class="math inline">k</span> observações mais próximas de <span class="math inline">{\bf x}</span>, i.e,</p>
<p><span class="math display">\mathcal{N}_{\bf x^*} = \{i \in \{1, \cdots, n\}\, : \, d({\bf x}_i, {\bf x^*}) \leq d_{\bf x^*}^k\},</span> em que <span class="math inline">d_{\bf x^*}^k</span> é a distância do <span class="math inline">k</span>-ésimo vizinho mais próximo de <span class="math inline">\bf{x^*}</span> em <span class="math inline">\bf{x}</span>. Portanto, o valor da regressão no ponto <span class="math inline">{\bf x^*},</span> i.e., o valor de <span class="math inline">r({\bf x^*}) = \mathbb{E}(Y|{\bf X} = {\bf x^*})</span> é estimado pela média de <span class="math inline">Y_{N_{\bf x^*}}</span>. Ou seja, estimamos por:</p>
<p><span class="math display">\overline{Y}_{N_{\bf x^*}} = \frac{1}{k} \sum_{i \in \mathcal{N}_{\bf x^*}}y_i.</span></p>
</section>
<section id="k-vizinhos-mais-próximo-próximos-2" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais próximo próximos</h2>
<p><br> <!-- http://leg.ufpr.br/~lucambio/MSM/MSM03.html --> Para determinar <span class="math inline">d_{\bf x^*}^k</span>, poderemos utilizar alguma métrica de distância e assim mensurarmos a proximidade. Entre elas:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><span class="red">Distância Euclidiana</span> ou distância <span class="math inline">L_2</span>: <span class="math inline">d(x^a, x^b) = \sqrt{(x_1^a - x_1^b)^2 + \cdots + (x_d^a - x_d^b)^2}</span>;</li>
</ol>
<p><br></p>
<ol start="2" type="1">
<li class="fragment"><span class="red">Distância de Manhattan</span>, City Block ou distância <span class="math inline">L_1</span>: <span class="math inline">d(x^a, x^b) = \sqrt{|x_1^a - x_1^b| + \cdots + |x_d^a - x_d^b|}</span>;</li>
</ol>
<p><br></p>
<ol start="2" type="1">
<li class="fragment"><span class="red">Distância de Mahalanobis</span>: <span class="math inline">d(x^a, x^b) = \sqrt{(x^a - x^b)^{T}S^{-1}(x^a - x^b)}</span>, em que <span class="math inline">S</span> é a matriz de covariância, em que na diagonal princial temos as variâncias e fora dela as covariâncias entre os pontos. Lembre-se que se <span class="math inline">x</span> e <span class="math inline">y</span> são vetores de dados quaisquer, a interdependência linear entre eles poderá ser estimada como:</li>
</ol>
<p><br></p>
<p><span class="math display">\mathrm{cov}(x, y) = \frac{1}{n-1}\sum_{i = 1}^n (x_i - \overline{x})(y_i - \overline{y}).</span></p>
</section>
<section id="k-vizinhos-mais-próximo-próximos-3" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais próximo próximos</h2>
<p><br></p>
<p>Você poderá utilizar qualquer outra medida de distância além das que foram citadas acima.</p>
<p><br></p>
<p>É importante perceber que <strong>o método</strong> <span class="math inline">k</span>NN não faz uma “compressão” dos dados como a regressão linear que estudamos. Lá, temos uma equação que utilizamos para estimar o valor de <span class="math inline">Y</span>, após as estimação dos coeficientes do modelo de regressão, ou seja, não precisamos mais dos dados para estimar novas observações. Já no <span class="math inline">k</span>NN, precisamos sempre nos recorrer aos dados para fazer novas predições, ou seja, sempre que desejarmos calcular <span class="math inline">r({\bf x}) = \mathbb{E}(Y|{\bf X} = {\bf x})</span> deveremos sempre fazer uma nova consulta aos dados para calcular a média dos vizinhos mais próximos. O <span class="math inline">k</span>NN não possui coeficientes para interpretar. Diferentemente da regressão linear, o <span class="math inline">k</span>NN é um pouco mais “<em>black box</em>”.</p>
<p><br></p>

<img data-src="gifs/giphy.gif" class="r-stretch"></section>
<section id="k-vizinhos-mais-próximo-próximos-4" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais próximo próximos</h2>
<p><br> O valor da constante <span class="math inline">k</span> é um hiperparâmetro do <span class="math inline">k</span>NN e deverá ser obtido por validação cruzada. Perceba que se <span class="math inline">k = n</span> temos um modelo muito viesado, porém com variância pequena. Isso, porquê para <span class="math inline">k = n</span> basicamente iremos tirar uma média dos dados. Para <span class="math inline">k = 1</span>, teremos um <em>overfitting</em>, uma vez que o estimador irá interpolar os dados.</p>
<p><br> <span class="red">Exemplo</span>: Considere novamente o conjunto de dados de expectativa de vida versus PIB per Capita disponíveis <a href="https://github.com/prdm0/dados/blob/main/dados_expectativa_renda.RData">aqui</a>. Utilizando o <a href="https://www.tidymodels.org/">tidymodels</a>, vamos construir uma fução que retorne um gráfico com as estimativas do <span class="math inline">k</span>NN. A função receberá como argumentos o conjunto de dados e o valor de <span class="math inline">k</span>. Você também poderia utilizar outras bibliotecas, como por exemplo a <a href="https://cran.r-project.org/web/packages/FNN/index.html">FNN</a>, ou a <a href="https://github.com/KlausVigo/kknn">KKNN</a>, esta útlima é a que é utilizada internamente na biblioteca <a href="https://parsnip.tidymodels.org/">parsnip</a> do <a href="https://www.tidymodels.org/">tidymodels</a>.</p>
<p><br></p>

<img data-src="imgs/parsnip.png" class="r-stretch"></section>
<section id="k-vizinhos-mais-próximo-próximos-5" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais próximo próximos</h2>
<p><br></p>
<p>Abrindo apenas um parêntese, o <a href="https://www.tidymodels.org/">tidymodels</a> refere-se a um conjunto de pacotes que são úteis para o tratamento, treinamento, tunagem e avaliação de modelos de aprendizagem de máquina, em que o <a href="https://parsnip.tidymodels.org/">parsnip</a> implementa as <em>engines</em> (algoritmos/motores) que iremos utilizar para treinar um modelo. Na verdade, os algoritmos estão implementados em pacotes de terceiros e não precisamente no <a href="https://parsnip.tidymodels.org/">parsnip</a>. Porém, o <a href="https://parsnip.tidymodels.org/">parsnip</a> unifica a sintaxe de diversos algoritmos implementados em pacotes separados.</p>
<p><br></p>

<img data-src="gifs/thumbs-up-nod.gif" class="r-stretch"></section>
<section id="k-vizinhos-mais-próximo-próximos-6" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais próximo próximos</h2>
<p><br></p>
<p>Observe que na imagem abaixo, a Figura <span class="red">A</span>, quando <span class="math inline">k=1</span>, percebemos initidamente que houve <em>overfitting</em>, i.e., há uma interpolação dos dados. Já na Figura <span class="red">D</span> temos um modelo com variância menor, porém, este é muito simplista, o que sugere um alto viés.</p>
<p><br></p>

<img data-src="imgs/knn_plot.png" class="r-stretch quarto-figure-center"></section>
<section id="k-vizinhos-mais-próximo-próximos-7" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais próximo próximos</h2>
<p><br></p>
<p>Estude o código! Ele fornece a solução para o exemplo.</p>
<p><br></p>
<div class="cell">
<details>
<summary>Solução pelo método knn do exemplo anterior</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="fu">library</span>(glue)</span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb12-6"><a href="#cb12-6"></a></span>
<span id="cb12-7"><a href="#cb12-7"></a>tidymodels<span class="sc">::</span><span class="fu">tidymodels_prefer</span>()</span>
<span id="cb12-8"><a href="#cb12-8"></a></span>
<span id="cb12-9"><a href="#cb12-9"></a><span class="co"># Lendo dados</span></span>
<span id="cb12-10"><a href="#cb12-10"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb12-11"><a href="#cb12-11"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb12-12"><a href="#cb12-12"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb12-13"><a href="#cb12-13"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb12-14"><a href="#cb12-14"></a></span>
<span id="cb12-15"><a href="#cb12-15"></a>dados <span class="ot">&lt;-</span> </span>
<span id="cb12-16"><a href="#cb12-16"></a>  dados_expectativa_renda <span class="sc">|&gt;</span> </span>
<span id="cb12-17"><a href="#cb12-17"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>CountryName) <span class="sc">|&gt;</span> </span>
<span id="cb12-18"><a href="#cb12-18"></a>  dplyr<span class="sc">::</span><span class="fu">rename</span>(<span class="at">y =</span> LifeExpectancy, <span class="at">x =</span> GDPercapita)</span>
<span id="cb12-19"><a href="#cb12-19"></a></span>
<span id="cb12-20"><a href="#cb12-20"></a>knn_exp_pip <span class="ot">&lt;-</span> <span class="cf">function</span>(dados, <span class="at">k =</span> 1L){</span>
<span id="cb12-21"><a href="#cb12-21"></a>  <span class="co"># Criando receita</span></span>
<span id="cb12-22"><a href="#cb12-22"></a>  receita <span class="ot">&lt;-</span> <span class="fu">recipe</span>(y <span class="sc">~</span> x, <span class="at">data =</span> dados)</span>
<span id="cb12-23"><a href="#cb12-23"></a>  </span>
<span id="cb12-24"><a href="#cb12-24"></a>  <span class="co"># Definindo o modelo</span></span>
<span id="cb12-25"><a href="#cb12-25"></a>  modelo_knn <span class="ot">&lt;-</span> <span class="fu">nearest_neighbor</span>(<span class="at">neighbors =</span> k) <span class="sc">|&gt;</span> </span>
<span id="cb12-26"><a href="#cb12-26"></a>    <span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb12-27"><a href="#cb12-27"></a>    <span class="fu">set_engine</span>(<span class="st">"kknn"</span>)</span>
<span id="cb12-28"><a href="#cb12-28"></a>  </span>
<span id="cb12-29"><a href="#cb12-29"></a>  <span class="co"># Workflow</span></span>
<span id="cb12-30"><a href="#cb12-30"></a>  ajuste_final <span class="ot">&lt;-</span> </span>
<span id="cb12-31"><a href="#cb12-31"></a>    <span class="fu">workflow</span>() <span class="sc">|&gt;</span> </span>
<span id="cb12-32"><a href="#cb12-32"></a>    <span class="fu">add_model</span>(modelo_knn) <span class="sc">|&gt;</span> </span>
<span id="cb12-33"><a href="#cb12-33"></a>    <span class="fu">add_recipe</span>(receita) <span class="sc">|&gt;</span> </span>
<span id="cb12-34"><a href="#cb12-34"></a>    <span class="fu">fit</span>(<span class="at">data =</span> dados)</span>
<span id="cb12-35"><a href="#cb12-35"></a>  </span>
<span id="cb12-36"><a href="#cb12-36"></a>  <span class="co"># Retornando previsoes</span></span>
<span id="cb12-37"><a href="#cb12-37"></a>  y_chapeu <span class="ot">&lt;-</span> <span class="fu">predict</span>(ajuste_final, <span class="at">new_data =</span> dados)</span>
<span id="cb12-38"><a href="#cb12-38"></a>  </span>
<span id="cb12-39"><a href="#cb12-39"></a>  dados <span class="ot">&lt;-</span> </span>
<span id="cb12-40"><a href="#cb12-40"></a>    dados <span class="sc">|&gt;</span> </span>
<span id="cb12-41"><a href="#cb12-41"></a>    <span class="fu">mutate</span>(<span class="at">y_chapeu =</span> y_chapeu<span class="sc">$</span>.pred)</span>
<span id="cb12-42"><a href="#cb12-42"></a>  </span>
<span id="cb12-43"><a href="#cb12-43"></a>  dados <span class="sc">|&gt;</span> </span>
<span id="cb12-44"><a href="#cb12-44"></a>    <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb12-45"><a href="#cb12-45"></a>    <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb12-46"><a href="#cb12-46"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y_chapeu), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb12-47"><a href="#cb12-47"></a>    <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"k-nearest neighbours"</span>, <span class="at">subtitle =</span> <span class="fu">glue</span>(<span class="st">"k = {k}"</span>)) <span class="sc">+</span></span>
<span id="cb12-48"><a href="#cb12-48"></a>    <span class="fu">theme</span>(</span>
<span id="cb12-49"><a href="#cb12-49"></a>      <span class="at">title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb12-50"><a href="#cb12-50"></a>    )</span>
<span id="cb12-51"><a href="#cb12-51"></a>}</span>
<span id="cb12-52"><a href="#cb12-52"></a></span>
<span id="cb12-53"><a href="#cb12-53"></a>p1 <span class="ot">&lt;-</span> <span class="fu">knn_exp_pip</span>(dados, <span class="at">k =</span> 1L)</span>
<span id="cb12-54"><a href="#cb12-54"></a>p2 <span class="ot">&lt;-</span> <span class="fu">knn_exp_pip</span>(dados, <span class="at">k =</span> 7L)</span>
<span id="cb12-55"><a href="#cb12-55"></a>p3 <span class="ot">&lt;-</span> <span class="fu">knn_exp_pip</span>(dados, <span class="at">k =</span> 10L)</span>
<span id="cb12-56"><a href="#cb12-56"></a>p4 <span class="ot">&lt;-</span> <span class="fu">knn_exp_pip</span>(dados, <span class="at">k =</span> 200L)</span>
<span id="cb12-57"><a href="#cb12-57"></a></span>
<span id="cb12-58"><a href="#cb12-58"></a>p <span class="ot">&lt;-</span> p1 <span class="sc">+</span> p2 <span class="sc">+</span> p3 <span class="sc">+</span> p4 <span class="sc">+</span> <span class="fu">plot_annotation</span>(<span class="at">tag_levels =</span> <span class="st">"A"</span>)</span>
<span id="cb12-59"><a href="#cb12-59"></a></span>
<span id="cb12-60"><a href="#cb12-60"></a><span class="fu">ggsave</span>(p, <span class="at">file =</span> <span class="st">"imgs/knn_plot.png"</span>, <span class="at">width =</span> <span class="dv">50</span>, <span class="at">height =</span> <span class="dv">30</span>, <span class="at">units =</span> <span class="st">"cm"</span>)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><br></p>

<img data-src="gifs/chapulin-colorado-no.gif" style="width:25.0%" class="r-stretch"></section>
<section id="k-vizinhos-mais-próximo-próximos-8" class="slide level2">
<h2><span class="math inline">k</span>-vizinhos mais próximo próximos</h2>
<p><br></p>
<p><strong>Algumas limitações do</strong> <span class="math inline">k</span>NN são:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>É totalmente dependente do conjunto de dados para fazer novas predições;</p></li>
<li class="fragment"><p>Para novas observações que são fora dos limites dos dados de treinamento, as predições do <span class="math inline">k</span>NN tendem a serem imprecisas;</p></li>
<li class="fragment"><p>Pode ser custoso para uma grande base de dados;</p></li>
<li class="fragment"><p>O cálculo de distâncias pode sofrer com a chamada “maldição da dimensionalidade”, em que a depender da métrica de distância utilizada, tudo fica muito distante.</p></li>
</ol>
<p><br></p>
<p><strong>Algumas vantagens são</strong>:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>É um método simples de ser implementado;</p></li>
<li class="fragment"><p>É muito utilizado para imputação de observações faltantes;</p></li>
<li class="fragment"><p>É comumente utilizado, por conta de sua simplicidade, em explorações iniciais.</p></li>
</ol>
</section>
<section id="exercícios-5" class="slide level2">
<h2>📚 Exercícios</h2>
<p><br></p>
<p><span class="red">Exercício</span>: Considere o problema em que o objetivo é prever a <strong>pontuação</strong> (<em>score</em>) / (item 2) do aluno com base em algumas variáveis. São elas:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Horas estudadas</strong>: o número total de horas gastas estudando por cada aluno;</li>
<li class="fragment"><strong>Pontuação</strong>: As notas obtidas pelos alunos em testes anteriores;</li>
<li class="fragment"><strong>Atividades extracurriculares</strong>: Se o aluno participa de atividades extracurriculares (Sim ou Não);</li>
<li class="fragment"><strong>Horas de sono</strong>: o número médio de horas de sono que o aluno teve por dia;</li>
<li class="fragment"><strong>Amostras de perguntas praticadas</strong>: O número de amostras de perguntas que o aluno praticou.</li>
</ol>
<p><br></p>
<p>Você poderá baixar e ter uma descrição maior da base de dados clicando <a href="https://www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression">aqui</a>. Avalie o poder preditivo do modelo lasso com o do <span class="math inline">k</span>NN. Sua análise deve conter uma análise exploratória dos dados, deverá utilizar um esquema de <span class="math inline">k</span>-<em>folds cross-validation</em>, com <span class="math inline">k = 20</span> e considerar um esquema de <em>data splitting</em> na proporção <span class="math inline">90\%</span> para treino e <span class="math inline">10\%</span> para teste. Sua análise deverá estar em um notebook de <a href="https://quarto.org/">quarto</a>, em que você deverá comentar cada passo da análise.</p>
</section>
<section id="nadaraya-watson" class="slide level2">
<h2>Nadaraya-Watson</h2>
<p><br></p>
<p>Uma variação do método <span class="math inline">k</span>NN que é bastante difundida na comunidade estatística é o método de Nadaraya-Watson, propostos nos artigos:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>Nadaraya, E. A. (1964). <strong>On estimating regression</strong>. Theory of Probability &amp; Its Applications, 9(1), 141–142;</p></li>
<li class="fragment"><p>Watson, G. S. (1964). <strong>Smooth regression analysis</strong>. Sankhya: The Indian Journal of Statistics, Series A, 359–372.</p></li>
</ol>
<p><br></p>
<p>Esse método também é chamado de estimador <span class="math inline">k</span>-vizinhos ponderados (<span class="math inline">k</span>NN ponderado), uma vez que a estimação da função de regressão em um dado ponto <span class="math inline">{\bf x}</span> utiliza de médias ponderadas das observações do conjunto de treinamento:</p>
<p><span class="math display">g({\bf x}) = \sum_{i = 1}^n w_i({\bf x})y_i,</span> em que <span class="math inline">w_i({\bf x})</span> é o peso atribuído à <span class="math inline">i</span>-ésima observação, medindo a similaridade de <span class="math inline">{\bf x}_i</span> à <span class="math inline">{\bf x}</span>.</p>
</section>
<section id="nadaraya-watson-1" class="slide level2">
<h2>Nadaraya-Watson</h2>
<p><br></p>
<p>Temos que <span class="math inline">w_i({\bf x})</span> é definido por:</p>
<p><span class="math display">w_i({\bf x}) = \frac{K({\bf x}, {\bf x}_i)}{\sum_{j = 1}^n K({\bf x}, {\bf x}_j)},</span> em que <span class="math inline">K({\bf x}, {\bf x}_j)</span> é um kernel de suavização usado para medir a similaridade entre as observações. Escolhas que são populares para <span class="math inline">K({\bf x}, {\bf x}_j)</span>, são:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><span class="red">Kernel uniforme</span>: <span class="math inline">K({\bf x}, {\bf x}_i) = \mathbb{I}(d({\bf x}, {\bf x}_i) \leq h)</span>;</li>
<li class="fragment"><span class="red">Kernel gaussiano</span>: <span class="math inline">K({\bf x}, {\bf x}_i) = (\sqrt{2\pi h^2})^{-1}\exp\left\{ -\frac{d^2({\bf x}, {\bf x}_i)}{2h^2}\right\}</span>;</li>
<li class="fragment"><span class="red">Kernel triangular</span>: <span class="math inline">K({\bf x}, {\bf x}_i) = (1 - d({\bf x}, {\bf x}_i)/h)\mathbb{I}(d({\bf x}, {\bf x}_i) \leq h)</span>;</li>
<li class="fragment"><span class="red">Kernel de Epanechnikov</span>: <span class="math inline">K({\bf x}, {\bf x}_i) = (1 - d^2({\bf x}, {\bf x}_i)/h^2)\mathbb{I}(d({\bf x}, {\bf x}_i) \leq h)</span>.</li>
</ol>
</section>
<section id="nadaraya-watson-2" class="slide level2">
<h2>Nadaraya-Watson</h2>
<p><br></p>
<p>Enquanto no kernel uniforme, os pesos são iguais para as observações a uma distância menor que <span class="math inline">h</span> de <span class="math inline">{\bf x}</span> e atribui peso zero para observações maiores que <span class="math inline">h</span>, os kernels triangular e de Epanechnikov atribui pesos maiores para observações mais próximas de <span class="math inline">{\bf x}</span>. A quantidade <span class="math inline">h</span> é um <span class="red"><em>tuning parameter</em></span>, e na prática, deve ser estimada por um procedimento de validação cruzada.</p>
<p><br></p>
<p><strong>Algumas propriedades de uma função kernel são</strong>:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Simetria</strong>: <span class="math inline">K(x,y) = K(y,x)</span>, permitindo que a função de similaridade seja invariante em relação a ordem dos argumentos;</li>
<li class="fragment"><strong>Positiva definida</strong>: Para qualquer vetor <span class="math inline">c</span>, em que seja possível fazer <span class="math inline">c^{T}K(x,y)</span>, temos que <span class="math inline">c^{T}K(x,y)c &gt; 0</span>.</li>
</ol>
<p><br></p>
<p>Você poderá encontrar outras funções kernel <a href="https://en.wikipedia.org/wiki/Kernel_(statistics)">aqui</a>.</p>
</section>
<section id="nadaraya-watson-3" class="slide level2">
<h2>Nadaraya-Watson</h2>
<p><br></p>
<p><span class="red">Exemplo</span>: Novamente, considerando o conjunto de dados de expectativa de vida versus PIB per Capita disponíveis <a href="https://github.com/prdm0/dados/blob/main/dados_expectativa_renda.RData">aqui</a>. Utilizando o <a href="https://www.tidymodels.org/">tidymodels</a>, vamos construir uma fução que retorne um gráfico com as estimativas. A função receberá como argumentos o conjunto de dados e o valor de <span class="math inline">h</span>. Observe a documentação da função <code>nearest_neighbor</code> do pacote <a href="https://parsnip.tidymodels.org/">parsnip</a>. Perceba que o argumento <code>weight_func</code> permite que possamos escolher entre algumas funções kernel. Porém, como métrica de distâncias, apenas poderemos utilizar a de Minkowski. Seja <span class="math inline">x = (x_1, \cdots, x_n)</span> e <span class="math inline">y = (y_1, \cdots, y_n)</span>, ambos vetores do <span class="math inline">\mathbb{R}^n</span>. Então, a distância de Minkowski é dada por:</p>
<p><span class="math display">d(x,y) = \left(\sum_{i = 1}^n |x_i - y_i|^p\right)^{\frac{1}{p}},</span> com <span class="math inline">p \geq 1</span>. Note que se <span class="math inline">p = 1</span> temos a distância euclidiana (distância <span class="math inline">L_1</span>) e se <span class="math inline">p = 2</span> teremos a distancia de Manhattan (distância <span class="math inline">L_2</span>).</p>
</section>
<section id="nadaraya-watson-4" class="slide level2">
<h2>Nadaraya-Watson</h2>
<p><br></p>
<p>Portanto, com o argumento <code>dist_power</code> da função <code>nearest_neighbor</code>, do pacote <a href="https://parsnip.tidymodels.org/">parsnip</a>, você poderá especificar o valor de <span class="math inline">p</span>, que inclusive poderá ser um hiperparâmetro, podendo ser obtido por meio de uma validação cruzada.</p>
<p><br></p>

<img data-src="gifs/mr-bean-pivot.gif" class="r-stretch"><p><br></p>
<p>No exemplo não iremos fazer a validação cruzada, pois apenas queremos implementar uma função em que seja possível experimentar o método para diferentes valores de <span class="math inline">h</span> e diferentes funções de kernel.</p>
</section>
<section id="nadaraya-watson-5" class="slide level2">
<h2>Nadaraya-Watson</h2>
<p><br> A imagem abaixo utiliza o kernel gaussiano:</p>
<p><br></p>

<img data-src="imgs/nadaraya_watson.png" class="r-stretch quarto-figure-center"></section>
<section id="nadaraya-watson-6" class="slide level2">
<h2>Nadaraya-Watson</h2>
<p><br></p>
<div class="cell">
<details>
<summary>Solução do exempĺo de Nadaraya-Watson</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="fu">library</span>(glue)</span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb13-6"><a href="#cb13-6"></a></span>
<span id="cb13-7"><a href="#cb13-7"></a>tidymodels<span class="sc">::</span><span class="fu">tidymodels_prefer</span>()</span>
<span id="cb13-8"><a href="#cb13-8"></a></span>
<span id="cb13-9"><a href="#cb13-9"></a></span>
<span id="cb13-10"><a href="#cb13-10"></a><span class="co"># Lendo dados</span></span>
<span id="cb13-11"><a href="#cb13-11"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb13-12"><a href="#cb13-12"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb13-13"><a href="#cb13-13"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb13-14"><a href="#cb13-14"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb13-15"><a href="#cb13-15"></a></span>
<span id="cb13-16"><a href="#cb13-16"></a>dados <span class="ot">&lt;-</span> </span>
<span id="cb13-17"><a href="#cb13-17"></a>  dados_expectativa_renda <span class="sc">|&gt;</span> </span>
<span id="cb13-18"><a href="#cb13-18"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>CountryName) <span class="sc">|&gt;</span> </span>
<span id="cb13-19"><a href="#cb13-19"></a>  dplyr<span class="sc">::</span><span class="fu">rename</span>(<span class="at">y =</span> LifeExpectancy, <span class="at">x =</span> GDPercapita)</span>
<span id="cb13-20"><a href="#cb13-20"></a></span>
<span id="cb13-21"><a href="#cb13-21"></a>nadaraya_watson_exp_pip <span class="ot">&lt;-</span> <span class="cf">function</span>(dados, <span class="at">h =</span> <span class="dv">1</span>, ...){</span>
<span id="cb13-22"><a href="#cb13-22"></a>  <span class="co"># Criando receita</span></span>
<span id="cb13-23"><a href="#cb13-23"></a>  receita <span class="ot">&lt;-</span> </span>
<span id="cb13-24"><a href="#cb13-24"></a>    <span class="fu">recipe</span>(y <span class="sc">~</span> x, <span class="at">data =</span> dados) <span class="sc">|&gt;</span> </span>
<span id="cb13-25"><a href="#cb13-25"></a>    <span class="fu">step_normalize</span>()</span>
<span id="cb13-26"><a href="#cb13-26"></a>  </span>
<span id="cb13-27"><a href="#cb13-27"></a>  <span class="co"># Definindo o modelo</span></span>
<span id="cb13-28"><a href="#cb13-28"></a>  modelo_knn <span class="ot">&lt;-</span> <span class="fu">nearest_neighbor</span>(<span class="at">dist_power =</span> h, ...) <span class="sc">|&gt;</span> </span>
<span id="cb13-29"><a href="#cb13-29"></a>    <span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb13-30"><a href="#cb13-30"></a>    <span class="fu">set_engine</span>(<span class="st">"kknn"</span>)</span>
<span id="cb13-31"><a href="#cb13-31"></a>  </span>
<span id="cb13-32"><a href="#cb13-32"></a>  <span class="co"># Workflow</span></span>
<span id="cb13-33"><a href="#cb13-33"></a>  ajuste_final <span class="ot">&lt;-</span> </span>
<span id="cb13-34"><a href="#cb13-34"></a>    <span class="fu">workflow</span>() <span class="sc">|&gt;</span> </span>
<span id="cb13-35"><a href="#cb13-35"></a>    <span class="fu">add_model</span>(modelo_knn) <span class="sc">|&gt;</span> </span>
<span id="cb13-36"><a href="#cb13-36"></a>    <span class="fu">add_recipe</span>(receita) <span class="sc">|&gt;</span> </span>
<span id="cb13-37"><a href="#cb13-37"></a>    <span class="fu">fit</span>(<span class="at">data =</span> dados)</span>
<span id="cb13-38"><a href="#cb13-38"></a>  </span>
<span id="cb13-39"><a href="#cb13-39"></a>  <span class="co"># Retornando previsoes</span></span>
<span id="cb13-40"><a href="#cb13-40"></a>  y_chapeu <span class="ot">&lt;-</span> <span class="fu">predict</span>(ajuste_final, <span class="at">new_data =</span> dados)</span>
<span id="cb13-41"><a href="#cb13-41"></a>  </span>
<span id="cb13-42"><a href="#cb13-42"></a>  dados <span class="ot">&lt;-</span> </span>
<span id="cb13-43"><a href="#cb13-43"></a>    dados <span class="sc">|&gt;</span> </span>
<span id="cb13-44"><a href="#cb13-44"></a>    <span class="fu">mutate</span>(<span class="at">y_chapeu =</span> y_chapeu<span class="sc">$</span>.pred)</span>
<span id="cb13-45"><a href="#cb13-45"></a>  </span>
<span id="cb13-46"><a href="#cb13-46"></a>  dados <span class="sc">|&gt;</span> </span>
<span id="cb13-47"><a href="#cb13-47"></a>    <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb13-48"><a href="#cb13-48"></a>    <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb13-49"><a href="#cb13-49"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y_chapeu), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb13-50"><a href="#cb13-50"></a>    <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Nadaraya-Watson"</span>, <span class="at">subtitle =</span> <span class="fu">glue</span>(<span class="st">"h = {h}"</span>)) <span class="sc">+</span></span>
<span id="cb13-51"><a href="#cb13-51"></a>    <span class="fu">theme</span>(</span>
<span id="cb13-52"><a href="#cb13-52"></a>      <span class="at">title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb13-53"><a href="#cb13-53"></a>    )</span>
<span id="cb13-54"><a href="#cb13-54"></a>}</span>
<span id="cb13-55"><a href="#cb13-55"></a></span>
<span id="cb13-56"><a href="#cb13-56"></a>p1 <span class="ot">&lt;-</span> <span class="fu">nadaraya_watson_exp_pip</span>(dados, <span class="at">h =</span> <span class="dv">1</span>, <span class="at">weight_func =</span> <span class="st">"gaussian"</span>)</span>
<span id="cb13-57"><a href="#cb13-57"></a>p2 <span class="ot">&lt;-</span> <span class="fu">nadaraya_watson_exp_pip</span>(dados, <span class="at">h =</span> <span class="dv">1000</span>, <span class="at">weight_func =</span> <span class="st">"gaussian"</span>)</span>
<span id="cb13-58"><a href="#cb13-58"></a></span>
<span id="cb13-59"><a href="#cb13-59"></a>p <span class="ot">&lt;-</span> p1 <span class="sc">+</span> p2 <span class="sc">+</span> <span class="fu">plot_annotation</span>(<span class="at">tag_levels =</span> <span class="st">"A"</span>)</span>
<span id="cb13-60"><a href="#cb13-60"></a></span>
<span id="cb13-61"><a href="#cb13-61"></a><span class="fu">ggsave</span>(p, <span class="at">file =</span> <span class="st">"imgs/nadaraya_watson.png"</span>, <span class="at">width =</span> <span class="dv">30</span>, <span class="at">height =</span> <span class="dv">15</span>, <span class="at">units =</span> <span class="st">"cm"</span>)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><br></p>

<img data-src="gifs/side-eyeing-chloe-chloe.gif" class="r-stretch"></section>
<section id="tidymodels-knn-e-nadaraya-watson" class="slide level2">
<h2>Tidymodels <span class="math inline">k</span>NN e Nadaraya-Watson</h2>
<p><br></p>
<p>Para que possamos experimentar o <a href="https://www.tidymodels.org/">tidymodels</a> seguindo todo o fluxo de “padrão” de aprendizagem de máquina, com divisão do conjunto de dados, validação cruzada e busca pelos melhores hiperparâmetros (“tunagem”). Iremos reproduir dois exemplos com os dados de expectativa de vida e PIB per Capita.</p>
<p><br></p>
<p><span class="red">Exemplo</span>: Nesse exemplo estamos realizando o <span class="math inline">k</span>NN, em que estamos obtendo um candidato para o valor de <span class="math inline">k</span>, por meio de um procedimento de <em>cross-validation</em> (10-<em>folds cross-validation</em>). Aqui, a busca do hiperparâmetro <span class="math inline">k</span> fará uso de um <em>grid search</em>. Perceba, no código que segue o exemplo, que no processo de “tunagem”, alterei o grid de valores usando a função <code>grid_max_entropy</code> do pacote <a href="https://dials.tidymodels.org/reference/grid_max_entropy.html">dails</a>. Uma observação é que você poderia criar uma tibble com valores do seu interesse. O argumento <code>grid</code> da função <code>tune_grid</code> do pacote <a href="https://tune.tidymodels.org/reference/tune_grid.html">tune</a> deve ser um data frame/tibble ou um número inteiro. Esse objeto conterá todas as possíveis combinações de hiperparâmetros que serão testadas na validação cruzada, no nosso caso, temos apenas um hiperparâmetro. Foi utilizado um <em>grid</em> de tamanho 60. Foi utilizado uma divisão de <span class="math inline">80\%</span> para treinamento e <span class="math inline">20\%</span> para teste.</p>
</section>
<section id="tidymodels-knn-e-nadaraya-watson-1" class="slide level2">
<h2>Tidymodels <span class="math inline">k</span>NN e Nadaraya-Watson</h2>
<p><br></p>
<div class="cell">
<details>
<summary>Workflow completo do treimanento de um modelo KNN.</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb14-3"><a href="#cb14-3"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb14-4"><a href="#cb14-4"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb14-5"><a href="#cb14-5"></a></span>
<span id="cb14-6"><a href="#cb14-6"></a><span class="co"># Resolvendo eventuais conflitos entre tidymodels e outros pacotes eventualmente</span></span>
<span id="cb14-7"><a href="#cb14-7"></a><span class="co"># carregados:</span></span>
<span id="cb14-8"><a href="#cb14-8"></a>tidymodels<span class="sc">::</span><span class="fu">tidymodels_prefer</span>()</span>
<span id="cb14-9"><a href="#cb14-9"></a></span>
<span id="cb14-10"><a href="#cb14-10"></a><span class="co"># Lendo a base de dados:</span></span>
<span id="cb14-11"><a href="#cb14-11"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb14-12"><a href="#cb14-12"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb14-13"><a href="#cb14-13"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb14-14"><a href="#cb14-14"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb14-15"><a href="#cb14-15"></a></span>
<span id="cb14-16"><a href="#cb14-16"></a>dados_expectativa_renda <span class="ot">&lt;-</span></span>
<span id="cb14-17"><a href="#cb14-17"></a>  dados_expectativa_renda <span class="sc">|&gt;</span></span>
<span id="cb14-18"><a href="#cb14-18"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>CountryName)</span>
<span id="cb14-19"><a href="#cb14-19"></a></span>
<span id="cb14-20"><a href="#cb14-20"></a><span class="co"># Setando uma semente</span></span>
<span id="cb14-21"><a href="#cb14-21"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb14-22"><a href="#cb14-22"></a></span>
<span id="cb14-23"><a href="#cb14-23"></a><span class="co"># Divisão da base de dados ------------------------------------------------</span></span>
<span id="cb14-24"><a href="#cb14-24"></a><span class="co"># Divisão inicial (treino e teste)</span></span>
<span id="cb14-25"><a href="#cb14-25"></a>splits <span class="ot">&lt;-</span> </span>
<span id="cb14-26"><a href="#cb14-26"></a>  rsample<span class="sc">::</span><span class="fu">initial_split</span>(</span>
<span id="cb14-27"><a href="#cb14-27"></a>    dados_expectativa_renda,</span>
<span id="cb14-28"><a href="#cb14-28"></a>    <span class="at">strata =</span> <span class="st">"LifeExpectancy"</span>, </span>
<span id="cb14-29"><a href="#cb14-29"></a>    <span class="at">prop =</span> <span class="fl">0.8</span> </span>
<span id="cb14-30"><a href="#cb14-30"></a>  )</span>
<span id="cb14-31"><a href="#cb14-31"></a></span>
<span id="cb14-32"><a href="#cb14-32"></a><span class="co"># O conjunto de dados de treinamento será utilizado para ajustar/treinar o</span></span>
<span id="cb14-33"><a href="#cb14-33"></a><span class="co"># modelo:</span></span>
<span id="cb14-34"><a href="#cb14-34"></a>treinamento <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(splits)</span>
<span id="cb14-35"><a href="#cb14-35"></a></span>
<span id="cb14-36"><a href="#cb14-36"></a><span class="co"># O conjunto de teste será utilizado apenas no fim, para avaliar o desempenho</span></span>
<span id="cb14-37"><a href="#cb14-37"></a><span class="co"># preditivo final do modelo:</span></span>
<span id="cb14-38"><a href="#cb14-38"></a>teste <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">testing</span>(splits) </span>
<span id="cb14-39"><a href="#cb14-39"></a></span>
<span id="cb14-40"><a href="#cb14-40"></a><span class="co"># Criando uma receita para os dados ---------------------------------------</span></span>
<span id="cb14-41"><a href="#cb14-41"></a></span>
<span id="cb14-42"><a href="#cb14-42"></a><span class="co"># Poderia ter passado o conjunto treinamento ou posso passar o conjunto "splits".</span></span>
<span id="cb14-43"><a href="#cb14-43"></a><span class="co"># O comando recipes já entende que deverá utilizar o conjunto de treinamento.</span></span>
<span id="cb14-44"><a href="#cb14-44"></a>receita <span class="ot">&lt;-</span> </span>
<span id="cb14-45"><a href="#cb14-45"></a>  recipes<span class="sc">::</span><span class="fu">recipe</span>(<span class="at">formula =</span> LifeExpectancy <span class="sc">~</span> ., <span class="at">data =</span> treinamento) <span class="sc">|&gt;</span> </span>
<span id="cb14-46"><a href="#cb14-46"></a>  <span class="fu">step_YeoJohnson</span>(<span class="fu">all_predictors</span>()) <span class="sc">|&gt;</span> <span class="co"># Ajuda na normalização dos dados. Pode ser bom!</span></span>
<span id="cb14-47"><a href="#cb14-47"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>()) <span class="co"># Normalizando variáveis numéricas.</span></span>
<span id="cb14-48"><a href="#cb14-48"></a></span>
<span id="cb14-49"><a href="#cb14-49"></a><span class="co"># Construindo modelo kNN --------------------------------------------------</span></span>
<span id="cb14-50"><a href="#cb14-50"></a>modelo_knn <span class="ot">&lt;-</span> </span>
<span id="cb14-51"><a href="#cb14-51"></a>  parsnip<span class="sc">::</span><span class="fu">nearest_neighbor</span>(<span class="at">neighbors =</span> <span class="fu">tune</span>()) <span class="sc">|&gt;</span> </span>
<span id="cb14-52"><a href="#cb14-52"></a>  parsnip<span class="sc">::</span><span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb14-53"><a href="#cb14-53"></a>  parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"kknn"</span>)</span>
<span id="cb14-54"><a href="#cb14-54"></a></span>
<span id="cb14-55"><a href="#cb14-55"></a><span class="co"># Construindo um workflow (pipeline) --------------------------------------</span></span>
<span id="cb14-56"><a href="#cb14-56"></a>wf_knn <span class="ot">&lt;-</span></span>
<span id="cb14-57"><a href="#cb14-57"></a>  workflows<span class="sc">::</span><span class="fu">workflow</span>() <span class="sc">|&gt;</span></span>
<span id="cb14-58"><a href="#cb14-58"></a>  workflows<span class="sc">::</span><span class="fu">add_recipe</span>(receita) <span class="sc">|&gt;</span></span>
<span id="cb14-59"><a href="#cb14-59"></a>  workflows<span class="sc">::</span><span class="fu">add_model</span>(modelo_knn)</span>
<span id="cb14-60"><a href="#cb14-60"></a></span>
<span id="cb14-61"><a href="#cb14-61"></a><span class="co"># Cross-validation --------------------------------------------------------</span></span>
<span id="cb14-62"><a href="#cb14-62"></a>cv <span class="ot">&lt;-</span> </span>
<span id="cb14-63"><a href="#cb14-63"></a>  treinamento <span class="sc">|&gt;</span> </span>
<span id="cb14-64"><a href="#cb14-64"></a>  rsample<span class="sc">::</span><span class="fu">vfold_cv</span>(<span class="at">v =</span> 10L, <span class="at">strata =</span> <span class="st">"LifeExpectancy"</span>)</span>
<span id="cb14-65"><a href="#cb14-65"></a></span>
<span id="cb14-66"><a href="#cb14-66"></a><span class="co"># Busca do hiperparâmetro k -----------------------------------------------</span></span>
<span id="cb14-67"><a href="#cb14-67"></a>metrica <span class="ot">&lt;-</span> <span class="fu">metric_set</span>(rmse)</span>
<span id="cb14-68"><a href="#cb14-68"></a></span>
<span id="cb14-69"><a href="#cb14-69"></a><span class="co"># Extraindo e atualizando range do parâmetro ------------------------------</span></span>
<span id="cb14-70"><a href="#cb14-70"></a>update_parametros <span class="ot">&lt;-</span></span>
<span id="cb14-71"><a href="#cb14-71"></a>  wf_knn <span class="sc">|&gt;</span> </span>
<span id="cb14-72"><a href="#cb14-72"></a>  <span class="fu">extract_parameter_set_dials</span>() <span class="sc">|&gt;</span></span>
<span id="cb14-73"><a href="#cb14-73"></a>  <span class="fu">update</span>(<span class="st">"neighbors"</span> <span class="ot">=</span> <span class="fu">neighbors</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">50</span>)))</span>
<span id="cb14-74"><a href="#cb14-74"></a></span>
<span id="cb14-75"><a href="#cb14-75"></a><span class="co"># Tunagem -----------------------------------------------------------------</span></span>
<span id="cb14-76"><a href="#cb14-76"></a>meu_grid <span class="ot">&lt;-</span> dials<span class="sc">::</span><span class="fu">grid_max_entropy</span>(update_parametros, <span class="at">size =</span> <span class="dv">60</span>)</span>
<span id="cb14-77"><a href="#cb14-77"></a></span>
<span id="cb14-78"><a href="#cb14-78"></a>tunagem <span class="ot">&lt;-</span></span>
<span id="cb14-79"><a href="#cb14-79"></a>  tune<span class="sc">::</span><span class="fu">tune_grid</span>(</span>
<span id="cb14-80"><a href="#cb14-80"></a>    wf_knn,</span>
<span id="cb14-81"><a href="#cb14-81"></a>    <span class="at">resamples =</span> cv,</span>
<span id="cb14-82"><a href="#cb14-82"></a>    <span class="at">grid =</span> meu_grid,</span>
<span id="cb14-83"><a href="#cb14-83"></a>    <span class="at">metrics =</span> metrica,</span>
<span id="cb14-84"><a href="#cb14-84"></a>    <span class="at">control =</span> <span class="fu">control_grid</span>(<span class="at">save_pred =</span> <span class="cn">TRUE</span>, <span class="at">verbose =</span> <span class="cn">TRUE</span>)</span>
<span id="cb14-85"><a href="#cb14-85"></a>  )</span>
<span id="cb14-86"><a href="#cb14-86"></a></span>
<span id="cb14-87"><a href="#cb14-87"></a>p_hiper <span class="ot">&lt;-</span> <span class="fu">autoplot</span>(tunagem) <span class="sc">+</span></span>
<span id="cb14-88"><a href="#cb14-88"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"KNN - Seleção do número k (vizinhos)"</span>, <span class="at">subtitle =</span> <span class="st">"Sintonização do hiperparâmetro (valor de k)"</span>) <span class="sc">+</span></span>
<span id="cb14-89"><a href="#cb14-89"></a>  <span class="fu">theme</span>(</span>
<span id="cb14-90"><a href="#cb14-90"></a>    <span class="at">title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb14-91"><a href="#cb14-91"></a>  )</span>
<span id="cb14-92"><a href="#cb14-92"></a></span>
<span id="cb14-93"><a href="#cb14-93"></a><span class="co"># Atualizando workflow ----------------------------------------------------</span></span>
<span id="cb14-94"><a href="#cb14-94"></a>wf_knn <span class="ot">&lt;-</span> </span>
<span id="cb14-95"><a href="#cb14-95"></a>  wf_knn <span class="sc">|&gt;</span> </span>
<span id="cb14-96"><a href="#cb14-96"></a>  <span class="fu">finalize_workflow</span>(<span class="fu">select_best</span>(tunagem))</span>
<span id="cb14-97"><a href="#cb14-97"></a></span>
<span id="cb14-98"><a href="#cb14-98"></a><span class="co"># Ajustar o modelo ao conjunto de treinamento e avaliar no teste --------</span></span>
<span id="cb14-99"><a href="#cb14-99"></a>ajuste_final <span class="ot">&lt;-</span> <span class="fu">last_fit</span>(wf_knn, splits)</span>
<span id="cb14-100"><a href="#cb14-100"></a></span>
<span id="cb14-101"><a href="#cb14-101"></a><span class="co"># Ajuste final com toda a base de dados -----------------------------------</span></span>
<span id="cb14-102"><a href="#cb14-102"></a>modelo_final <span class="ot">&lt;-</span> <span class="fu">fit</span>(wf_knn, <span class="at">data =</span> dados_expectativa_renda)</span>
<span id="cb14-103"><a href="#cb14-103"></a></span>
<span id="cb14-104"><a href="#cb14-104"></a><span class="co"># Visualizando as predições na base de treino</span></span>
<span id="cb14-105"><a href="#cb14-105"></a>p_ajuste <span class="ot">&lt;-</span> ajuste_final<span class="sc">$</span>.predictions[[1L]] <span class="sc">|&gt;</span> </span>
<span id="cb14-106"><a href="#cb14-106"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> LifeExpectancy, <span class="at">y =</span> .pred)) <span class="sc">+</span> </span>
<span id="cb14-107"><a href="#cb14-107"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb14-108"><a href="#cb14-108"></a>  <span class="fu">labs</span>(</span>
<span id="cb14-109"><a href="#cb14-109"></a>    <span class="at">title =</span> <span class="st">"Predições versus Real"</span>, </span>
<span id="cb14-110"><a href="#cb14-110"></a>    <span class="at">subtitle =</span> <span class="st">"Usando apenas os dados de teste"</span></span>
<span id="cb14-111"><a href="#cb14-111"></a>  ) <span class="sc">+</span></span>
<span id="cb14-112"><a href="#cb14-112"></a>  <span class="fu">xlab</span>(<span class="st">"LifeExpectancy"</span>) <span class="sc">+</span> </span>
<span id="cb14-113"><a href="#cb14-113"></a>  <span class="fu">xlab</span>(<span class="st">"LifeExpectancy predito"</span>) <span class="sc">+</span> </span>
<span id="cb14-114"><a href="#cb14-114"></a>  <span class="fu">theme</span>(</span>
<span id="cb14-115"><a href="#cb14-115"></a>    <span class="at">title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb14-116"><a href="#cb14-116"></a>  )</span>
<span id="cb14-117"><a href="#cb14-117"></a></span>
<span id="cb14-118"><a href="#cb14-118"></a><span class="co"># Unindo os dois plots</span></span>
<span id="cb14-119"><a href="#cb14-119"></a>p <span class="ot">&lt;-</span> p_hiper <span class="sc">+</span> p_ajuste <span class="sc">+</span> <span class="fu">plot_annotation</span>(<span class="at">tag_levels =</span> <span class="st">"A"</span>)</span>
<span id="cb14-120"><a href="#cb14-120"></a></span>
<span id="cb14-121"><a href="#cb14-121"></a><span class="co"># Salvando gráficos</span></span>
<span id="cb14-122"><a href="#cb14-122"></a><span class="fu">ggsave</span>(p, <span class="at">file =</span> <span class="st">"imgs/plot_hiper_ajuste_tidymodels_knn_whatson.png"</span>, <span class="at">width =</span> <span class="dv">30</span>,</span>
<span id="cb14-123"><a href="#cb14-123"></a>       <span class="at">height =</span> <span class="dv">15</span>, <span class="at">units =</span> <span class="st">"cm"</span>)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>O pacote <a href="https://dials.tidymodels.org/reference/grid_max_entropy.html">dials</a> fornece três funções o <em>grid</em> de parâmetros. São apenas funções que criam <em>grids</em> para os hiperparâmetros, segundo algumas metodologias, mas que na prática não há garantias de qual irá funcionar melhor. A ideia é experimentar e, por meio de uma validação cruzada, decidir por qual utilizar. Normalmente, a <code>grid_max_entropy</code>, utilizada no código acima, funciona bem.</p>
<div class="columns">
<div class="column" style="width:50%;">
<ol type="1">
<li class="fragment"><a href="https://dials.tidymodels.org/reference/grid_max_entropy.html"><code>grid_max_entropy</code></a></li>
<li class="fragment"><a href="https://dials.tidymodels.org/reference/grid_max_entropy.html"><code>grid_latin_hypercube</code></a></li>
<li class="fragment"><a href="https://dials.tidymodels.org/reference/grid_regular.html"><code>grid_regular</code></a></li>
<li class="fragment"><a href="https://dials.tidymodels.org/reference/grid_regular.html"><code>grid_random</code></a></li>
</ol>
</div><div class="column" style="width:50%;">
<p><img data-src="imgs/dails.png"></p>
</div>
</div>
<p><img data-src="gifs/thumbs-up-nod.gif"></p>
</section>
<section id="tidymodels-knn-e-nadaraya-watson-2" class="slide level2">
<h2>Tidymodels <span class="math inline">k</span>NN e Nadaraya-Watson</h2>
<p><br></p>
<p>Para demonstar que podemos realizar transformações (receitas) na base de dados, entre do processo de treinamento, foi realizado duas transformações na variável <code>GDPercapita</code>. A primeira foi a Yeo–Johnson <em>transformation</em> e a segunda foi uma simples normalização dos dados (subitrair da média e dividir pelo desvio padrão). A <a href="https://en.wikipedia.org/wiki/Power_transform#Yeo%E2%80%93Johnson_transformation">transformação de Yeo–Johnson</a> é uma transformação semelhante a de <a href="https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation">Box-Cox</a>. A transformação de <a href="https://en.wikipedia.org/wiki/Power_transform#Yeo%E2%80%93Johnson_transformation">Yeo–Johnson</a> trata de situações que a transformação de <a href="https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation">Box-Cox</a> não trata. Por exemplo, ela trata de valores negativos e zero.</p>
<p><br></p>
<p>A ideia do <em>steps</em> utilizados na faze de pré-processamento dos dados, em que utilizamos o pacote recipes do R, é que para novas observações, depois do modelo ajustado, essas transformações serão automaticamente aplicadas aos novos dados.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="tidymodels-knn-e-nadaraya-watson-3" class="slide level2">
<h2>Tidymodels <span class="math inline">k</span>NN e Nadaraya-Watson</h2>
<p><br></p>

<img data-src="imgs/plot_hiper_ajuste_tidymodels_knn.png" style="width:50.0%" class="r-stretch quarto-figure-center"></section>
<section id="tidymodels-knn-e-nadaraya-watson-4" class="slide level2">
<h2>Tidymodels <span class="math inline">k</span>NN e Nadaraya-Watson</h2>
<p><br></p>
<p><span class="red">Exemplo</span>: Vamos reproduzir todo o fluxo de treinamento que fizemos com o método <span class="math inline">k</span>NN no exemplo anterior, agora, utilizando o modelo Nadaraya-Watson. Note que uma vez que entendemos o <a href="https://www.tidymodels.org/">tidymodels</a>, fica fácil adaptar um código já existente para o treinamento de um outro modelo. Na verdade, o método de Nadaraya-Watson implementado no tidymodels é um pouco diferente. Ainda utilizamos a informação de <span class="math inline">k</span>, em que o valor de <span class="math inline">h</span> é deverminado pela média dos vizinhos mais próximos de <span class="math inline">{\bf x}_i</span> à <span class="math inline">{\bf x}</span>. Além de <span class="math inline">k</span>, temos o valor de <span class="math inline">p</span> para a distância de Minkowski como hiperparâmetro. Portanto, aqui, teremos dois hiperparâmetros (parâmetros de sintonização).</p>
<p><br></p>
<div class="cell">
<details>
<summary>Workflow completo do treimanento de um modelo Nadaraya-Watson.</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb15-3"><a href="#cb15-3"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb15-5"><a href="#cb15-5"></a><span class="fu">library</span>(doMC)</span>
<span id="cb15-6"><a href="#cb15-6"></a></span>
<span id="cb15-7"><a href="#cb15-7"></a><span class="co"># Paralelizando o código em sistemas Unix</span></span>
<span id="cb15-8"><a href="#cb15-8"></a><span class="fu">registerDoMC</span>(<span class="at">cores =</span> parallel<span class="sc">::</span><span class="fu">detectCores</span>())</span>
<span id="cb15-9"><a href="#cb15-9"></a></span>
<span id="cb15-10"><a href="#cb15-10"></a><span class="co"># Resolvendo eventuais conflitos entre tidymodels e outros pacotes eventualmente</span></span>
<span id="cb15-11"><a href="#cb15-11"></a><span class="co"># carregados:</span></span>
<span id="cb15-12"><a href="#cb15-12"></a>tidymodels<span class="sc">::</span><span class="fu">tidymodels_prefer</span>()</span>
<span id="cb15-13"><a href="#cb15-13"></a></span>
<span id="cb15-14"><a href="#cb15-14"></a><span class="co"># Lendo a base de dados:</span></span>
<span id="cb15-15"><a href="#cb15-15"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb15-16"><a href="#cb15-16"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb15-17"><a href="#cb15-17"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb15-18"><a href="#cb15-18"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb15-19"><a href="#cb15-19"></a></span>
<span id="cb15-20"><a href="#cb15-20"></a>dados_expectativa_renda <span class="ot">&lt;-</span></span>
<span id="cb15-21"><a href="#cb15-21"></a>  dados_expectativa_renda <span class="sc">|&gt;</span></span>
<span id="cb15-22"><a href="#cb15-22"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>CountryName)</span>
<span id="cb15-23"><a href="#cb15-23"></a></span>
<span id="cb15-24"><a href="#cb15-24"></a><span class="co"># Setando uma semente</span></span>
<span id="cb15-25"><a href="#cb15-25"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb15-26"><a href="#cb15-26"></a></span>
<span id="cb15-27"><a href="#cb15-27"></a><span class="co"># Divisão da base de dados ------------------------------------------------</span></span>
<span id="cb15-28"><a href="#cb15-28"></a><span class="co"># Divisão inicial (treino e teste)</span></span>
<span id="cb15-29"><a href="#cb15-29"></a>splits <span class="ot">&lt;-</span> </span>
<span id="cb15-30"><a href="#cb15-30"></a>  rsample<span class="sc">::</span><span class="fu">initial_split</span>(</span>
<span id="cb15-31"><a href="#cb15-31"></a>    dados_expectativa_renda,</span>
<span id="cb15-32"><a href="#cb15-32"></a>    <span class="at">strata =</span> <span class="st">"LifeExpectancy"</span>, </span>
<span id="cb15-33"><a href="#cb15-33"></a>    <span class="at">prop =</span> <span class="fl">0.8</span> </span>
<span id="cb15-34"><a href="#cb15-34"></a>  )</span>
<span id="cb15-35"><a href="#cb15-35"></a></span>
<span id="cb15-36"><a href="#cb15-36"></a><span class="co"># O conjunto de dados de treinamento será utilizado para ajustar/treinar o</span></span>
<span id="cb15-37"><a href="#cb15-37"></a><span class="co"># modelo:</span></span>
<span id="cb15-38"><a href="#cb15-38"></a>treinamento <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(splits)</span>
<span id="cb15-39"><a href="#cb15-39"></a></span>
<span id="cb15-40"><a href="#cb15-40"></a><span class="co"># O conjunto de teste será utilizado apenas no fim, para avaliar o desempenho</span></span>
<span id="cb15-41"><a href="#cb15-41"></a><span class="co"># preditivo final do modelo:</span></span>
<span id="cb15-42"><a href="#cb15-42"></a>teste <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">testing</span>(splits) </span>
<span id="cb15-43"><a href="#cb15-43"></a></span>
<span id="cb15-44"><a href="#cb15-44"></a><span class="co"># Criando uma receita para os dados ---------------------------------------</span></span>
<span id="cb15-45"><a href="#cb15-45"></a></span>
<span id="cb15-46"><a href="#cb15-46"></a><span class="co"># Poderia ter passado o conjunto treinamento ou posso passar o conjunto "splits".</span></span>
<span id="cb15-47"><a href="#cb15-47"></a><span class="co"># O comando recipes já entende que deverá utilizar o conjunto de treinamento.</span></span>
<span id="cb15-48"><a href="#cb15-48"></a>receita <span class="ot">&lt;-</span> </span>
<span id="cb15-49"><a href="#cb15-49"></a>  recipes<span class="sc">::</span><span class="fu">recipe</span>(<span class="at">formula =</span> LifeExpectancy <span class="sc">~</span> ., <span class="at">data =</span> treinamento) <span class="sc">|&gt;</span> </span>
<span id="cb15-50"><a href="#cb15-50"></a>  <span class="fu">step_YeoJohnson</span>(<span class="fu">all_predictors</span>()) <span class="sc">|&gt;</span> <span class="co"># Ajuda na normalização dos dados. Pode ser bom!</span></span>
<span id="cb15-51"><a href="#cb15-51"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>()) <span class="co"># Normalizando variáveis numéricas.</span></span>
<span id="cb15-52"><a href="#cb15-52"></a></span>
<span id="cb15-53"><a href="#cb15-53"></a><span class="co"># Construindo modelo Nadaraya ---------------------------------------------</span></span>
<span id="cb15-54"><a href="#cb15-54"></a>modelo_nadaraya <span class="ot">&lt;-</span> </span>
<span id="cb15-55"><a href="#cb15-55"></a>  parsnip<span class="sc">::</span><span class="fu">nearest_neighbor</span>(<span class="at">dist_power =</span> <span class="fu">tune</span>(), <span class="at">weight_func =</span> <span class="st">"gaussian"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb15-56"><a href="#cb15-56"></a>  parsnip<span class="sc">::</span><span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb15-57"><a href="#cb15-57"></a>  parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"kknn"</span>)</span>
<span id="cb15-58"><a href="#cb15-58"></a></span>
<span id="cb15-59"><a href="#cb15-59"></a><span class="co"># Construindo um workflow (pipeline) --------------------------------------</span></span>
<span id="cb15-60"><a href="#cb15-60"></a>wf_nadaraya <span class="ot">&lt;-</span></span>
<span id="cb15-61"><a href="#cb15-61"></a>  workflows<span class="sc">::</span><span class="fu">workflow</span>() <span class="sc">|&gt;</span></span>
<span id="cb15-62"><a href="#cb15-62"></a>  workflows<span class="sc">::</span><span class="fu">add_recipe</span>(receita) <span class="sc">|&gt;</span></span>
<span id="cb15-63"><a href="#cb15-63"></a>  workflows<span class="sc">::</span><span class="fu">add_model</span>(modelo_nadaraya)</span>
<span id="cb15-64"><a href="#cb15-64"></a></span>
<span id="cb15-65"><a href="#cb15-65"></a><span class="co"># Cross-validation --------------------------------------------------------</span></span>
<span id="cb15-66"><a href="#cb15-66"></a>cv <span class="ot">&lt;-</span> </span>
<span id="cb15-67"><a href="#cb15-67"></a>  treinamento <span class="sc">|&gt;</span> </span>
<span id="cb15-68"><a href="#cb15-68"></a>  rsample<span class="sc">::</span><span class="fu">vfold_cv</span>(<span class="at">v =</span> 10L, <span class="at">strata =</span> <span class="st">"LifeExpectancy"</span>)</span>
<span id="cb15-69"><a href="#cb15-69"></a></span>
<span id="cb15-70"><a href="#cb15-70"></a><span class="co"># Busca do hiperparâmetro k -----------------------------------------------</span></span>
<span id="cb15-71"><a href="#cb15-71"></a>metrica <span class="ot">&lt;-</span> <span class="fu">metric_set</span>(rmse)</span>
<span id="cb15-72"><a href="#cb15-72"></a></span>
<span id="cb15-73"><a href="#cb15-73"></a><span class="co"># Extraindo e atualizando range do parâmetro ------------------------------</span></span>
<span id="cb15-74"><a href="#cb15-74"></a>update_parametros <span class="ot">&lt;-</span></span>
<span id="cb15-75"><a href="#cb15-75"></a>  wf_nadaraya <span class="sc">|&gt;</span> </span>
<span id="cb15-76"><a href="#cb15-76"></a>  <span class="fu">extract_parameter_set_dials</span>() <span class="sc">|&gt;</span></span>
<span id="cb15-77"><a href="#cb15-77"></a>  <span class="fu">update</span>(<span class="st">"dist_power"</span> <span class="ot">=</span> <span class="fu">dist_power</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">50</span>)))</span>
<span id="cb15-78"><a href="#cb15-78"></a></span>
<span id="cb15-79"><a href="#cb15-79"></a><span class="co"># Tunagem -----------------------------------------------------------------</span></span>
<span id="cb15-80"><a href="#cb15-80"></a>meu_grid <span class="ot">&lt;-</span> dials<span class="sc">::</span><span class="fu">grid_max_entropy</span>(update_parametros, <span class="at">size =</span> 100L)</span>
<span id="cb15-81"><a href="#cb15-81"></a></span>
<span id="cb15-82"><a href="#cb15-82"></a>tunagem <span class="ot">&lt;-</span></span>
<span id="cb15-83"><a href="#cb15-83"></a>  tune<span class="sc">::</span><span class="fu">tune_grid</span>(</span>
<span id="cb15-84"><a href="#cb15-84"></a>    wf_knn,</span>
<span id="cb15-85"><a href="#cb15-85"></a>    <span class="at">resamples =</span> cv,</span>
<span id="cb15-86"><a href="#cb15-86"></a>    <span class="at">grid =</span> meu_grid,</span>
<span id="cb15-87"><a href="#cb15-87"></a>    <span class="at">metrics =</span> metrica,</span>
<span id="cb15-88"><a href="#cb15-88"></a>    <span class="at">control =</span> <span class="fu">control_grid</span>(<span class="at">save_pred =</span> <span class="cn">TRUE</span>, <span class="at">verbose =</span> <span class="cn">TRUE</span>)</span>
<span id="cb15-89"><a href="#cb15-89"></a>  )</span>
<span id="cb15-90"><a href="#cb15-90"></a></span>
<span id="cb15-91"><a href="#cb15-91"></a>p_hiper <span class="ot">&lt;-</span> <span class="fu">autoplot</span>(tunagem) <span class="sc">+</span></span>
<span id="cb15-92"><a href="#cb15-92"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Nadaraya-Watson - Seleção do parâmetro p"</span>, <span class="at">subtitle =</span> <span class="st">"Sintonização do hiperparâmetro (distância p)"</span>) <span class="sc">+</span></span>
<span id="cb15-93"><a href="#cb15-93"></a>  <span class="fu">theme</span>(</span>
<span id="cb15-94"><a href="#cb15-94"></a>    <span class="at">title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb15-95"><a href="#cb15-95"></a>  )</span>
<span id="cb15-96"><a href="#cb15-96"></a></span>
<span id="cb15-97"><a href="#cb15-97"></a><span class="co"># Atualizando workflow ----------------------------------------------------</span></span>
<span id="cb15-98"><a href="#cb15-98"></a>wf_nadaraya <span class="ot">&lt;-</span> </span>
<span id="cb15-99"><a href="#cb15-99"></a>  wf_nadaraya <span class="sc">|&gt;</span> </span>
<span id="cb15-100"><a href="#cb15-100"></a>  <span class="fu">finalize_workflow</span>(<span class="fu">select_best</span>(tunagem))</span>
<span id="cb15-101"><a href="#cb15-101"></a></span>
<span id="cb15-102"><a href="#cb15-102"></a><span class="co"># Ajustar o modelo ao conjunto de treinamento e avaliar no teste --------</span></span>
<span id="cb15-103"><a href="#cb15-103"></a>ajuste_final <span class="ot">&lt;-</span> <span class="fu">last_fit</span>(wf_nadaraya, splits)</span>
<span id="cb15-104"><a href="#cb15-104"></a></span>
<span id="cb15-105"><a href="#cb15-105"></a><span class="co"># Ajuste final com toda a base de dados -----------------------------------</span></span>
<span id="cb15-106"><a href="#cb15-106"></a>modelo_final <span class="ot">&lt;-</span> <span class="fu">fit</span>(wf_nadaraya, <span class="at">data =</span> dados_expectativa_renda)</span>
<span id="cb15-107"><a href="#cb15-107"></a></span>
<span id="cb15-108"><a href="#cb15-108"></a><span class="co"># Visualizando as predições na base de treino</span></span>
<span id="cb15-109"><a href="#cb15-109"></a>p_ajuste <span class="ot">&lt;-</span> ajuste_final<span class="sc">$</span>.predictions[[1L]] <span class="sc">|&gt;</span> </span>
<span id="cb15-110"><a href="#cb15-110"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> LifeExpectancy, <span class="at">y =</span> .pred)) <span class="sc">+</span> </span>
<span id="cb15-111"><a href="#cb15-111"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>, <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb15-112"><a href="#cb15-112"></a>  <span class="fu">labs</span>(</span>
<span id="cb15-113"><a href="#cb15-113"></a>    <span class="at">title =</span> <span class="st">"Predições versus Real"</span>, </span>
<span id="cb15-114"><a href="#cb15-114"></a>    <span class="at">subtitle =</span> <span class="st">"Usando apenas os dados de teste"</span></span>
<span id="cb15-115"><a href="#cb15-115"></a>  ) <span class="sc">+</span></span>
<span id="cb15-116"><a href="#cb15-116"></a>  <span class="fu">xlab</span>(<span class="st">"LifeExpectancy"</span>) <span class="sc">+</span> </span>
<span id="cb15-117"><a href="#cb15-117"></a>  <span class="fu">xlab</span>(<span class="st">"LifeExpectancy predito"</span>) <span class="sc">+</span> </span>
<span id="cb15-118"><a href="#cb15-118"></a>  <span class="fu">theme</span>(</span>
<span id="cb15-119"><a href="#cb15-119"></a>    <span class="at">title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">"bold"</span>)</span>
<span id="cb15-120"><a href="#cb15-120"></a>  )</span>
<span id="cb15-121"><a href="#cb15-121"></a></span>
<span id="cb15-122"><a href="#cb15-122"></a><span class="co"># Unindo os dois plots</span></span>
<span id="cb15-123"><a href="#cb15-123"></a>p <span class="ot">&lt;-</span> p_hiper <span class="sc">+</span> p_ajuste <span class="sc">+</span> <span class="fu">plot_annotation</span>(<span class="at">tag_levels =</span> <span class="st">"A"</span>)</span>
<span id="cb15-124"><a href="#cb15-124"></a></span>
<span id="cb15-125"><a href="#cb15-125"></a><span class="co"># Salvando gráficos</span></span>
<span id="cb15-126"><a href="#cb15-126"></a><span class="fu">ggsave</span>(p, <span class="at">file =</span> <span class="st">"imgs/plot_hiper_ajuste_tidymodels_nadaraya.png"</span>, <span class="at">width =</span> <span class="dv">30</span>,</span>
<span id="cb15-127"><a href="#cb15-127"></a>       <span class="at">height =</span> <span class="dv">15</span>, <span class="at">units =</span> <span class="st">"cm"</span>)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="tidymodels-knn-e-nadaraya-watson-5" class="slide level2">
<h2>Tidymodels <span class="math inline">k</span>NN e Nadaraya-Watson</h2>
<p><br></p>
<p><img data-src="imgs/plot_hiper_ajuste_tidymodels_nadaraya.png" data-fig-aling="center"> ## Tidymodels <span class="math inline">k</span>NN e Nadaraya-Watson</p>
<p><br></p>
<p>Podemos visualizar a melhor combinação de hiperparâmetros, segundo <span class="math inline">\widehat{R}(g)</span> usando fazendo:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># As cinco melhores combinações</span></span>
<span id="cb16-2"><a href="#cb16-2"></a>tunagem <span class="sc">|&gt;</span> </span>
<span id="cb16-3"><a href="#cb16-3"></a>  <span class="fu">show_best</span>()</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img data-src="gifs/bean_simple.gif"></p>
</section>
<section id="comparando-dois-ou-mais-modelos" class="slide level2">
<h2>Comparando dois ou mais modelos</h2>
<p><br></p>
<p><strong>Você poderia perguntar</strong>: “Certo, mais tenho como comparar dois ou mais modelos de uma única vez?”</p>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="gifs/giphy.gif"></p>
</div><div class="column" style="width:50%;">
<p><br></p>
<p>A resposta é <strong>Sim</strong>!</p>
<p><br></p>
<p><img data-src="gifs/yes.gif"></p>
</div>
</div>
</section>
<section id="comparando-dois-ou-mais-modelos-1" class="slide level2">
<h2>Comparando dois ou mais modelos</h2>
<p><br></p>
<p>Na verdade, é bem mais interessante comparar conjuntamente, visto que para poder compararmos devemos garantir que as mesmas amostras de treinamento e teste estão sendo utilizadas, i.e., para termos uma comparação mais justa. Claro que dá para fazer de forma separada, fixando a semente dos geradores de números pseudo-aleatórios, para que a biblioteca rsample possa reproduzir a mesma divisão para ambos os modelos. Porém, a estratégia do exemplo abaixo é mais consistente.</p>
<p><br></p>
<p><span class="red">Exemplo</span>: Estude o código que segue! Ele compara os modelos de <span class="math inline">k</span>NN com o método de Nadaraya-Watson.</p>
<p><br></p>
<div class="cell">
<details>
<summary>Workflow completo do treimanento e comparação de dois modelos (KNN e Nadaraya-Whatson).</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb17-3"><a href="#cb17-3"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb17-4"><a href="#cb17-4"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb17-5"><a href="#cb17-5"></a></span>
<span id="cb17-6"><a href="#cb17-6"></a><span class="co"># Resolvendo eventuais conflitos entre tidymodels e outros pacotes eventualmente</span></span>
<span id="cb17-7"><a href="#cb17-7"></a><span class="co"># carregados:</span></span>
<span id="cb17-8"><a href="#cb17-8"></a>tidymodels<span class="sc">::</span><span class="fu">tidymodels_prefer</span>()</span>
<span id="cb17-9"><a href="#cb17-9"></a></span>
<span id="cb17-10"><a href="#cb17-10"></a><span class="co"># Lendo a base de dados:</span></span>
<span id="cb17-11"><a href="#cb17-11"></a>url <span class="ot">&lt;-</span> <span class="st">"https://github.com/prdm0/dados/raw/main/dados_expectativa_renda.RData"</span></span>
<span id="cb17-12"><a href="#cb17-12"></a>arquivo_temp <span class="ot">&lt;-</span> <span class="fu">tempfile</span>()</span>
<span id="cb17-13"><a href="#cb17-13"></a><span class="fu">download.file</span>(<span class="at">url =</span> url, <span class="at">destfile =</span> arquivo_temp)</span>
<span id="cb17-14"><a href="#cb17-14"></a><span class="fu">load</span>(arquivo_temp)</span>
<span id="cb17-15"><a href="#cb17-15"></a></span>
<span id="cb17-16"><a href="#cb17-16"></a>dados_expectativa_renda <span class="ot">&lt;-</span></span>
<span id="cb17-17"><a href="#cb17-17"></a>  dados_expectativa_renda <span class="sc">|&gt;</span></span>
<span id="cb17-18"><a href="#cb17-18"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>CountryName)</span>
<span id="cb17-19"><a href="#cb17-19"></a></span>
<span id="cb17-20"><a href="#cb17-20"></a><span class="co"># Setando uma semente</span></span>
<span id="cb17-21"><a href="#cb17-21"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb17-22"><a href="#cb17-22"></a></span>
<span id="cb17-23"><a href="#cb17-23"></a><span class="co"># Divisão da base de dados ------------------------------------------------</span></span>
<span id="cb17-24"><a href="#cb17-24"></a><span class="co"># Divisão inicial (treino e teste)</span></span>
<span id="cb17-25"><a href="#cb17-25"></a>splits <span class="ot">&lt;-</span> </span>
<span id="cb17-26"><a href="#cb17-26"></a>  rsample<span class="sc">::</span><span class="fu">initial_split</span>(</span>
<span id="cb17-27"><a href="#cb17-27"></a>    dados_expectativa_renda,</span>
<span id="cb17-28"><a href="#cb17-28"></a>    <span class="at">strata =</span> <span class="st">"LifeExpectancy"</span>, </span>
<span id="cb17-29"><a href="#cb17-29"></a>    <span class="at">prop =</span> <span class="fl">0.8</span> </span>
<span id="cb17-30"><a href="#cb17-30"></a>  )</span>
<span id="cb17-31"><a href="#cb17-31"></a></span>
<span id="cb17-32"><a href="#cb17-32"></a><span class="co"># O conjunto de dados de treinamento será utilizado para ajustar/treinar o</span></span>
<span id="cb17-33"><a href="#cb17-33"></a><span class="co"># modelo:</span></span>
<span id="cb17-34"><a href="#cb17-34"></a>treinamento <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">training</span>(splits)</span>
<span id="cb17-35"><a href="#cb17-35"></a></span>
<span id="cb17-36"><a href="#cb17-36"></a><span class="co"># O conjunto de teste será utilizado apenas no fim, para avaliar o desempenho</span></span>
<span id="cb17-37"><a href="#cb17-37"></a><span class="co"># preditivo final do modelo:</span></span>
<span id="cb17-38"><a href="#cb17-38"></a>teste <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">testing</span>(splits) </span>
<span id="cb17-39"><a href="#cb17-39"></a></span>
<span id="cb17-40"><a href="#cb17-40"></a><span class="co"># Criando uma receita para os dados ---------------------------------------</span></span>
<span id="cb17-41"><a href="#cb17-41"></a></span>
<span id="cb17-42"><a href="#cb17-42"></a><span class="co"># Poderia ter passado o conjunto treinamento ou posso passar o conjunto "splits".</span></span>
<span id="cb17-43"><a href="#cb17-43"></a><span class="co"># O comando recipes já entende que deverá utilizar o conjunto de treinamento.</span></span>
<span id="cb17-44"><a href="#cb17-44"></a>receita_knn <span class="ot">&lt;-</span> </span>
<span id="cb17-45"><a href="#cb17-45"></a>  recipes<span class="sc">::</span><span class="fu">recipe</span>(<span class="at">formula =</span> LifeExpectancy <span class="sc">~</span> ., <span class="at">data =</span> treinamento) <span class="sc">|&gt;</span> </span>
<span id="cb17-46"><a href="#cb17-46"></a>  <span class="fu">step_YeoJohnson</span>(<span class="fu">all_predictors</span>()) <span class="sc">|&gt;</span> <span class="co"># Ajuda na normalização dos dados. Pode ser bom!</span></span>
<span id="cb17-47"><a href="#cb17-47"></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>()) <span class="co"># Normalizando variáveis numéricas.</span></span>
<span id="cb17-48"><a href="#cb17-48"></a></span>
<span id="cb17-49"><a href="#cb17-49"></a>receita_nadaraya <span class="ot">&lt;-</span> receita_knn</span>
<span id="cb17-50"><a href="#cb17-50"></a></span>
<span id="cb17-51"><a href="#cb17-51"></a><span class="co"># Construindo modelo kNN --------------------------------------------------</span></span>
<span id="cb17-52"><a href="#cb17-52"></a>modelo_knn <span class="ot">&lt;-</span> </span>
<span id="cb17-53"><a href="#cb17-53"></a>  parsnip<span class="sc">::</span><span class="fu">nearest_neighbor</span>(<span class="at">neighbors =</span> <span class="fu">tune</span>()) <span class="sc">|&gt;</span> </span>
<span id="cb17-54"><a href="#cb17-54"></a>  parsnip<span class="sc">::</span><span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-55"><a href="#cb17-55"></a>  parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"kknn"</span>)</span>
<span id="cb17-56"><a href="#cb17-56"></a></span>
<span id="cb17-57"><a href="#cb17-57"></a><span class="co"># Construindo modelo Nadaraya ---------------------------------------------</span></span>
<span id="cb17-58"><a href="#cb17-58"></a>modelo_nadaraya <span class="ot">&lt;-</span> </span>
<span id="cb17-59"><a href="#cb17-59"></a>  parsnip<span class="sc">::</span><span class="fu">nearest_neighbor</span>(<span class="at">dist_power =</span> <span class="fu">tune</span>(), <span class="at">neighbors =</span> <span class="fu">tune</span>(),</span>
<span id="cb17-60"><a href="#cb17-60"></a>                            <span class="at">weight_func =</span> <span class="st">"gaussian"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-61"><a href="#cb17-61"></a>  parsnip<span class="sc">::</span><span class="fu">set_mode</span>(<span class="st">"regression"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-62"><a href="#cb17-62"></a>  parsnip<span class="sc">::</span><span class="fu">set_engine</span>(<span class="st">"kknn"</span>)</span>
<span id="cb17-63"><a href="#cb17-63"></a></span>
<span id="cb17-64"><a href="#cb17-64"></a><span class="co"># Validação cruzada -------------------------------------------------------</span></span>
<span id="cb17-65"><a href="#cb17-65"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb17-66"><a href="#cb17-66"></a>cv <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">vfold_cv</span>(treinamento, <span class="at">v =</span> 5L)</span>
<span id="cb17-67"><a href="#cb17-67"></a></span>
<span id="cb17-68"><a href="#cb17-68"></a><span class="co"># Criando workflow conjunto -----------------------------------------------</span></span>
<span id="cb17-69"><a href="#cb17-69"></a>all_wf <span class="ot">&lt;-</span> </span>
<span id="cb17-70"><a href="#cb17-70"></a>  <span class="fu">workflow_set</span>(</span>
<span id="cb17-71"><a href="#cb17-71"></a>    <span class="at">preproc =</span> <span class="fu">list</span>(receita_knn, receita_nadaraya),</span>
<span id="cb17-72"><a href="#cb17-72"></a>    <span class="at">models =</span> <span class="fu">list</span>(<span class="at">modelo_knn =</span> modelo_knn, <span class="at">modelo_nadaraya =</span> modelo_nadaraya)</span>
<span id="cb17-73"><a href="#cb17-73"></a>  )</span>
<span id="cb17-74"><a href="#cb17-74"></a></span>
<span id="cb17-75"><a href="#cb17-75"></a><span class="co"># Tunando ambos os modelos ------------------------------------------------</span></span>
<span id="cb17-76"><a href="#cb17-76"></a>tunagem <span class="ot">&lt;-</span> </span>
<span id="cb17-77"><a href="#cb17-77"></a>  all_wf <span class="sc">|&gt;</span> </span>
<span id="cb17-78"><a href="#cb17-78"></a>  <span class="fu">workflow_map</span>(</span>
<span id="cb17-79"><a href="#cb17-79"></a>    <span class="at">seed =</span> <span class="dv">0</span>, </span>
<span id="cb17-80"><a href="#cb17-80"></a>    <span class="at">verbose =</span> <span class="cn">TRUE</span>,</span>
<span id="cb17-81"><a href="#cb17-81"></a>    <span class="at">resamples =</span> cv,</span>
<span id="cb17-82"><a href="#cb17-82"></a>    <span class="at">grid =</span> <span class="dv">50</span>,</span>
<span id="cb17-83"><a href="#cb17-83"></a>    <span class="at">metrics =</span> <span class="fu">metric_set</span>(rmse)</span>
<span id="cb17-84"><a href="#cb17-84"></a>  )</span>
<span id="cb17-85"><a href="#cb17-85"></a></span>
<span id="cb17-86"><a href="#cb17-86"></a><span class="co"># Selecionando o melhor de cada um dos modelos ----------------------------</span></span>
<span id="cb17-87"><a href="#cb17-87"></a>melhor_knn <span class="ot">&lt;-</span> </span>
<span id="cb17-88"><a href="#cb17-88"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb17-89"><a href="#cb17-89"></a>  <span class="fu">extract_workflow_set_result</span>(<span class="st">"recipe_1_modelo_knn"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-90"><a href="#cb17-90"></a>  <span class="fu">select_best</span>(<span class="st">"rmse"</span>)</span>
<span id="cb17-91"><a href="#cb17-91"></a></span>
<span id="cb17-92"><a href="#cb17-92"></a>melhor_nadaraya <span class="ot">&lt;-</span> </span>
<span id="cb17-93"><a href="#cb17-93"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb17-94"><a href="#cb17-94"></a>  <span class="fu">extract_workflow_set_result</span>(<span class="st">"recipe_1_modelo_nadaraya"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-95"><a href="#cb17-95"></a>  <span class="fu">select_best</span>(<span class="st">"rmse"</span>)</span>
<span id="cb17-96"><a href="#cb17-96"></a></span>
<span id="cb17-97"><a href="#cb17-97"></a><span class="co"># Avaliando o desempenho no conjunto de teste</span></span>
<span id="cb17-98"><a href="#cb17-98"></a>teste_knn <span class="ot">&lt;-</span> </span>
<span id="cb17-99"><a href="#cb17-99"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb17-100"><a href="#cb17-100"></a>  <span class="fu">extract_workflow</span>(<span class="st">"recipe_1_modelo_knn"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-101"><a href="#cb17-101"></a>  <span class="fu">finalize_workflow</span>(melhor_knn) <span class="sc">|&gt;</span> </span>
<span id="cb17-102"><a href="#cb17-102"></a>  <span class="fu">last_fit</span>(<span class="at">split =</span> splits)</span>
<span id="cb17-103"><a href="#cb17-103"></a></span>
<span id="cb17-104"><a href="#cb17-104"></a>teste_nadaraya <span class="ot">&lt;-</span> </span>
<span id="cb17-105"><a href="#cb17-105"></a>  tunagem <span class="sc">|&gt;</span> </span>
<span id="cb17-106"><a href="#cb17-106"></a>  <span class="fu">extract_workflow</span>(<span class="st">"recipe_1_modelo_nadaraya"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-107"><a href="#cb17-107"></a>  <span class="fu">finalize_workflow</span>(melhor_nadaraya) <span class="sc">|&gt;</span> </span>
<span id="cb17-108"><a href="#cb17-108"></a>  <span class="fu">last_fit</span>(<span class="at">split =</span> splits)</span>
<span id="cb17-109"><a href="#cb17-109"></a></span>
<span id="cb17-110"><a href="#cb17-110"></a><span class="co"># Visualizando as métricas de cada um</span></span>
<span id="cb17-111"><a href="#cb17-111"></a><span class="fu">collect_metrics</span>(teste_knn)</span>
<span id="cb17-112"><a href="#cb17-112"></a><span class="fu">collect_metrics</span>(teste_nadaraya)</span>
<span id="cb17-113"><a href="#cb17-113"></a></span>
<span id="cb17-114"><a href="#cb17-114"></a><span class="co"># Ajustando o modelo com todos os dados. Aqui escolhemos o Nadaraya-Watson</span></span>
<span id="cb17-115"><a href="#cb17-115"></a>modelo_final <span class="ot">&lt;-</span> </span>
<span id="cb17-116"><a href="#cb17-116"></a>  teste_nadaraya <span class="sc">|&gt;</span> </span>
<span id="cb17-117"><a href="#cb17-117"></a>  <span class="fu">extract_workflow</span>(<span class="st">"recipe_1_modelo_nadaraya"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb17-118"><a href="#cb17-118"></a>  <span class="fu">fit</span>(<span class="at">data =</span> dados_expectativa_renda)</span>
<span id="cb17-119"><a href="#cb17-119"></a></span>
<span id="cb17-120"><a href="#cb17-120"></a><span class="co"># Fazendo previsões com novos dados. Aqui usarei os mesmos dados</span></span>
<span id="cb17-121"><a href="#cb17-121"></a><span class="fu">predict</span>(modelo_final, <span class="at">new_data =</span> dados_expectativa_renda)</span>
<span id="cb17-122"><a href="#cb17-122"></a></span>
<span id="cb17-123"><a href="#cb17-123"></a><span class="co"># Salvando o modelo em um arquivo. Aqui estou supondo que salvei em</span></span>
<span id="cb17-124"><a href="#cb17-124"></a><span class="co"># "~/Downloads/modelo_final.rds":</span></span>
<span id="cb17-125"><a href="#cb17-125"></a><span class="co"># saveRDS(modelo_final, file = "~/Downloads/modelo_final.rds")</span></span>
<span id="cb17-126"><a href="#cb17-126"></a></span>
<span id="cb17-127"><a href="#cb17-127"></a><span class="co"># Lendo um modelo salvo para depois fazer predições. Aqui estou supondo que </span></span>
<span id="cb17-128"><a href="#cb17-128"></a><span class="co"># o modelo encontra-se salvo em "~/Downloads/modelo_final.rds":</span></span>
<span id="cb17-129"><a href="#cb17-129"></a><span class="co"># readRDS("~/Downloads/modelo_final.rds")</span></span>
<span id="cb17-130"><a href="#cb17-130"></a><span class="sc">!</span>[](gifs<span class="sc">/</span>bom.gif)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="suport-vector-regression-machine" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>Métodos de estimação da função de regressão <span class="math inline">r({\bf x})</span> com base em <em>Reproducing Kernel Hilbert Spaces</em> - RKHs são famílias de metodologias bastante gerais. A ideia desses métodos envolvem definir uma função objetivo para a quantificação da qualidade das predições e, porteriormente, busca-se uma função que melhor se ajuste ao espaço de funções <span class="math inline">\mathcal{H}</span>. Busca-se uma solução para</p>
<p><span id="eq-objetivo-geral-hilbert"><span class="math display">\argmin_{g \in \mathcal{H}} \sum_{k = 1}^n L(g({\bf x}_k, y_k)) + \mathcal{P}(g), \tag{4}</span></span> em que <span class="math inline">L</span> é uma função de perda arbitrária, <span class="math inline">\mathcal{P}</span> uma medida de complexidade de <span class="math inline">g</span> e <span class="math inline">\mathcal{H}</span> um subespaço de funções.</p>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p>Ocorre que para um espaço de funções arbitrário, a solução para o problema seria bastante difícil. A ideia é poder uma grande família de espaços <span class="math inline">\mathcal{H}</span> (RKHs) de modo que a solução do problema seja relativamente simples de ser implementada.</p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="gifs/ok.gif"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="suport-vector-regression-machine-1" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>Métodos de regressão que se baseiam em panalização em RKHS são uma generalização da <a href="#/suport-vector-regression-machine" class="quarto-xref">Equação&nbsp;4</a>, em que a função objetivo nesse caso é dada por:</p>
<p><span id="eq-funcao-objetivo-geral"><span class="math display">\argmin_{g \in \mathcal{H}_k} \sum_{k = 1}^n L(g({\bf x}_k, y_k)) + \lambda||g||_{\mathcal{H}_k}^2, \tag{5}</span></span> em que <span class="math inline">\mathcal{H}_{k}</span> é um RKHS e <span class="math inline">L</span> é uma função adequada para o problema em questão. Calma que vai ser relativamente simples definir <span class="math inline">\mathcal{H}_{k}</span>, uma vez que isso é feito utilizando funções kernel, em particular, o <a href="https://en.wikipedia.org/wiki/Mercer%27s_theorem">kernel de Mercer</a>.</p>

<img data-src="gifs/bean_simple.gif" class="r-stretch"></section>
<section id="suport-vector-regression-machine-2" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>O termo <span class="math inline">||g||_{\mathcal{H}_k}^2</span> na <a href="#/suport-vector-regression-machine-1" class="quarto-xref">Equação&nbsp;5</a> reflete a suavidade das funções em <span class="math inline">\mathcal{H}_k</span> e cada espaço poderá conter uma noção de suavidade diferente, a depender da função de kernel escolhida. Sim, a função ed kernel e função de perda <span class="math inline">L</span> são escolhidas pelo usuário da metodologia.</p>
<p><br></p>
<p>É claro que para diferenças kernel (kernel de Mercer), poderemos ter diferentes resultados que pode ser avaliado em um procedimento de validação-cruzada (<em>cross-validation</em>). O parâmetro <span class="math inline">\lambda</span> é um hiperparâmetro que podemos obter dentro de uma validação-cruzada, testando, por exemplo, um <em>grid</em> de parâmetros, para selecionarmos um <span class="math inline">\lambda</span> que forneça o melhor risco estimado <span class="math inline">\widehat{R}(g)</span>.</p>
<p><br></p>
<p>A parcela <span class="math inline">||g||_{\mathcal{H}_k}^2</span> mensura a suavidade das funções em <span class="math inline">\mathcal{H}_k</span>. Assim como <span class="math inline">g</span>, <span class="math inline">||g||_{\mathcal{H}_k}^2</span> será definidas em termos da função de kernel, e essa é a grande sacada!</p>
<p><br></p>

<img data-src="gifs/uau.gif" class="r-stretch"></section>
<section id="suport-vector-regression-machine-3" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p><strong>Definição</strong> (<span class="red">Kernel de Mercer</span>): Seja <span class="math inline">K({\bf x}_a, {\bf x}_b)</span> uma função com domínio em <span class="math inline">\mathcal{X} \times \mathcal{X}</span> (domínio das <em>features</em>/covariáveis/espaço de características) que poderá ser mais geral que <span class="math inline">\mathbb{R}^d</span>. Diremos que uma função <span class="math inline">K</span>, tal que <span class="math inline">K:{\mathcal{X}\times\mathcal{X}}\longrightarrow \mathbb{R}</span> é um <strong>Kernel de Mercer</strong> se ele satisfaz às condições que seguem:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p><span class="red">Simetria</span>: <span class="math inline">K({\bf x}_a, {\bf x}_b) = K({\bf x}_b, {\bf x}_a)</span>, para todo <span class="math inline">{\bf x}_a,{\bf x}_b \in \mathcal{X}</span>;</p></li>
<li class="fragment"><p><span class="red">Positivo semi-definido</span>: a matriz <span class="math inline">\big[K({\bf x}_a,{\bf x}_b)\big]_{i,j = 1}^n</span> é positiva semi-definida para todo <span class="math inline">n \in \mathbb{N}</span> e para todo <span class="math inline">{\bf x}_1,\cdots, {\bf x}_n \in \mathcal{X}</span>.</p></li>
</ol>
<p><br></p>
<p>Ser positiva semi-definida, significa que para qualquer sequência <span class="math inline">c_r \in \mathbb{R}\,, \forall r = 1, \cdots, n</span>, temos que</p>
<p><span class="math display">\sum_{i = 1}^n\sum_{k = 1}^n c_i c_k K({\bf x}_i,{\bf x}_k)\geq 0,\, \forall n \geq 2.</span></p>
</section>
<section id="suport-vector-regression-machine-4" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>Alguns kernels de Mercer comuns são:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Kernel Linear</strong>: <span class="math inline">K({\bf x}_i, {\bf x}_l) = {\bf x}_i^T{\bf x}_l</span>;</li>
<li class="fragment"><strong>Kernel Polinomial de grau</strong> <span class="math inline">p</span>: <span class="math inline">K({\bf x}_i, {\bf x}_l) = (1 + \langle{\bf x}_i, {\bf x}_l\rangle)^p, \gamma &gt; 0, \theta \geq 0, p \in \mathbb{N}</span>;</li>
<li class="fragment"><strong>Kernel Gaussiano</strong>: <span class="math inline">K({\bf x}_i, {\bf x}_l) = \exp\left\{-\frac{d^2({\bf x}_i, {\bf x}_l)}{2h^2}\right\},\, h &gt; 0</span>;</li>
<li class="fragment"><strong>Kernel Laplaciano</strong>: <span class="math inline">K({\bf x}_i, {\bf x}_l) = \mathrm{e}^{-\gamma d({\bf x}_i, {\bf x}_l)}\,, \gamma &gt; 0</span>;</li>
<li class="fragment"><strong>Kernel Sigmóide</strong>: <span class="math inline">K({\bf x}_i, {\bf x}_l) = \tanh(\gamma {\bf x}_i^T{\bf x}_l + \theta), \, \gamma&gt;0, \theta&gt;0</span>.</li>
</ol>
<p><br></p>
<p>em que <span class="math inline">\gamma, h, \theta</span> e <span class="math inline">p</span>, são parâmetros do kernel.</p>
</section>
<section id="suport-vector-regression-machine-5" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>A ideia principal por trás dos métodos que fazem uso de kernel é fazer o uso de um mapeamento não-linear arbitrário <span class="math inline">\phi</span> do espaço original dos padrões de entrada para um espaço de mais alta dimensão, <span class="math inline">\mathcal{X} \times \mathcal{X}</span> chamado de espaço de características.</p>
<p><br></p>
<p>Um conjunto de padrões que entrada, em um problema de classificação, por exemplo, que não é linearmente separável poderá se tornar linearmente separável através desse mapeamento não linear.</p>
<p><br></p>
<p>Em um espaço vetorial, o <strong>produto interno</strong>, <span class="math inline">\langle{\bf x}_i,{\bf x}_j\rangle = {\bf x}_i^{T}{\bf x}_j</span>, normalmente é utilizado como medida de similaridade entre vetores. Porém, como não conhecemos <span class="math inline">\phi(\cdot)</span>, não é possível realizar <span class="math inline">\phi({\bf x}_i)^T\phi({\bf x}_j)</span> em <span class="math inline">\mathcal{X}\times\mathcal{X}.</span> Aparentemente é bem complicado calcular <span class="math inline">\langle\phi({\bf x}_i),\phi({\bf x}_j)\rangle</span>, sem conhecer <span class="math inline">\phi(\cdot)</span>, não é verdade?!</p>

<img data-src="gifs/ok.gif" class="r-stretch"></section>
<section id="suport-vector-regression-machine-6" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>A essência dos métodos métodos baseado em kernel, é que não precisamos conhecer <span class="math inline">\phi(\cdot)</span>. Os produtos internos no espaço de características poderá ser calculado utilizando um kernel de Mercer.</p>
<p><br></p>
<p><span class="red">Teorema de Mercer</span>: Todo kernel de Mercer <span class="math inline">K</span> pode ser decomposto como</p>
<p><span id="eq-mercer-decomposicao"><span class="math display">K({\bf x}_a,{\bf x}_b) = \sum_{i \geq 0} \gamma_i \phi_i({\bf x}_a)\phi_i({\bf x}_b), \tag{6}</span></span> em que <span class="math inline">\sum_{i \geq 0} \gamma_i^2 &lt; \infty</span> e <span class="math inline">\phi_0, \phi_1, \cdots</span> é uma sequência de funções. Essa propriedade dos em que o kernel de Mercer poderá ser decomposto na forma acima e que não precisamos conhecer <span class="math inline">\phi(\cdot)</span> para o cálculo de produtos internos no espaço de características é comumente denominada de <span class="red"><em>kernel trick</em></span>/<span class="red">truque kernel</span>. Ver detalhes em <a href="https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.1909.0016">aqui</a>.</p>
</section>
<section id="suport-vector-regression-machine-7" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p><span class="red">Definição</span>: Seja <span class="math inline">K</span> um kernel de Mercer, e sejam <span class="math inline">\phi_i</span> e <span class="math inline">\gamma_i,\, i \geq 0</span>, da forma do teorema acima. Então,</p>
<p><br></p>
<p><span class="math display">\mathcal{H}_K = \left\{g \in L^2(\mathcal{X}):\,\, existem\,\, (c_i)_{i\geq0}\,\, com\,\,\sum_{i\geq1}\frac{c_i^2}{\gamma_i} &lt; \infty\,\,,\, tais\,\, que\,\, g({\bf x}) = \sum_{i\geq1} c_i\phi_i({\bf x}) \right\}.</span></p>
<p><br></p>
<p>Diremos que <span class="math inline">\mathcal{H}_k</span> é o <em>Reproducing Kernel Hilbert Space</em> - RKHS associado ao kernel <span class="math inline">K</span>, em que a norma de uma função <span class="math inline">g({\bf x}) = \sum_{i\geq0}c_i\phi({\bf x})</span> é definda por</p>
<p><span class="math display">||g||_{\mathcal{H}_k}^2 := \sum_{i \geq 0} c_i^2/\gamma_i.</span></p>
<p><br></p>
<p>Além disso, tem-se que <span class="math inline">\mathcal{H}_K</span> é <strong>única</strong> para um dado <span class="math inline">K</span>, muito embora a decomposição dada pela <a href="#/suport-vector-regression-machine-6" class="quarto-xref">Equação&nbsp;6</a> não seja única.</p>
</section>
<section id="suport-vector-regression-machine-8" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br> O Teorema que segue, frequentemente atribuído a ao artigo Kimeldorf, G. S. &amp; Wahba, G. (1970). <strong>A correspondence between Bayesian estimation on stochastic processes and smoothing by splines</strong>. The Annals of Mathematical Statistics, 495–502, simplifica o problema de otimizar a função objetivo dada na <a href="#/suport-vector-regression-machine-1" class="quarto-xref">Equação&nbsp;5</a>.</p>
<p><br></p>
<p><span class="red">Teorema da Representação</span>: Seja <span class="math inline">K</span> um kernel de Mercer correspondente ap RKHS <span class="math inline">\mathcal{H}_K</span>. Considere o conjunto de treinamento <span class="math inline">({\bf x}_1, y_1), \cdots, ({\bf x}_n, y_n)</span> e uma função de perda arbitrária <span class="math inline">L</span>. Então, a solução de</p>
<p><br></p>
<p><span id="eq-teorema-representacao"><span class="math display">\argmin_{g \in \mathcal{H}_K} \sum_{k = 1}^n L(g({\bf x}_k), y_k) + \lambda||g||_{\mathcal{H}_K^2}, \tag{7}</span></span> existe, e é única, em que</p>
<p><br></p>
<p><span class="math display">g({\bf x}) = \sum_{k=1}^n \alpha_k K({\bf x}_k, {\bf x}),</span> em que <span class="math inline">\alpha_1, \cdots, \alpha_n</span> é uma sequência de valores reais.</p>
</section>
<section id="suport-vector-regression-machine-9" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>É possível desmontrar que a otimização da <a href="#/suport-vector-regression-machine-8" class="quarto-xref">Equação&nbsp;7</a> poderá se dar pela otimização de</p>
<p><br></p>
<p><span class="math display">\argmin_{\alpha_1, \cdots, \alpha_n}\sum_{k = 1}^n L\left(\overbrace{\sum_{i=1}^n\alpha_iK({\bf x}_i, {\bf x})}^{g({\bf x})}, y_k \right) + \lambda\underbrace{\sum_{1 \leq j,k \leq n}\alpha_i\alpha_k K({\bf x}_j, {\bf x}_k)}_{||g||_{\mathcal{H}_K}^2}.</span></p>
<p><br></p>
<p>Portanto, o problema de otimizar a função objetivo dada na <a href="#/suport-vector-regression-machine-1" class="quarto-xref">Equação&nbsp;5</a> se reduz a encontrar os valores de <span class="math inline">\alpha_1, \cdots, \alpha_n</span> que minimiza a <a href="#/suport-vector-regression-machine-8" class="quarto-xref">Equação&nbsp;7</a>. Portanto, irá especificar o kernel <span class="math inline">K</span> e a função de perda <span class="math inline">L</span>, em que <span class="math inline">\lambda</span> é um parâmetro de sintonização (hiperparâmetro) que poderá ser obtido por uma validação cruzada.</p>
</section>
<section id="suport-vector-regression-machine-10" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p>Em se tratando de estimadores de regressão, <em>Support Vector Regression Machines</em>, utiliza-se uma função de perda diferente da quadrática, como definido em Drucker, H., Burges, C. J., Kaufman, L., Smola, A. J. &amp; Vapnik, V. (1997). <strong>Support vector regression machines</strong> em Advances in neural information processing systems.</p>
<p><br></p>
<p>Eles definem a seguinte função de perda <span class="math inline">L(g({\bf x}_k, y_k)) = (|y_k - g({\bf x}_k)| - \varepsilon)_{+}</span>, que assume o valor 0 se <span class="math inline">|y_k - g({\bf x}_k)| &lt; \varepsilon</span> e assumirá <span class="math inline">|y_k - g({\bf x}_k)| - \varepsilon</span>, caso contrário.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="suport-vector-regression-machine-11" class="slide level2">
<h2>Suport Vector Regression Machine</h2>
<p><br></p>
<p><span class="red">Exemplo</span>: Vamos consirar a base de dados de vinho vermelho🍷, disponíveis <a href="https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009">aqui</a>, faça uma pequena análise exploratória dos dados. No <a href="https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009">link</a> do Kaggle você consegue uma explicação sobre o que significa cada uma das variáveis.</p>
<p><br></p>
 <iframe id="example1" src="reports_code/tufte_svm_regressao.html" style="border: none; width: 100%; height: 90%" frameborder="0"></iframe>
</section>
<section id="exercícios-6" class="slide level2">
<h2>📚 Exercícios</h2>
<p><br></p>
<p><span class="red">Exercício</span>: Considere o problema em que o objetivo é prever a <strong>pontuação</strong> (<em>score</em>) / (item 2) do aluno com base em algumas variáveis. São elas:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Horas estudadas</strong>: o número total de horas gastas estudando por cada aluno;</li>
<li class="fragment"><strong>Pontuação</strong>: As notas obtidas pelos alunos em testes anteriores;</li>
<li class="fragment"><strong>Atividades</strong> Extracurriculares: Se o aluno participa de atividades extracurriculares (Sim ou Não);</li>
<li class="fragment"><strong>Horas de sono</strong>: o número médio de horas de sono que o aluno teve por dia;</li>
<li class="fragment"><strong>Amostras de perguntas praticadas</strong>: O número de amostras de perguntas que o aluno praticou.</li>
</ol>
<p><br></p>
<p>Você poderá baixar e ter uma descrição maior da base de dados clicando <a href="https://www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression">aqui</a>. Avalie o poder preditivo do modelo lasso com o do SVRM. Sua análise deve conter uma análise exploratória dos dados, deverá utilizar um esquema de <span class="math inline">k</span>-<em>folds cross-validation</em>, com <span class="math inline">k = 20</span> e considerar um esquema de <em>data splitting</em> na proporção <span class="math inline">90\%</span> para treino e <span class="math inline">10\%</span> para teste. Sua análise deverá estar em um notebook de <a href="https://quarto.org/">quarto</a>, em que você deverá comentar cada passo da análise.</p>
</section>
<section id="exercícios-7" class="slide level2">
<h2>📚 Exercícios</h2>
<p><br></p>
<p><span class="red">Exercício</span>: Refaça o exercício anterior considerando a base de dados de dados pessoais de custos médicos disponível <a href="https://www.kaggle.com/datasets/mirichoi0218/insurance">aqui</a>. As informações contidas na base são:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Idade</strong> (<code>age</code>): idade do beneficiário principal;</li>
<li class="fragment"><strong>Sexo</strong> (<code>sex</code>): sexo do beneficiário;</li>
<li class="fragment"><strong>Índice de massa corporal - IMC</strong> (<code>bmi</code>): <span class="math inline">IMC = \frac{Peso}{altura^2}</span>, com peso em <span class="math inline">Kg</span> e altura em <span class="math inline">m</span> (metro);</li>
<li class="fragment"><strong>Número de filhos</strong> (<code>children</code>): número de filhos cobertos pelo plano de saúde;</li>
<li class="fragment"><strong>Fumante</strong> (<code>smoker</code>): variável <em>dummy</em> que informa se o beneficiário é ou não fumante;</li>
<li class="fragment"><strong>Região</strong> (<code>region</code>): região dos EUA em que o beneficiário reside (<em>northeast</em>, <em>southeast</em>, <em>southwest</em> ou <em>northwest</em>);</li>
<li class="fragment"><strong>Custo</strong> (<code>charges</code>): custos médicos cobrados, em dólares.</li>
</ol>
<p><br></p>
<p>O objetivo é prever o custo (<code>charges</code>) com base nas demais informações. <strong>Dica</strong>: utilizando a biblioteca <a href="https://recipes.tidymodels.org/">recipes</a>, utilize a função <code>step_dummy</code> especificando o argumento <code>one_hot = TRUE</code> para realizar um <em>one hot encoding</em> com a variável <code>region</code>. Dessa forma, <code>region</code> deixará ser ser uma variável nominal e se tornará uma variável numérica.</p>
</section>
<section id="árvores-de-regressão" class="slide level2">
<h2>🌳 Árvores de regressão</h2>
<p><br></p>
<p>Em aprendizagem de máquina, uma arvore de regressão consiste em uma metodologia não-paramétrica que nos leva a resultados que são facilmente interpretáveis. A árvore é construirda por meio de particionamentos recursivos no espaço das covariáveis. Cada partição recebe o nome de <span class="red">nó</span> e o resultado final (valor da regressão) é denominado de <span class="red">folha</span> 🍃.</p>
<p><br></p>
<p><span class="red">Exemplo</span>: Construindo uma árvore de regressão, usando a biblioteca <a href="https://cran.r-project.org/web/packages/rpart/index.html">rpart</a> para construção da árvore e a biblioteca <a href="https://cran.r-project.org/web/packages/rpart.plot/index.html">rpart.plot</a> para a plotagem da árvore estimada.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb18-3"><a href="#cb18-3"></a></span>
<span id="cb18-4"><a href="#cb18-4"></a>dados <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(<span class="at">file =</span> <span class="st">"dados/winequality-red.csv"</span>, <span class="at">show_col_types =</span> <span class="cn">FALSE</span>)</span>
<span id="cb18-5"><a href="#cb18-5"></a>arvore <span class="ot">&lt;-</span>  rpart<span class="sc">::</span><span class="fu">rpart</span>(<span class="at">formula =</span> quality <span class="sc">~</span> ., <span class="at">data =</span> dados)</span>
<span id="cb18-6"><a href="#cb18-6"></a><span class="fu">rpart.plot</span>(arvore, <span class="at">type =</span> <span class="dv">4</span>, <span class="at">extra =</span> <span class="dv">1</span>)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="index_files/figure-revealjs/unnamed-chunk-19-1.png" width="960" class="r-stretch"><p>Perceba que há um particionamento binário em cada nó da árvore 🌳 e que poderemos sequir para a aresta da esquerda quando a condição no nó for verdadeira e para direita caso contrário. Dada uma nova observação <span class="math inline">{\bf x}_i</span>, poderemos seguir as condições da árvore até chegar a uma folha 🍃 dessa árvore. Nesse caso, a folha 🍃 contém uma estimativa da qualidade do vinho.</p>
<p><br></p>
<p>Por exemplo, na árvore acima podemos perceber que vinhos com teor alcoólico inferior à 11 nos conduzem a vinhos 🍷 de qualidade inferior. Temos que vinhos com teor alcoólico maior que 11 e com sulfato maior que 0.65 nos levam à ótimos vinhos, segundo o conjunto de dados utilizado.</p>
<p><br></p>
<p>Perceba que dado <span class="math inline">{\bf x}_i</span>, poderemos percorrer a árvore a mão.</p>
</section>
<section id="árvores-de-regressão-1" class="slide level2">
<h2>🌳 Árvores de regressão</h2>
<p><br></p>
<p>Formalmente, temos que a metodologia de estimação de uma árvore de regressão cria uma partição no espaço das covariáveis em regiões distintas e disjuntas, que denotaremos de <span class="math inline">R_1, R_2, \cdots, R_j</span>, com <span class="math inline">R_a \cap R_b</span>, <span class="math inline">\forall a,b \in 1, \cdots, j</span>. A predição é dada por:</p>
<p><span class="math display">g({\bf x}) = \frac{1}{|\{i:{\bf x}_i \in R_k\}|}\sum_{i:{\bf x}_i \in R_k} y_i.</span></p>
<p><br></p>
<p><strong>A construção de uma árvore de regressão envolve dois passos principais</strong>:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><p>A criação de uma árvore complexa que nos leve a partições “puras”, i.e., a partições nas observações do conjunto de covariáveis que nos leve a valores de <span class="math inline">Y</span>, no conjunto de treinamento em cada uma das folhas sejam homogêneas;</p></li>
<li class="fragment"><p>Podar a árvore, com a finalidade de evitarmos super-ajuste (<em>overffiting</em>), e portanto, termos uma alta variância do modelo.</p></li>
</ol>
</section>
<section id="árvores-de-regressão-2" class="slide level2">
<h2>🌳 Árvores de regressão</h2>
<p><br></p>
<p>A ideia de uma árvore 🌳 pura é de que “todo mundo” que cai em uma dada folha é muito homogêneo em relação a variável resposta (ao rótulo/<em>label</em>).</p>
<p><br></p>
<p>Em uma árvore de regressão, a maneira mais simples de definir pureza é utilizar o EQM.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="árvores-de-regressão-3" class="slide level2">
<h2>🌳 Árvores de regressão</h2>
<p><br></p>
<p>No passo 1, para avaliarmos o quão razoável é uma árvore <span class="math inline">T</span>, utilizamos o Erro Quadrático Médio - EQM, em que</p>
<p><span class="math display">\mathcal{P}(T) = \sum_{R}\sum_{i:{\bf x}_i \in R} \frac{(y_i - \widehat{y}_R)^2}{n},</span> em que <span class="math inline">\widehat{y}_R</span> é o valor predito de <span class="math inline">y</span> para a resposta de uma observação pertencente à região <span class="math inline">R</span>. No gráfico de uma árvore de regressão, o <span class="math inline">R</span> é dado por todos os indivíduos na base de dados que estão em uma dada folha 🍃.</p>
<p><br></p>
<p>Encontrar uma árvore <span class="math inline">T</span> que minimize <span class="math inline">\mathcal{P}(T)</span> é uma tarefa computacionalmente cara. Por isso, que os algoritmos de estimação de <span class="math inline">T</span> normalmente utilizam partições binárias, como no exemplo anterior.</p>
<p><br></p>
<p>Existem diversos algoritmos utilizados para estimação de <span class="math inline">T</span>, em que o <span class="red"><em>C</em></span><em>lassification <span class="red">A</span>nd <span class="red">R</span>egression <span class="red">T</span>ree</em> - <span class="red">CART</span> é o mais conhecido. O algoritmo foi estabelecido no livro Breiman L., Friedman J. H., Olshen R. A., and Stone, C. J. (1984) <strong>Classification and Regression Trees</strong>. Wadsworth.</p>
</section>
<section id="árvores-de-regressão-4" class="slide level2">
<h2>🌳 Árvores de regressão</h2>
<p><br></p>
<p>O algoritmo particiona o espaço de covariáveis em duas regiões disjuntas. Para a escolha dessa partição, busca-se, dentre todas as covariáveis <span class="math inline">x_i</span> e cortes <span class="math inline">t_1</span>, a combinação que conduz a uma partição <span class="math inline">(R_1, R_2)</span> com menor predições de erro quadrático, i.e., dado um nó, a partição é construída de modo a minimizar:</p>
<p><span id="eq-sse-tree"><span class="math display">\overbrace{SSE}^{\text{sum of squares error}} = \sum_{i:{\bf x}_i \in R_1}^n (y_i - \widehat{y}_{R_1})^2 + \sum_{i:{\bf x}_i \in R_2}^n (y_i - \widehat{y}_{R_2})^2, \tag{8}</span></span> em que <span class="math inline">\widehat{y}_{R_k}</span> é a predição de <span class="math inline">y</span> fornecida pela região <span class="math inline">R_k</span> e SSE é denominado <span class="red"><em>S</em></span><em>um of <span class="red">S</span>quares of <span class="red">E</span>rrors</em> - SSE.</p>
<p><br></p>
<p>Assim, tem-se que o algoritmo irá fornecer:</p>
<p><br></p>
<p><span class="math display">R_1 = \{{\bf x} : {\bf x}_i &lt; t_1\}\, \mathrm{e}\, R_2 = \{{\bf x} : {\bf x}_i \geq t_1\},</span> em que <span class="math inline">x_i</span> é a variável escolhida e <span class="math inline">t_1</span> é o corte definido.</p>
</section>
<section id="árvores-de-regressão-5" class="slide level2">
<h2>🌳 Árvores de regressão</h2>
<p><br></p>
<p>Uma vez que estabelecemos um nó raiz, este é fixado. No passo seguinte, o algoritmo irá particionar as regiões <span class="math inline">R_1</span> e <span class="math inline">R_2</span> em regiões menores, seguindo o mesmo critério,tanto para <span class="math inline">R_1</span> quanto para <span class="math inline">R_2</span>. O algoritmo continua de forma recursiva até que tenhamos uma árvore com poucas observações em uma das folhas 🍃.</p>
<p><br></p>
<p>Por exemplo, podemos decidir em parar de tornar a árvore profunda quando em cada folha tivermos menos de 5 observações. Porém, essa árvore criada irá produzir boas predições para o conjunto de treinamento, porém, não irá performar bem em novas observações. Isso, por conta, do <em>trade off</em> entre viés e variância. Em outras palavas, haverá <em>overfitting</em>.</p>
<p><br> Na etapa do processo de poda, cada nó é retirado, um por vez, e observa-se como o erro de predição varia no conjunto de validação. Com base nisso, decide-se quais nós permanecem na árvore. O processo de poda reduz o <em>overffiting</em> do modelo.</p>
</section>
<section id="árvores-de-regressão-6" class="slide level2">
<h2>🌳 Árvores de regressão</h2>
<p><br></p>
<p>Uma observação importante é que sempre é bom crescer a árvore ao máximo possível e depois proceder com a etapa de “poda”, em que removemos os ramos mais profundos e reavaliamos o poder preditivo da árvore 🌳 <span class="math inline">T</span>. Isso porquê a melhoria do poder preditivo da árvore não é linear, i.e., as vezes podemos ter uma divisão que piore um pouco a capacidade preditiva da árvore, porém, a divisão seguinte poderá dar um grande salto de melhoria. Dessa forma, é mais conveniente deixar a árvore “profunda” para depois sairmos “podando” ✂️ a árvore 🌳.</p>
<p><br></p>

<img data-src="gifs/cortando-a-arvore-fail.gif" class="r-stretch"></section>
<section id="árvores-de-regressão-7" class="slide level2">
<h2>🌳 Árvores de regressão</h2>
<p><br></p>
<p>Para limitar a profundidade da arvore <span class="math inline">T</span> a ser estimada, um parâmetro de penalização/complexidade poderá ser introduzido na <a href="#/árvores-de-regressão-4" class="quarto-xref">Equação&nbsp;8</a>. Assim, o problema consistem em obter regiões e pontos de cortes que minimize:</p>
<p><span class="math display">SSE + \alpha|T|,</span> em que <span class="math inline">\alpha &gt; 0</span> é o hiperparâmetro de complexidade do modelo e <span class="math inline">|T|</span> é um valor inteiro que define a profundidade máxima da árvore. Por exemplo, <span class="math inline">|T|</span> na biblioteca <a href="https://cran.r-project.org/web/packages/rpart/index.html">rpart</a> é definido pelo parâmetro <code>tree_depth</code> em 30. Normalmente nos concentramos em em tunar o parâmetro <code>cost_complexity</code> que é o <span class="math inline">\alpha</span>.</p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="árvores-de-regressão-8" class="slide level2">
<h2>🌳 Árvores de regressão</h2>
<p><br></p>
<p><span class="red">Exemplo</span>: Poderemos podar a árvore usando a função <code>prune</code> do pacote <a href="https://cran.r-project.org/web/packages/rpart/index.html">rpart</a>. Considerando o exemplo anterior, tornando a árvore menos compleça, poderíamos decidir em podar alterando o seu parâmetro de complexidade.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb19-2"><a href="#cb19-2"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb19-3"><a href="#cb19-3"></a></span>
<span id="cb19-4"><a href="#cb19-4"></a>dados <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(<span class="at">file =</span> <span class="st">"dados/winequality-red.csv"</span>)</span>
<span id="cb19-5"><a href="#cb19-5"></a></span>
<span id="cb19-6"><a href="#cb19-6"></a>arvore <span class="ot">&lt;-</span> dados <span class="sc">|&gt;</span> </span>
<span id="cb19-7"><a href="#cb19-7"></a>  rpart<span class="sc">::</span><span class="fu">rpart</span>(<span class="at">formula =</span> quality <span class="sc">~</span> ., <span class="at">data =</span> _) </span>
<span id="cb19-8"><a href="#cb19-8"></a></span>
<span id="cb19-9"><a href="#cb19-9"></a><span class="co"># O melhor parâmetro de custo</span></span>
<span id="cb19-10"><a href="#cb19-10"></a>melhor_grau <span class="ot">&lt;-</span> arvore<span class="sc">$</span>cptable[<span class="fu">nrow</span>(arvore<span class="sc">$</span>cptable),][1L]</span>
<span id="cb19-11"><a href="#cb19-11"></a></span>
<span id="cb19-12"><a href="#cb19-12"></a><span class="co"># Aumentando o parâmetro de custo de 0.01 para 0.04</span></span>
<span id="cb19-13"><a href="#cb19-13"></a>arvore_podada <span class="ot">&lt;-</span> rpart<span class="sc">::</span><span class="fu">prune</span>(<span class="at">tree =</span> arvore, <span class="at">cp =</span> <span class="fl">0.04</span>)</span>
<span id="cb19-14"><a href="#cb19-14"></a></span>
<span id="cb19-15"><a href="#cb19-15"></a><span class="co"># Plotando a árvore podada</span></span>
<span id="cb19-16"><a href="#cb19-16"></a>rpart.plot<span class="sc">::</span><span class="fu">rpart.plot</span>(arvore_podada)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="index_files/figure-revealjs/unnamed-chunk-20-1.png" width="960" class="r-stretch"></section>
<section id="árvores-de-regressão-9" class="slide level2">
<h2>🌳 Árvores de regressão</h2>
<p><br> O parâmetro de complexidade/custo é um hiperparâmetro, e deverá ser estimado dentro de um procedimento de validação cruzada.</p>
<p><br></p>

<img data-src="gifs/bom.gif" class="r-stretch"></section>
<section id="árvores-de-regressão-10" class="slide level2">
<h2>🌳 Árvores de regressão</h2>
<p><br> <strong>Algumas observações sobre a árvore 🌳 de regressão</strong>:</p>
<p><br></p>
<ol type="1">
<li class="fragment">É um método não-paramétrico;</li>
<li class="fragment">Pode ser representado graficamente;</li>
<li class="fragment">É útil na análise exploratória dos dados do problema em questão;</li>
<li class="fragment">Pode ser utilizada para selecionar variáveis. Aparentemente, as variáveis que pertence à árvore tem uma maior importância para o problema em questão;</li>
<li class="fragment">Poderá trabalhar com variáveis numéricas, mas também poderá trabalhar com variáveis categóricas;</li>
<li class="fragment">A árvore é robusta na presença de variáveis irrelevantes.</li>
</ol>
<p><br></p>

<img data-src="gifs/chapulin-colorado-no.gif" class="r-stretch"></section>
<section id="árvores-de-regressão-11" class="slide level2">
<h2>🌳 Árvores de regressão</h2>
<p><br></p>
<p>O nível de complexidade é obtido via <em>cross-validation</em> de modo a encontrar a menor subárvore que generaliza melhor o problema para dados não visto.</p>
<p><br></p>
<p>Assim como nos modelos de regressão com penalidade que vimos anteriormente, aqui, para valores menores de <span class="math inline">\alpha</span> tende a produzir modelos mais complexos. Consequentemente, à medida que uma árvore cresce, a redução no <span class="math inline">SSE</span> deve ser maior do que a penalidade de complexidade de custo.</p>
<p><br></p>

<img data-src="gifs/chaves-isso.gif" class="r-stretch"></section>
<section id="árvores-de-regressão-12" class="slide level2">
<h2>🌳 Árvores de regressão</h2>
<p><br></p>
<p>Experimente alterar o parâmetro de complexidade e perceba como ele influencia nas regições da árvore de regressão. Quando maior o valor, maior a penalidade, e portanto, mais simples será a árvore de regressão que irá estimar os valores de <span class="math inline">Y_i</span> com base em <span class="math inline">X_i</span>.</p>
<p><br></p>
 <iframe id="example1" src="https://pedro-rafael.shinyapps.io/shiny_apps/#section-%C3%A1rvore-de-regress%C3%A3o" style="border: none; width: 100%; height: 90%" frameborder="0"></iframe>

<img data-src="gifs/giphy.gif" class="r-stretch"></section>
<section id="árvores-de-regressão-13" class="slide level2">
<h2>🌳 Árvores de regressão</h2>
<p><br></p>
<p><strong>Algumas outras características das 🌳 árvores de regressão são</strong>:</p>
<p><br></p>
<ol type="1">
<li class="fragment">A função de regressão estimada <span class="math inline">\widehat{r}({\bf x})</span> é <strong>constante por partes</strong>, i.e., em uma folha 🍃 tendemos predizer que vários indivíduos distintos tem o mesmo valor de <span class="math inline">\widehat{r}({\bf x})</span>;</li>
<li class="fragment">Em uma árvore 🌳 de regressão as interações entre variáveis são consideradas de forma automática, enquanto em uma regressão as interações são introduzidas como os produtos entre covariáveis;</li>
<li class="fragment">Elas são uma espécie de “samambaias”, em que crescem para baixo;</li>
<li class="fragment">É fácil introduzir variáveis categóricas 🎉;</li>
<li class="fragment">Uma pessoa sem muito conhecimento poderá estimar <span class="math inline">\widehat{r}({\bf x})</span>, dada uma árvore, para cada nova observação <span class="math inline">{\bf x}</span>.</li>
</ol>

<img data-src="gifs/hum_02.gif" class="r-stretch"></section>
<section id="exercícios-8" class="slide level2">
<h2>📚 Exercícios</h2>
<p><br></p>
<p><span class="red">Exercício</span>: Considere a variável aleatória <span class="math inline">Y_i \sim \mathcal{N}(\sin(X_i), \sigma^2)</span>, com <span class="math inline">X_i \in \mathcal{U}(0, 10)\,, \forall i.</span> Utilizando a biblioteca <strong>rpart</strong>, implemente a função <code>arvore(n = 250L, complexidade, sigma = 0.1)</code> que devolve o gráfico <em>scatterplot</em> com os pontos <span class="math inline">X_i</span> e <span class="math inline">Y_i</span> e por cima deles o gráfico de linha com os valores preditos. A função deverá ter três argumentos, <code>n</code>, <code>complexidade</code> e <code>sigma</code>, que são o tamanho da amostra, o grau de complexidade do modelo, e o desvio padrão, respectivamente. Aqui não se preocupe com divisão entre treino e teste nem validação cruzada. A solução desse exercício não tem como objetivo encontrar o melhor hiperparâmetro, i.e., não é necessário “tunar” o parâmetro <code>complexidade</code>. A função deverá retornar algo como:</p>

<img data-src="index_files/figure-revealjs/unnamed-chunk-21-1.png" width="960" class="r-stretch"></section>
<section id="exercícios-9" class="slide level2">
<h2>📚 Exercícios</h2>
<p><br></p>
<p><span class="red">Exercício</span>: Ainda com base no exercício anterior, construa uma função que retorne várias estimativas obtidas por árvores de regressão, com base em várias amostra de <span class="math inline">X_i</span> e <span class="math inline">Y_i</span>. O gráfico deverá mostrar as estimativas das diversas árvores de regressão, sem mostrar os pontos. Perceba a flutuação das estimativas em diferentes valores de <code>complexidade</code>, dado <code>n</code> e <code>sigma</code> fixos.</p>
<p><br></p>
<p><span class="red">Exercício</span>: Sem utilizar a biblioteca <strong>tidymodels</strong>, apenas as bibliotecas <strong>rsample</strong> e <strong>rpart</strong>, treine um modelo com 10 mil observações geradas. No procedimento <span class="math inline">k</span>-folds cross-validation, para <span class="math inline">k = 20</span>, encontre um bom valor para o grau de complexidade considerando um <em>grid</em> de possíveis valores. <em>Dica</em>: experimente testar um parâmetro dentro do conjunto de treino no procedimento de <em>cross-validation</em>. Por exemplo, experimente criar um <em>grid</em> com valores entre <span class="math inline">0.001</span> e <span class="math inline">0.4</span>. Aprensente o gráfico com a estimativa do melhor modelo. Não esqueça de fixar um valor de semente, para que os resultados possam ser reproduzidos.</p>
<p><br></p>
<p><span class="red">Exercício</span>: Refaça o exercício anterior utilizando a biblioteca <strong>tidymodels</strong>. Fique livre para tunar o hiperparâmetro que achar necessário, da <em>engine</em> que utilizar com a biblioteca <strong>parsnip</strong>. Compare o risco preditivo do exercício anterior com o que você obteve utilizando o <strong>tidymodels</strong>.</p>
</section>
<section id="combinando-predições---bagging" class="slide level2">
<h2>Combinando predições - bagging</h2>
<p><br></p>
<p>As árvores 🌳 de regressão tem a característica de ser facilmente interpretável, porém, costumam ser uma capacidade preditiva baixa.</p>
<p><br></p>
<p>Uma das ideias de melhorar a capacidade preditiva é combinar árvores usando a metodologia de reamostragem <em>bootstrap</em>. As técnicas de reamostragem via bootstrap não-paramétrico são bastante difundidas na estatística e consiste reamostrar da amostra original com reposição.</p>
<p><br></p>
<p>Na estatística, a ideia de <em>bootstrap</em> é muito comum para correção de víes de um estimador, cálculo do erro-padrão de um estimador, construção de intervalos de confianças e teste de hipóteses.</p>
<p><br></p>
<p>Em aprendizagem de máquina, o conceito de <em>bagging</em> consiste em reaplicar uma metodologia, nesse caso a de árvore 🌳 de regressão em diferentes amostras obtidas da amostra original com reposição.</p>
<p><br></p>

<img data-src="gifs/laughing-ha.gif" class="r-stretch"></section>
<section id="combinando-predições---bagging-1" class="slide level2">
<h2>Combinando predições - bagging</h2>
<p><br> Caso você tenha interesse e entender o método de <em>bootstrap</em>, no contexto mais difundido na estatística como mencionado no slide anterior, assista a vídeo aula sobre bootstrap do Prof.&nbsp;Pedro Rafael do Departamento de Estatística da Universidade Federal da Paraíba - UFPB.</p>
<p><br></p>
<iframe data-external="1" src="https://www.youtube.com/embed/XEQlzfVc6lI" width="50%" height="50%" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</section>
<section id="combinando-predições---bagging-2" class="slide level2">
<h2>Combinando predições - bagging</h2>
<p><br></p>
<p>Portanto, é importante ficar claro que o <em>bagging</em> é uma metodologia genérica que poderá ser aplicada em situações ao qual desejamos combinar modelos. Por exemplo, o método <a href="https://en.wikipedia.org/wiki/Random_forest">ramdom forest</a> que veremos mais a frente é uma pequena modificação de um <em>bagging</em> de árvores de regressão que vimos anteriormente. Na verdade o <a href="https://en.wikipedia.org/wiki/Random_forest">ramdom forest</a>, assim como as árvores de decisões podem ser utilizadas também para problemas de classificação, como veremos mais adiante no curso.</p>
<p><br></p>

<img data-src="gifs/ok-2.gif" class="r-stretch"></section>
<section id="combinando-predições---bagging-3" class="slide level2">
<h2>Combinando predições - bagging</h2>
<p><br></p>
<p>A ideia de combinar árvores de regressão é interessante, uma vez que o risco preditivo da média das previsões das árvores é menor que o risco individual de cada uma das árvores.</p>
<p><br></p>
<p>Lembre-se que quando falávamos em momentos anteriores do curso sobre o balanço entre viés e variância, apresentamos uma decomposição do risco quadrático <span class="math inline">R(g)</span> condicional a um novo <span class="math inline">{\bf x}</span> dada na <a href="#/balanço-viés-e-variância-1" class="quarto-xref">Equação&nbsp;2</a>. Para que não seja necessário necessário voltar um grande número de slides, recoloco a decomposição abaixo:</p>
<p><br></p>
<p><span class="math display">\mathbb{E}\left[(Y - \widehat{g}({\bf X}))^2| {\bf X} = {\bf x}\right] = \underbrace{\mathbb{V}[Y | {\bf X = x}]}_{\mathrm{i - Variância\,\, intrínseca}} + \overbrace{(r({\bf x}) - \mathbb{E}[\widehat{g}({\bf x})])^2}^{\mathrm{ii - Viés\, ao\, quadrado\, do\, modelo}} + \underbrace{\mathbb{V}[\widehat{g}({\bf x})]}_{\mathrm{iii - Variância\, do\, modelo}}.</span></p>
</section>
<section id="combinando-predições---bagging-4" class="slide level2">
<h2>Combinando predições - bagging</h2>
<p><br></p>
<p>Ainda no caso de árvores 🌳 de regressão, suponha um caso mais simples, em que temos dois modelos de árvores de regressão, sejam eles <span class="math inline">g_1</span> e <span class="math inline">g_2</span>, respectivamente, em que a previsão combinada é dada por:</p>
<p><span class="math display">\widehat{g}({\bf x}) = \frac{\widehat{g_1}({\bf x})  + \widehat{g_2}({\bf x})}{2},</span> em que <span class="math inline">\widehat{g_1}({\bf x})</span> e <span class="math inline">\widehat{g_2}({\bf x})</span> são as estimativas da função de regressão <span class="math inline">r({\bf x})</span> fornecidades pelas árvores <span class="math inline">g_1</span> e <span class="math inline">g_2</span>, respectivamente.</p>
<p><br></p>
<p>Supondo que <span class="math inline">\widehat{g_1}({\bf x})</span> e <span class="math inline">\widehat{g_2}({\bf x})</span> são não-viesados e possuem a mesma variância, e além disso são não correlacionados então:</p>
<span class="math display">\begin{align*}
\mathbb{E}[(Y - g({\bf x}))^2|{\bf x}] &amp;= \mathbb{V}[Y|{\bf x}] + \frac{1}{4}(\mathbb{V}[\widehat{g}_1({\bf x}) + \widehat{g}_2({\bf x})|{\bf x}]) \\
&amp;\quad + \left(\mathbb{E}[Y|{\bf x}] - \frac{\mathbb{E}[\widehat{g}_1({\bf x})|{\bf x}]+\mathbb{E}[\widehat{g}_2({\bf x})|{\bf x}]}{2} \right)^2
\end{align*}</span>
</section>
<section id="combinando-predições---bagging-5" class="slide level2">
<h2>Combinando predições - bagging</h2>
<p><br></p>
<p>Note que se <span class="math inline">\widehat{g_1}({\bf x})</span> e <span class="math inline">\widehat{g_2}({\bf x})</span> então <span class="math inline">\mathbb{E}(\widehat{g}_1({\bf x})|{\bf x}) = \mathbb{E}(\widehat{g}_2({\bf x})|{\bf x}) = r({\bf x})</span>. Isso faz com que a última parcela da equação anterior seja zero. Se além disso, se os estimadores possuem a mesma variância, então a decomposição do risco preditivo combinado é simplificada e dada por:</p>
<p><span id="eq-risco-combinado-decom"><span class="math display">\overbrace{\mathbb{E}[(Y - \widehat{g}({\bf x}))^2|{\bf x}]}^{\text{Risco preditivo combinado}} = \mathbb{V}[Y|{\bf x}] + \frac{1}{2}\mathbb{V}[\widehat{g}_i({\bf x})| {\bf x}] \leq \underbrace{\mathbb{E}[(Y - \widehat{g}_i({\bf x}))^2 | {\bf x}]}_{\text{Risco preditivo individual}}, \tag{9}</span></span> para um dado <span class="math inline">i</span>, com <span class="math inline">i = 1, 2</span>. Dado as suposições de estimadores não-viesados, variância iguais e que os estimadores são não-correlacionados, a <a href="#/combinando-predições---bagging-5" class="quarto-xref">Equação&nbsp;9</a> poderia ser generalizada para o caso de mais de dois estimadores, no nosso caso, para mais de duas árvores de regressão 🌳. Basta utilizar indução matemática!</p>
<p><br></p>

<img data-src="gifs/bom.gif" class="r-stretch"></section>
<section id="combinando-predições---bagging-6" class="slide level2">
<h2>Combinando predições - bagging</h2>
<p><br></p>
<p>Devemos notar, contudo que, para que tenhamos árvores 🌳 aproximadamente não-viesadas, não devemos “podar” as árvores️ 🌳. Muito embora elas possam aprensenta <em>overfitting</em> quando consideradas individualmente o <strong>risco preditivo combinado irá diminuir</strong>.</p>
<p><br></p>
<p>Portanto, seja <span class="math inline">B</span> o número de pseudo-amostras <em>bootstrap</em>, i.e., amostras obtidas da amostra original com reposição. Então, o estimador combinado em um procedimento de <em>bagging</em> é dado por:</p>
<p><span class="math display">\widehat{g}({\bf x}) = \frac{1}{B}\sum_{b = 1}^B \widehat{g}_b({\bf x}).</span></p>

<img data-src="gifs/bean_01.gif" class="r-stretch"></section>
<section id="combinando-predições---bagging-7" class="slide level2">
<h2>Combinando predições - bagging</h2>
<p><br></p>
<p>Uma observação importante é que o <em>bagging</em> pode ser ruim em modelos quando utilizado em modelos intrinsecamente estáveis, como é o caso de modelos lineares de regressão, <span class="math inline">k</span>NN, regressão logística, entre outros. O procedimento de <em>bagging</em> costuma ser eficaz quando são utilizados em modelos que possuem uma alta variância e que tendem a ter <em>overfitting</em>, como o caso da árvore de decisão profunda (árvores de regressão e de classificação).</p>
<p><br></p>

<img data-src="gifs/bean_01.gif" class="r-stretch"></section>
<section id="combinando-predições---bagging-8" class="slide level2">
<h2>Combinando predições - bagging</h2>
<p><br></p>
<p>Uma forma de perceber a estabilidade de um modelo de aprendizagem de máquina é avaliar as predições do modelo em diferentes partições do conjunto de dados, por exemplo, em um procedimento de validação cruzada repetida. A função <code>rsample::vfold_cv()</code>, permite a possibilidade de retepir uma validação cruzada por meio do argumento <code>repeats</code>, que por <em>default</em> é igual à <span class="math inline">1</span>.</p>
<p><br></p>
<p>Uma outra forma seria utilizar um procedimento de <em>bootstrap</em> não-paramétrico (reamostrar da amostra com reposição) e treinar o modelo em cada pseudo-amostra <em>bootstrap</em> e avaliar a variabilidade das estimativas no conjunto de validação.</p>
<p><br></p>
<p>Uma outra forma seria avaliar a <strong>curva de aprendizado</strong> do modelo. Essa curva poderá ser obtida treinando o modelo em diferentes tamanhos de conjunto de treinamento, em que observa-se como varia a peformance do modelo nos diferentes tamanhos do conjunto de treinamento.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="combinando-predições---bagging-9" class="slide level2">
<h2>Combinando predições - bagging</h2>
<p><br></p>
<p>Muito embora a combinação preditiva usando <em>bagging</em> de um conjunto de árvores de regressão não são tão fáceis de interpretar quando comparada com uma única árvore de regressão, ele permite que possamos criar uma medida de importância para cada variável. Essa medida baseia-se na redução da <em>Residual Sum of squares</em> - RSS <strong>de cada divisão</strong>.</p>
<p><br></p>

<img data-src="imgs/dendograma_simples.png" class="r-stretch quarto-figure-center"><p class="caption">Uma simples divisão binária em qualquer ponto de uma dada árvore 🌳 de regressão. Desejamos encontrar a importância da variável “pai” que poderá aparecer em diversas divisões em uma mesma árvore.</p><p><br></p>
<p>Devemos computar a redução da soma dos quadrados dos resíduos em cada nó em que a variável do nó “pai” aparece em todas as árvores obtidas pelo procedimento de <em>bagging</em>, em que calculamos a soma dos quadrados no nó “pai” e subtraímos da soma dos quadrados do nó esquerdo e do nó direito.</p>
</section>
<section id="combinando-predições---bagging-10" class="slide level2">
<h2>Combinando predições - bagging</h2>
<p><br></p>
<p>Assim, a importância da variável no nó “pai”, em uma dada divisão binária em uma dada árvore 🌳 do procedimento de <em>bagging</em> é dada por:</p>
<p><br></p>
<p><span class="math display">\begin{align*}
\text{Importância local} &amp;= RSS_{pai} - RSS_{esq} - RSS_{dir} = \\
&amp; \sum_{i \in pai} (y_i - \overline{y}_{pai})^2 - \sum_{i \in esq} (y_i - \overline{y}_{esq})^2 - \sum_{i \in dir} (y_i - \overline{y}_{dir})^2.
\end{align*}</span></p>
<p><br></p>
<p>Foi denominado de <strong>“Importância local”</strong>, uma vez que o nó “pai” (variável de interesse) poderá aparecer nas diversas <span class="math inline">B</span> árvores de regressão do procedimento <em>bagging</em>, e mais, poderá aparecer várias vezes em uma mesma árvore. Portanto, em todas as <span class="math inline">B</span> e em todas as ocorrências da variável “pai” em qualquer ponto de uma árvore a “Importância local” deverá ser calculada. Ao fim, deverá somar todas as importâncias locais para se ter a <strong>importância global</strong> da variável no nó “pai”.</p>
</section>
<section id="exercícios-10" class="slide level2">
<h2>📚 Exercícios</h2>
<p><br></p>
<p>Considere a variável aleatória <span class="math inline">Y_i \sim \mathcal{N}(\sin(X_i), \sigma^2)</span>, com <span class="math inline">X_i \in \mathcal{U}(0, 10)\,, \forall i</span>.</p>
<p><br></p>
<p><span class="red">Exercício</span>: Avalie a estabilidade de uma árvore de regressão usando um validação cruzada repetida. Construa um gráfico com os riscos observados na validação para um <span class="math inline">\sigma^2</span> fixo.</p>
<p><br></p>
<p><span class="red">Exercício</span>: Avalie a estibilidade usando um procedimento de bootstrap. Construa um gráfico com os riscos observados na validação para um <span class="math inline">\sigma^2</span> fixo.</p>
<p><br></p>
<p><span class="red">Exercício</span>: Por fim, avalie a estabilidade utilizando avaliando a curva de aprendizado do modelo.</p>
<p><br></p>

<img data-src="gifs/bean_01.gif" class="r-stretch"></section>
<section id="random-forest" class="slide level2">
<h2>🌲🐞🌳🐝🌲🦋🌳 Random Forest</h2>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p>O <em>random forest</em> (floresta aleatória) é um procedimento de <em>bagging</em> em que introduz um níveo de aleatoriedade maior no processo de selação das variáveis, visando reduzir ainda mais a correção entre as árvores de regressão. Isso é feito sorteando <span class="math inline">m &lt; d</span> covariáveis em cada particionamento, i.e., essa randomização é feita em toda divisão de todas as <span class="math inline">B</span> árvores do procedimento de <em>bagging</em>, em que <span class="math inline">d</span> é o total de covariáveis consideradas.</p>
<p><br></p>
<p>O valor de <span class="math inline">m</span> poderá ser obtido via algum procedimento de <em>cross-validation</em>, i.e., é um hiperparâmetro que poderá ser “tunado”.</p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="gifs/forest.gif"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="random-forest-1" class="slide level2">
<h2>🌲🐞🌳🐝🌲🦋🌳 Random Forest</h2>
<p><br></p>
<p>A ideia de construir árvores que sejam menos correlacionadas uma com as outras nos aproxima melhor do critério de não correlação que utilizamos para mostrar que o risco preditivo combinado é menor que o risco preditivo ao considerar uma única árvore, como mostra a <a href="#/combinando-predições---bagging-5" class="quarto-xref">Equação&nbsp;9</a>.</p>
<p><br></p>
<p>O algoritmo <em>random forest</em> consegue produzir um estimador com menor variância que o <em>bagging</em>. Além disso, assim como no <em>bagging</em>, podemos calcular a importância de cada variável usando o mesmo procedimento apresentado anteriormente.</p>
<p><br></p>
<p>Muito embora o <em>bagging</em> é um procedimento útil para diminuir a variância de um modelo utilizando predições combinadas, o procedimento de <em>random forest</em> para o caso de árvores de regressão ou de classificação nos conduz a um estimador com menor variância.</p>
<p><br></p>
<p>Um valor de <span class="math inline">m</span> frequentemente considerado é o valor inteiro que aproxima <span class="math inline">\sqrt{d}</span>.</p>
</section>
<section id="random-forest-2" class="slide level2">
<h2>🌲🐞🌳🐝🌲🦋🌳 Random Forest</h2>
<p><br></p>
<p>A aplicação abaixo permite que você possa comparar as estratégias de <em>bagging</em> com o <em>random forest</em>, sendo este um <em>bagging</em> de árvores de regressão com o pequeno ajuste mencionado anteriormente, permitindo termos árvores menos correlacionadas. Perceba que o <em>random forest</em> consegue diminuir ainda mais a variância das previsões de <span class="math inline">Y</span>. A linha <span class="red">vermelha</span> é a distribuição verdadeira.</p>
<p><br></p>
 <iframe id="example1" src="https://pedro-rafael.shinyapps.io/shiny_apps/#section-bagging-vs-random-forest" style="border: none; width: 100%; height: 90%" frameborder="0"></iframe>
<p><br></p>
<p>Se desejar ver de forma ampliada, acesse a aplicação clicando <a href="https://pedro-rafael.shinyapps.io/shiny_apps">aqui</a>.</p>
</section>
<section id="boosting" class="slide level2">
<h2>🚀 Boosting</h2>
<p><br></p>
<p>Da mesma forma que métodos como <em>bagging</em> e <em>random forest</em>, os métodos de <em>boosting</em> também tem como objetivo agregar diferentes estimadores da função de regreção <span class="math inline">r({\bf x})</span>. A ideia desses métodos que combinam diferentes estimadores da função de regressão é melhora a precisão e a performance preditiva dos modelos de máquina, convertendo vários aprendizes fracos em um único modelo de aprendizado forte.</p>
<p><br></p>
<p>O <em>boosting</em> 🚀 funciona construindo os modelos de forma sequencial, dando mais peso às instâncias que foram classificadas incorretamente nos modelos anteriores. O funcionamento do <em>boosting</em> é semelhante ao <em>bagging</em> e <em>random forest</em>, exceto pelo fato de que a árvore irá crescendo sequencialmente: cada árvore é “cultivada” 🌱🚿🌳 usando informações de árvores crescidas. <em>Boosting</em> não envolve amostragem <em>bootstrap</em>; em vez de cada árvore é ajustada em uma versão modificada do conjunto de dados original.</p>
<p><br></p>

<img data-src="gifs/prestando_atencao.gif" class="r-stretch"></section>
<section id="boosting-1" class="slide level2">
<h2>🚀 Boosting</h2>
<p><br></p>
<p>Existem diversas variações e implementações de <em>boosting</em> com diversas implementações distintas em diferentes <em>frameworks</em> de aprendizagem de máquina. Aqui, será descrito o conceito geral, sendo este a forma mais usual do <em>boosting</em>.</p>
<p><br></p>
<p>No <em>boosting</em>, como mencionado anteriormente, o estimador <span class="math inline">g({\bf x})</span> é constrído incrementalmente, i.e., de forma sequencial, melhorando a cada passo. Inicialmente considera-se <span class="math inline">g({\bf x}) \equiv 0</span>. Fazer <span class="math inline">g({\bf x}) \equiv 0</span> estamos iniciando um estimador com alto viés, porém, com variância muito baixa, a saber, variância zero.</p>
<p><br></p>
<p>A cada passo do algoritmo, procuramos obter uma árvore menos viesada, porém, como mencionado em anteriormente, quadando falamos sobre o <em>trade off</em> entre viés e variância, obtemos uma árvore atualizada com uma variância um pouco maior. Por isso é importante partir de uma árvore inicial com variância muito baixa.</p>
<p><br></p>

<img data-src="gifs/hum.gif" class="r-stretch"></section>
<section id="boosting-2" class="slide level2">
<h2>🚀 Boosting</h2>
<p><br></p>
<p>A ideia é construir um estimador para <span class="math inline">r_i</span>, em que na primeira iteração, considera-se <span class="math inline">r_i = y_i</span>. A cada passo subsequente, atualizamos <span class="math inline">r_i</span> para <span class="math inline">r_i = y_i - g({\bf x}_i)</span>, em que <span class="math inline">r_i</span> denominados de resíduo. Portanto, a ideia do <em>boosting</em> é prever o resíduo que inicia-se no rótulo/<em>label</em> <span class="math inline">y_i</span>.</p>
<p><br></p>
<p>Uma observação importante é que as árvores tenham <strong>profundida pequena</strong> de modo a evitar <em>overfitting</em>.</p>
<p><br></p>
<p>Além disso, considera-se uma taxa de aprendizagem (<em>learning rate</em>) que denotaremos por <span class="math inline">\lambda \in [0, 1]</span> que tem como objetivo controlar o super-ajuste. Trata-se de um hiperparâmetro que deverá ser obtido via algum procedimento de <em>cross-validation</em>. Portanto, deveremos “tunar” 🎛 o valor de <span class="math inline">\lambda</span> de modo a encontrar um valor adequado que maximize nosso risco observado, i.e., que maximize a previsão de <span class="math inline">R(g)</span>.</p>
</section>
<section id="boosting-3" class="slide level2">
<h2>🚀 Boosting</h2>
<p><br></p>
<p>Os passos para conseguir boas estimativas de usando o <em>boosting</em> são:</p>
<p><br></p>
<ol type="1">
<li class="fragment">Definimos <span class="math inline">g({\bf x}) \equiv 0</span> e <span class="math inline">r_i = y_i, \, \forall i = 1, \cdots, n</span>;</li>
<li class="fragment">Para cada <span class="math inline">b = 1, \cdots, B</span>, fazemos:</li>
</ol>
<ul>
<li class="fragment">Ajustamos uma árvore com <span class="math inline">p</span> folhas para <span class="math inline">({\bf x}_1, r_1), \cdots, ({\bf x}_n, r_n)</span>, em que denotamos essa função de predição por <span class="math inline">g^b({\bf x})</span>. Lembre-se que estamos estimando <span class="math inline">r_i</span> com base em <span class="math inline">{\bf x}_i</span>;</li>
<li class="fragment">Atualizamos <span class="math inline">g</span> e os resíduos: <span class="math inline">g({\bf x}) \leftarrow g({\bf x}) + \lambda g^b({\bf x})</span> e <span class="math inline">r_i \leftarrow Y_i - g({\bf x})</span>.</li>
</ul>
<ol start="3" type="1">
<li class="fragment">Retorna-se o modelo final <span class="math inline">g({\bf x})</span>.</li>
</ol>
<p>No <em>boosting</em>, os valores de <span class="math inline">\lambda</span>, <span class="math inline">p</span> e <span class="math inline">B</span> são hiperparâmetros e devem ser obtidos por meio de algum procedimento de validação cruzada, i.e., você deverá “tunar” 🎛. É comum que <span class="math inline">\lambda</span> seja pequeno, por exemplo, <span class="math inline">0,01</span> ou <span class="math inline">0,001</span>, <span class="math inline">B \approx 1000</span> e <span class="math inline">p</span> de ordem próxima à <span class="math inline">2</span> ou <span class="math inline">4</span>.</p>
</section>
<section id="boosting-4" class="slide level2">
<h2>🚀 Boosting</h2>
<p><br></p>
<p>Considere a variável aleatória <span class="math inline">Y_i \sim \mathcal{N}(\sin(X_i), \sigma^2)</span>, com <span class="math inline">X_i \in \mathcal{U}(0, 10)\,, \forall i.</span> Experimente na aplicação web que segue o método <em>boosting</em>. Observe o comportamento do estimador variando os parâmetros do algoritmo <em>boosting</em>. Perceba que para <span class="math inline">\lambda = 0</span> não há aprendizado algum! Se desejar visualizar a aplicação abaixo de forma ampliada, clique <a href="https://pedro-rafael.shinyapps.io/shiny_apps/#section-boosting">aqui</a>.</p>
<p><br></p>
 <iframe id="example1" src="https://pedro-rafael.shinyapps.io/shiny_apps/#section-boosting" style="border: none; width: 100%; height: 90%" frameborder="0"></iframe>
</section>
<section id="boosting-5" class="slide level2">
<h2>🚀 Boosting</h2>
<p><br></p>
<p>Em geral, as abordagens de aprendizagem estatística que aprendem lentamente tendem a executar bem. Observe que no <em>boosting</em>, ao contrário do ensacamento, a construção de cada árvore depende fortemente das árvores que já foram “cultivadas” 🌱🚿🌳. Portanto, diferentemente do <em>bagging</em> que é um algoritmo em que as árvores em cada iteração são mutuamente independentes e, portanto, facilmente paralelizável, no <em>boosting</em> as árvores são dependentes umas das outras.</p>
<p><br></p>
<p>Na literatura de <em>machine learning</em> há diversos algoritmos que implementam o <em>boosting</em>. Uma implementação bastante popular, por conta de seu desempenho, é denominada <strong>XGBoost</strong>, proposto em CHEN, Tianqi; GUESTRIN, Carlos. <a href="https://arxiv.org/pdf/1603.02754.pdf"><strong>Xgboost: A scalable tree boosting system</strong></a>. In: <em>Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</em>. 2016. p.&nbsp;785-794.</p>
<p><br></p>
<p>No R, o algoritmo está implementado na biblioteca <a href="https://github.com/dmlc/xgboost">xgboost</a>. Caso deseje utilizar a biblioteca <a href="https://www.tidymodels.org/">tidymodels</a> em sua modelagem, é possível utilizar o <strong>XGBoost</strong> usando a função <code>boost_tree</code> da biblioteca <a href="https://parsnip.tidymodels.org/reference/boost_tree.html">parsnip</a> que faz parte do <a href="https://www.tidymodels.org/">tidymodels</a>. Por padrão, essa função já utiliza o algoritmo <strong>XGBoost</strong>.</p>
</section>
<section id="boosting-6" class="slide level2">
<h2>🚀 Boosting</h2>
<p><br></p>
<p>Uma outra forma de obter um valor adequado de <span class="math inline">B</span> é parar de adicionar iterações quando o risco observado começa a aumentar. Lembre-se que há um <em>trade off</em> entre viés e variância, em que as iterações começam por um estimador <span class="math inline">\widehat{g}</span> com variância nula e na medida que as iterações progridem, <span class="math inline">\widehat{g}</span> aumenta sua variância em troca da diminuição do viés. Para algum valor de <span class="math inline">B</span> o estimador poderá perder um pouco de performance. A ideia é escolher um valor de <span class="math inline">B</span> antes de atingir uma piora de <span class="math inline">\widehat{R}(g)</span> (risco observado). Essa estratégia é denominada de <em>early stopping</em> ⌛.</p>
<p><br></p>
<p><strong>Muito embora aqui apresentamos o algoritmo <em>boostring</em> em um contexto de árvores de regressão, esse algoritmo é genérico e poderá ser utilizado como estatégia para combinar modelos que são individualmente fracos</strong>.</p>
<p><br></p>

<img data-src="gifs/pensativo.gif" class="r-stretch"></section>
<section id="exercícios-11" class="slide level2">
<h2>📚 Exercícios</h2>
<p><br></p>
<p><span class="red">Exercício</span>: Considere a variável aleatória <span class="math inline">Y_i \sim \mathcal{N}(\sin(X_i), \sigma^2)</span>, com <span class="math inline">X_i \in \mathcal{U}(0, 10)\,, \forall i.</span> Implemente uma função em R que construi o gráfico da aplicação anterior. Essa função deverá ter os argumentos do algoritmo <em>boosting</em>, além de outros argumentos para controle do tamanho da amostra e da variância <span class="math inline">\sigma^2</span>.</p>
<p><br></p>
<p><span class="red">Exercício</span>: Construa um gráfico com a avaliação do risco observado do método de <em>boosting</em> para diferentes valores de <span class="math inline">B</span>. Considere <span class="math inline">B = 1, \cdots, 10000</span>. Comente o resultado.</p>
<p><br></p>
<p><span class="red">Exercício</span>: Utilizando um procedimento de validação cruzada, e o <a href="https://www.tidymodels.org/">tidymodels</a>, obtenha uma estimativa para os hiperparâmetros da função <a href="https://parsnip.tidymodels.org/reference/boost_tree.html"><code>boost_tree()</code></a>, a saber, os argumentos <code>trees</code> (número de árvores), <code>tree_depth</code> (profundidade da árvore) e <code>learn_rate</code> (taxa de aprendizado). Avalie o risco preditivo do modelo, i.e., estime <span class="math inline">\widehat{R}(g)</span>.</p>
<p><br></p>
<p><span class="red">Exercício</span>: Compare o <em>bagging</em> de árvores de regressão, com o método <em>random forest</em> e o <em>boosting</em> de árvores de regressão. Qual o risco preditivo de cada um deles para prever <span class="math inline">Y_i</span>? Construa um gráfico com as previsões, no conjunto de teste, de cada um dos modelos.</p>
</section>
<section id="exercícios-12" class="slide level2">
<h2>📚 Exercícios</h2>
<p><br></p>
<p><span class="red">Exercício</span>: Considere o problema em que o objetivo é prever a <strong>pontuação</strong> (<em>score</em>) / (item 2) do aluno com base em algumas variáveis. São elas:</p>
<p><br></p>
<ol type="1">
<li class="fragment"><strong>Horas estudadas</strong>: o número total de horas gastas estudando por cada aluno;</li>
<li class="fragment"><strong>Pontuação</strong>: As notas obtidas pelos alunos em testes anteriores;</li>
<li class="fragment"><strong>Atividades extracurriculares</strong>: Se o aluno participa de atividades extracurriculares (Sim ou Não);</li>
<li class="fragment"><strong>Horas de sono</strong>: o número médio de horas de sono que o aluno teve por dia;</li>
<li class="fragment"><strong>Amostras de perguntas praticadas</strong>: O número de amostras de perguntas que o aluno praticou.</li>
</ol>
<p><br></p>
<p>Você poderá baixar e ter uma descrição maior da base de dados clicando <a href="https://www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression">aqui</a>. Avalie o poder preditivo do <em>random forest</em> comparando com o <em>boosting</em> e <span class="math inline">k</span>NN. Construa uma análise usando um notebook de <a href="https://quarto.org/">quarto</a>, comentando os passos.</p>
<p><br></p>
<p><span class="red">Exercício</span>: Refaça o exercício anterior usando os dados de vermelho 🍇🍷, disponíveis <a href="https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009">aqui</a>.</p>


<img src="https://www.ufpb.br/de/contents/imagens/logode.png" class="slide-logo r-stretch"><div class="footer footer-default">
<p><a href="https://www.ufpb.br/de">Departamento de Estatística da UFPB</a></p>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-pointer/pointer.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-attribution/attribution.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'pointer': {"key":"q","color":"red","pointerSize":16,"alwaysVisible":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: true,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, RevealPointer, RevealAttribution, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copiada");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copiada");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>