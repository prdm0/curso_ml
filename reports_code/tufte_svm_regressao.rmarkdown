---
title: "Suport Vector Regression Machine - SVRM"
subtitle: "Comparando com outros modelos"
date: "`r Sys.Date()`"
lang: pt
author: "Prof. Dr. Pedro Rafael D. Marinho"
format:
  html:
    html-math-method: katex
    code-tools: true
    code-copy: true
    number-sections: true
page-layout: full
editor: source
---


## Carregando bibliotecas necess√°rias


```{r}
library(tidymodels)
library(tidyverse)
library(GGally)
library(skimr)

# Resolvendo poss√≠veis conflitos entre o tidymodels e outras bibliotecas
tidymodels::tidymodels_prefer()
```


## Importando a base de dados

Utilizando os dados de vinho vermelhoüç∑, dispon√≠veis
[aqui](https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009),
fa√ßa uma pequena an√°lise explorat√≥ria dos dados. No
[link](https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009)
do Kaggle voc√™ consegue uma explica√ß√£o sobre o que significa cada uma
das vari√°veis.

Os dados, no meu caso, est√£o no diret√≥rio
`"../dados/winequality-red.csv"`. Voc√™ dever√° alterar o *path* para o
diret√≥rio encontra-se a base que dever√° ser obtida no link acima.


```{r}
dados <- readr::read_csv(file = "../dados/winequality-red.csv")
```


## Uma explora√ß√£o r√°pida dos dados

√â sempre importante olhar os dados antes de tentar modelar. Uma an√°lise
explorat√≥ria sempre ser√° √∫til para identificarmos poss√≠veis
inconsist√™ncias.


```{r}
visdat::vis_dat(dados)
```


O gr√°fico acima mostra que temos uma base de dados sem informa√ß√µes
faltantes e todas as *features* presentes na base s√£o num√©ricas. √â uma
situa√ß√£o confort√°vel, haja vista que, aqui, n√£o precisaremos nos
preocupar com imputa√ß√£o de dados faltantes.

Um resumo dos dados poder√° ser obtido utilizando a fun√ß√£o `glimpse` do
pacote **dplyr** que √© carregado com a biblioteca **tidyverse** de R.


```{r}
dados |> 
  dplyr::glimpse()
```


√â poss√≠vel todas as correla√ß√µes entre todas as vari√°veis da base, com a
fun√ß√£o `data_vis_cor`. Um gr√°fico √∫til com as correla√ß√µes poder√° ser
obtido usando a fun√ß√£o `vis_cor`, conforme abaixo:


```{r}
visdat::data_vis_cor(dados)
visdat::vis_cor(dados)
```


Um gr√°fico de scatterplot para as vari√°veis num√©ricas poder√° ser √∫til.
Voc√™ poder√° fazer isso, usando a fun√ß√£o `ggcatmat` do pacote
[GGally](https://ggobi.github.io/ggally/index.html).


```{r}
dados |> 
  GGally::ggscatmat()
```


As bibliotecas **GGally** e **skimr** tamb√©m possuem fun√ß√µes √∫teis que
podem nos auxiliar no processo de explora√ß√£o dos dados.


```{r}
dados |> 
  GGally::ggpairs()

dados |> 
  skimr::skim()
```


## Construindo os workflows dos modelos

Iremos comparar os modelos de regress√£o linar utilizando *elastic net*,
com o m√©todo $k$NN e *suport vector regression machine* - SVRM.
Buscaremos pelo melhor modelo de cada uma das metodologias consideradas.
Posteriormente iremos escolher o melhor modelo entre os melhores de cada
uma das classes. A ideia √© escolher o melhor modelo que consiga prever
melhor a qualidade do vinho, i.e., prever a vari√°vel `quality`.

### Divis√£o dos dados

Aqui usaremos as fun√ß√µes `initial_split`, `training` e `testing` para realizar o 
m√©todo *hold-out* (divis√£o inicial dos dados) em treino e teste. Vamos considerar
$80\%$ para treino e $20\%$ para teste. A fun√ß√£o `initial_split` ir√° realizar a divis√£o,
por√©m, as fun√ß√µes `training` e `testing` s√£o respons√°veis para obtermos as tibbles da 
base de treino e teste, respectivamente. 

Aqui, os dados est√° sendo estratificado pelo *label*, i.e., pela vari√°vel `quality` que desejamos
prever:


```{r}
set.seed(0) # Fixando uma semente
divisao_inicial <- rsample::initial_split(dados, prop = 0.8, strata = "quality")
treinamento <- rsample::training(divisao_inicial) # Conjunto de treinamento
teste <- rsample::testing(divisao_inicial) # Conjunto de teste
```


### Tratamento dos dados (pr√©-processamento)

Apesar de n√£o haver muito o que fazer nos dados que estamos utilizando
nesse exemplo, em que nosso objetivo aqui √© ter uma an√°lise explicativa
de como comparar modelos usando o
[tidymodels](https://www.tidymodels.org/), iremos utilizar a biblioteca
[recipes](https://recipes.tidymodels.org/). Os dados cont√©m apenas
vari√°veis num√©ricas com todas informa√ß√µes presentes, tornando o problema
um pouco mais simples.

**Na receita, iremos colocar as seguintes etapas**:

1.  Tomaremos o logar√≠tmo de todas as vari√°veis peditoras (*features*);
2.  Remover vari√°veis preditoras (*features*) que eventualmente est√£o
    altamente correlacionadas (usando a fun√ß√£o `step_corr`);
3.  Remover vari√°veis que possam ter vari√¢ncia pr√≥xima √† zero, i.e., que
    sejam aproximadamente constantes (usando `step_zv`);
4.  Normalizar os dados utilizando a fun√ß√£o `step_normalize`.

Para que iremos remover vari√°veis altamente correlacionadas apenas nas
vari√°veis preditoras, utilizamos a fun√ß√£o `all_predictors` como
argumentod a fun√ß√£o `step_corr`. J√° no passo de normaliza√ß√£o dos dados,
quando consideramos todas as vari√°veis num√©ricas, passamos para a fun√ß√£o
`step_normalize` a fun√ß√£o `all_numeric` que especifica que dever√° ser
normalizado todas as vari√°veis num√©ricas. Na verdade, a normaliza√ß√£o se
d√° em todas as vari√°veis num√©ricas, e portanto, esse argumento poderia
ser omitido. Al√©m disso, toda nossa base √© formada por vari√°veis
num√©ricas, o que torna redundante o uso, mas irei deixar expl√≠cito que
todas as vari√°veis num√©ricas est√£o sendo normalizadas.


```{r}
receita_1 <- 
  treinamento |> 
    recipe(formula = quality ~ .) |>
    step_YeoJohnson(all_predictors()) |>
    step_normalize(all_predictors()) |>
    step_zv(all_predictors()) |>
    step_corr(all_predictors())

receita_2 <- 
  treinamento |> 
    recipe(formula = quality ~ .) |>
    step_YeoJohnson(all_predictors()) |>
    step_normalize(all_predictors())
```



**Como fazemos para observar se nosso pr√©-processamento funcionou?**

![](/gifs/hum.gif)

F√°cil, assim:


```{r}
receita_1 |> 
  prep() |> 
  juice()
```


A fun√ß√£o `prep` estima uma receita de pr√©-processamento. Algumas fun√ß√µes `step_*` pode
conter par√¢metros que devem ser estimados. Inclusive, poderemos tunar esses par√¢metros com 
a fun√ß√£o `tune` do pacote [tune](https://tune.tidymodels.org/). Por exemplo, se tivessemos interesse
em interpolar uma *feature* usando a fun√ß√£o `step_ns`, o argumento `deg_free` que refere-se ao
grau do polin√¥mio poderia ser "tunado".

Todas as etapas de pr√©-processamento s√£o estimadas em cima do conjunto de dados de treinamento. Por exemplo, na etapa em que realiza-se a normaliza√ß√£o dos dados, a m√©dia a vari√¢ncia dos dados s√£o estimadas uma √∫nica vez na base de dados completa, e sempre que essa refeita for aplicada a novos dados, ser√° utilizado essa mesma m√©dia e vari√¢ncia, ou seja, **n√£o ser√° recalculada com base no novo conjunto de dados**.

Poder√≠amos utilizar a fun√ß√£o `bake` ao inv√©s da `juice`. A diferen√ßa de uma para a outra √© que a fun√ß√£o `bake` utiliza uma receita j√° estimada com `prep` e poder√° ser aplicada √† novos dados. J√° a `juice` retorna a tibble com a receita preparada para o conjunto de dados de treinamento, ou ao conjunto de dados ao qual uma receita foi preparada com `prep`. Usando `bake` para o conjunto de dados de treinamento, poder√≠amos fazer:


```{r}
receita_1 |> 
  prep() |> 
  bake(new_data = treinamento)
```


### Definindo os modelos

O c√≥digo que segue faz a configura√ß√£o realiza a configura√ß√£o dos modelos
que ser√£o comparados. O c√≥digo `tune::tune()` especifica que o
respectivo par√¢metro de sintoniza√ß√£o ser√° obtido no processo de
valida√ß√£o cruzada, particularmente, um *grid search*.


```{r}
modelo_elastic <- 
  parsnip::linear_reg(penalty = tune::tune(), mixture = tune::tune()) |> 
  parsnip::set_mode("regression") |> 
  parsnip::set_engine("glmnet")

modelo_knn <-
  parsnip::nearest_neighbor(
    neighbors = tune::tune(),
    dist_power = tune::tune(), 
    weight_func = "gaussian" 
  ) |> 
  parsnip::set_mode("regression") |> 
  parsnip::set_engine("kknn")

modelo_svm <- 
  parsnip::svm_rbf(
    cost = tune::tune(),
    rbf_sigma = tune::tune(),
    margin = tune::tune()
  ) |> 
  parsnip::set_mode("regression") |> 
  parsnip::set_engine("kernlab")
```


### Criando o conjunto de valida√ß√£o 

Uma vez que temos a divis√£o dos dados em conjunto de treinamento e conjunto de teste, precisamos construir o conjunto de valida√ß√£o, que necesse caso utilizaremos um $k$-*fold cross-validation*. Lembre-se que a valida√ß√£o cruzada ocorrer√° sob a amostra de treinamento e necesso processo, a base de dados de treinamento ser√° dividida em $k$ partes aproximadamente iguais em que trainamos o modelo em $k-1$ e validanos no *fold* restante, em que isso √© feito $k$ vezes. Utilizando o conjunto $k-1$ em cada uma das divis√µes da valida√ß√£o cruzada em que diferentes combina√ß√µes de hiperpar√¢metros s√£o experimentadas e avaliada no conjunto de valida√ß√£o em cada divis√£o da valida√ß√£o cruzada.

O c√≥digo que segue, em que utiliza a fun√ß√£o `vfold_cv` da biblioteca [rsample](https://rsample.tidymodels.org/) apenas cria a valida√ß√£o cruzada. Nesse caso, a valida√ß√£o ser√° estratificada considerando a o *label* `quality` (qualidade do vinho), assim como foi feito na divis√£o inicial no *hold-out* (divis√£o inicial dos dados).

Utilizaremos $k=8$:


```{r}
validacao_cruzada <- 
  treinamento |> 
  rsample::vfold_cv(v = 8L, strata = quality)
```


### Criando um *workflow* completo 

Aqui criaremos um *workflow* completo com todos modelos a serem comparados. Chamaremos ele de `wf_todos`:


```{r}
wf_todos <-
  workflow_set(
    preproc = list(receita_1, receita_2),
    models = list(
      knn_fit = modelo_knn,
      elastic_fit = modelo_elastic,
      svm_fit = modelo_svm
    ),
    cross = TRUE
  )
```


Podemos manipular alguns argumentos que controlam aspectos da pesquisa em grade (*grid search*).. Fazemos isso com o uso da fun√ß√£o `control_grid` da biblioteca [parsnip](https://parsnip.tidymodels.org/). Por exemplo, podemos informar que desejamos paralelizar essa pesquisa, passando o argumento ` parallel_over = "resamples"`. Podemos tamb√©m salvar as predi√ß√µes para cada um dos modelos especificando o argumento `save_pred = TRUE`. Se desejarmos anexar √† sa√≠da o *workflow*, fazemos `save_workflow = TRUE`.

Assim, criaremos o objeto `controle_grid`, para que possamos passar a fun√ß√£o `workflow_map` posteriormente. Tem-se:


```{r}
controle_grid <- control_grid(
  save_pred = TRUE,
  save_workflow = TRUE,
  parallel_over = "resamples"
)
```


### Trainando o modelo

Nessa etapa iremos treinar o modelo usando a fun√ß√£o `workflow_map` do pacote [workflow](https://workflows.tidymodels.org). Temos ent√£o:


```{r}
treino <-
  wf_todos |> 
  workflow_map(
    resamples = validacao_cruzada,
    grid = 20L,
    control = controle_grid
  )
```


A fun√ß√£o `autoplot` do pacote **ggplot2** √© √∫til para que possamos visualizar o desempenho de cada um dos modelos considerando a m√©trica do EQM. Isso √© feito da seguinte forma:


```{r}
autoplot(treino, metric = "rmse") + 
  labs(
    title = "Avalia√ß√£o dos modelos de regress√£o",
    subtitle = "Utilizando a m√©trica do EQM"
  ) + 
  xlab("Rank dos Workflows") +
  ylab("Erro Quadr√°tico M√©dio - EQM")
```

Perceba que no gr√°fico acima, temos os 8 modelos avaliados no $8$-*folds cross-validation*. Portanto, para cada um dos modelos comparados, temos 8 avalia√ß√µes. Caso deseje avaliar as m√©tricas do melhor cen√°rio de cada um dos modelos, fazemos:


```{r}
melhores <- 
  treino |> 
  rank_results(select_best = TRUE, rank_metric = "rmse")

autoplot(treino, select_best = TRUE)

```

Vamos agora selecionar o melhor modelo dentre os modelos comparados. Isso n√£o quer dizer que o modelo seja bom para resolver o problema em quest√£o. Para ver um rank e saber qual modelo e receita foram as melhores, fazemos:


```{r}
treino |> 
  rank_results()
```


Assim, podemos perceber que a `receita_1` combinada com o modelo $k$NN √© o melhor escolha entre os modelos e receitas comparadas. Portanto:


```{r}
melhor_modelo <- 
  treino |> 
  extract_workflow_set_result(id = "recipe_1_knn_fit") |> 
  select_best(metric = "rmse")

melhor_modelo
```


### Avalia√ß√£o final do melhor modelo

Ap√≥s a escolha do melhor modelo e da estima√ß√£o de seus hiperpar√¢metros, nesse caso, o modelo $k$NN com $k = 12$ e `dist_power \approx 1.47`, precisamos testar o desempenho do modelo final segundo a base de dados de teste. Para tanto, utilizamos a fun√ß√£o `last_fit` do pacote [tune](https://tune.tidymodels.org/reference/last_fit.html). Temos que:


```{r}
wf_final <- 
  treino |> 
  extract_workflow(id = "recipe_1_knn_fit") |> 
  finalize_workflow(melhor_modelo)

teste <- 
  wf_final |> 
  last_fit(split = divisao_inicial)

teste$.metrics
```


Agora que temos os hiperpar√¢metros estimados e temos uma boa estimativa do risco preditivo real do modelo final selecionado, poderemos preceder com um ajuste final, com toda a base de dados (treinamento + teste).


```{r}
modelo_final <- 
  wf_final |> 
  fit(dados)
```


### Salvando o modelo

Depois que temos o modelo finalizado, podemos ter alguns interesses de como utilizar o resultado, i.e., o modelo treinado. Entre alguns motivos, posso citar:

1. Salvar o modelo para uso no futuro, sem ter que retreinar;
2. Distribuir o modelo para que outras pessoas possam experimentar, sem terem que executar seu script R e retreinar o modelo;
3. Introduzir seu modelo treinado em uma API que ir√° consumir os resultados, i.e., consumir as previs√µes do modelo.

Nessas situa√ß√µes, √© conveniente salvar o modelo em um arquivo serializado (*R Data Serialization*). Tais arquivos possuem a extens√£o `.rds`. Devemos fazer:



```{r}
#| eval: false

# Salvando o modelo em um arquivo serializado
saveRDS(modelo_final, file = "modelo_final.rds")

# Lendo o arquivo serializado com o modelo final
load(file = "modelo_final.rds")

# Fazendo novas previs√µes
predict(modelo_final, new_data = novos_dados)
```

